<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns#">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />

<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="http://www.lindonslog.com/xmlrpc.php" />
<link href='http://fonts.googleapis.com/css?family=Economica:400,400italic,700,700italic' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans:400,700' rel='stylesheet' type='text/css'>
<!--[if lt IE 9]>
<script src="http://www.lindonslog.com/wp-content/themes/Winter/js/html5.js" type="text/javascript"></script>
<![endif]-->


<!-- This site is optimized with the Yoast SEO plugin v3.1.2 - https://yoast.com/wordpress/plugins/seo/ -->
<title>Mathematics Archives - Page 2 of 3 - Ive Moved</title>
<link rel="canonical" href="https://michaellindon.github.io/" />
<link rel="prev" href="../../index.html" />
<link rel="next" href="../3/index.html" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="object" />
<meta property="og:title" content="Mathematics Archives - Page 2 of 3 - Ive Moved" />
<meta property="og:url" content="http://www.lindonslog.com/category/mathematics/page/2/" />
<meta property="og:site_name" content="Ive Moved" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Mathematics Archives - Page 2 of 3 - Ive Moved" />
<meta name="twitter:site" content="@lindonslog" />
<!-- / Yoast SEO plugin. -->

<link rel='dns-prefetch' href='http://s0.wp.com/' />
<link rel='dns-prefetch' href='http://s.gravatar.com/' />
<link rel='dns-prefetch' href='http://s.w.org/' />
<link rel="alternate" type="application/rss+xml" title="Ive Moved &raquo; Feed" href="../../../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="Ive Moved &raquo; Comments Feed" href="../../../../comments/feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="Ive Moved &raquo; Mathematics Category Feed" href="../../feed/index.html" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.lindonslog.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.6.1"}};
			!function(a,b,c){function d(a){var c,d,e,f,g,h=b.createElement("canvas"),i=h.getContext&&h.getContext("2d"),j=String.fromCharCode;if(!i||!i.fillText)return!1;switch(i.textBaseline="top",i.font="600 32px Arial",a){case"flag":return i.fillText(j(55356,56806,55356,56826),0,0),!(h.toDataURL().length<3e3)&&(i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,65039,8205,55356,57096),0,0),c=h.toDataURL(),i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,55356,57096),0,0),d=h.toDataURL(),c!==d);case"diversity":return i.fillText(j(55356,57221),0,0),e=i.getImageData(16,16,1,1).data,f=e[0]+","+e[1]+","+e[2]+","+e[3],i.fillText(j(55356,57221,55356,57343),0,0),e=i.getImageData(16,16,1,1).data,g=e[0]+","+e[1]+","+e[2]+","+e[3],f!==g;case"simple":return i.fillText(j(55357,56835),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode8":return i.fillText(j(55356,57135),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode9":return i.fillText(j(55358,56631),0,0),0!==i.getImageData(16,16,1,1).data[0]}return!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i;for(i=Array("simple","flag","unicode8","diversity","unicode9"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='papercite_css-css'  href='../../../../wp-content/plugins/papercite/papercite.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='style-css'  href='../../../../wp-content/themes/Winter/style.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='grid-css'  href='../../../../wp-content/themes/Winter/css/grid.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='wp-style-css'  href='../../../../wp-content/themes/Winter/css/wp-style.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fontwaesome-css'  href='../../../../wp-content/themes/Winter/css/font-awesome.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fontwaesome-ie-css'  href='../../../../wp-content/themes/Winter/css/font-awesome-ie7.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='flexslider-css'  href='../../../../wp-content/themes/Winter/css/flexslider.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fancybox-css'  href='../../../../wp-content/themes/Winter/css/jquery.fancybox.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='jetpack_css-css'  href='../../../../wp-content/plugins/jetpack/css/jetpack.css%3Fver=3.9.7.css' type='text/css' media='all' />
<script type='text/javascript' src='../../../../wp-includes/js/jquery/jquery.js%3Fver=1.12.4'></script>
<script type='text/javascript' src='../../../../wp-includes/js/jquery/jquery-migrate.min.js%3Fver=1.4.1'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/papercite/js/papercite.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jquery.flexslider-min.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jquery.fancybox.pack.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jwplayer.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/custom.js%3Fver=1'></script>
<link rel='https://api.w.org/' href='../../../../wp-json/index.html' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 4.6.1" />

<link rel='dns-prefetch' href='http://jetpack.wordpress.com/'>
<link rel='dns-prefetch' href='http://s0.wp.com/'>
<link rel='dns-prefetch' href='http://s1.wp.com/'>
<link rel='dns-prefetch' href='http://s2.wp.com/'>
<link rel='dns-prefetch' href='http://public-api.wordpress.com/'>
<link rel='dns-prefetch' href='http://0.gravatar.com/'>
<link rel='dns-prefetch' href='http://1.gravatar.com/'>
<link rel='dns-prefetch' href='http://2.gravatar.com/'>
<link rel='dns-prefetch' href='http://widgets.wp.com/'>
<style type="text/css" id="syntaxhighlighteranchor"></style>
<link rel="icon" href="../../../../wp-content/uploads/2015/09/cropped-L-32x32.png" sizes="32x32" />
<link rel="icon" href="../../../../wp-content/uploads/2015/09/cropped-L-192x192.png" sizes="192x192" />
<link rel="apple-touch-icon-precomposed" href="../../../../wp-content/uploads/2015/09/cropped-L-180x180.png" />
<meta name="msapplication-TileImage" content="http://www.lindonslog.com/wp-content/uploads/2015/09/cropped-L-270x270.png" />

<style id="custom-css-css">.input_prompt{color:#06c}.output_prompt{color:#c00}.prompt{font-family:monospace;font-size:14px}.c,c1{color:#408080;font-style:italic}.k{color:#382;font-weight:700}.kn{color:#382;font-weight:700}.mi{color:#080}.mf{color:#080}.o{color:#96f}.ow{color:#BA22FF;font-weight:700}.nb{color:#382}.n{color:#000}.s,.s1{color:#c22}.se{color:#c22;font-weight:700}.si{color:#C06688;font-weight:700}.nn{color:#4D00FF;font-weight:700}.output_area pre{background-color:#FFF;padding-left:5%}.code_cell{padding-left:1%}.cell{margin-top:10px;margin-bottom:10px}br{line-height:2}.cell h1,h2,h3,h4{margin-top:30px;margin-bottom:10px}</style>
</head>

<body class="archive paged category category-mathematics category-5 paged-2 category-paged-2">
<div id="page" class="hfeed site">

	<header id="masthead" class="site-header" role="banner">
		<div id="botmenu">
			<div class="container_6">
			<div id="submenu" class="menu-top-container"><ul id="web2feel" class="sfmenu"><li id="menu-item-434" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-434"><a href="../../../../index.html">About</a></li>
<li id="menu-item-405" class="menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item menu-item-has-children menu-item-405"><a href="../../index.html">Mathematics</a>
<ul class="sub-menu">
	<li id="menu-item-814" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-814"><a href="../../linear-algebra/index.html">Linear Algebra</a></li>
	<li id="menu-item-406" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-406"><a href="../../statistics/index.html">Statistics</a></li>
</ul>
</li>
<li id="menu-item-404" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-404"><a href="../../../linux-unix/index.html">Linux/Unix</a></li>
<li id="menu-item-407" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-407"><a href="../../../programming/index.html">Programming</a>
<ul class="sub-menu">
	<li id="menu-item-408" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-408"><a href="../../../programming/openmp/index.html">OpenMP</a></li>
	<li id="menu-item-487" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-487"><a href="../../../programming/r/index.html">R</a></li>
</ul>
</li>
<li id="menu-item-842" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-842"><a href="../../../../links/index.html">Links</a></li>
</ul></div>			</div>
		</div>
		
		<div class="top cf ">
			<div class="head container_6 cf">
				<div class="logo grid_3">
					
					<h1 class="site-title logo"><a id="blogname" rel="home" href="../../../../index.html" title="Ive Moved">Ive Moved</a></h1>
	
					<h2 class="site-description"></h2>
				</div>
				<div class="topbar grid_3">
						<form method="get" id="searchform" action="../../../../index.html" role="search">
		<label for="s" class="assistive-text">Search</label>
		<input type="text" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search" />
	</form>
				</div>
			</div>	
		</div>
	</header><!-- #masthead .site-header -->

	<div id="main" class="site-main container_6">
		<section id="primary" class="content-area">
			<div id="content" class="site-content" role="main">

			
				<header class="page-header">
					<h1 class="page-title">
						Category Archives: <span>Mathematics</span>					</h1>
									</header><!-- .page-header -->

								
					
<article id="post-762" class="grid_6 post-762 post type-post status-publish format-standard hentry category-mathematics category-r category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1247 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/mcmc-interweaving-parameterization-efficiency/index.html#comments"><span class="dsq-postid" data-dsqidentifier="762 http://www.lindonslog.com/?p=762">1 Comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/mcmc-interweaving-parameterization-efficiency/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Model Scale Parameterization for MCMC Efficiency</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>I recently came across a very interesting paper by Y. Yu and X. Meng[<a class="papercite_bibcite" href="index.html#paperkey_0">1</a>] who present an interweaving strategy between different model parameterizations to improve mixing. It is well known that different model parameterizations can perform better than others under certain conditions. Papaspiliopoulos, Roberts and Sköld [<a class="papercite_bibcite" href="index.html#paperkey_1">2</a>] present a general framework for how to parameterize hierarchical models and provide insights into the conditions under which centered and non-centered parameterizations outperform each other. One isn&#8217;t, however, restricted to reperameterizations of location parameters only, as outlined in the aforementioned paper, and so I decided to experiment with reparameterizations of the scale parameter in a simple hierarchical model with improper priors on the parameters.</p>
<h2>Centered Parameterization</h2>
<p>Papaspiliopoulos gave a general definition of the centered parameterization to be when <img src="http://s0.wp.com/latex.php?latex=Y_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{i}" title="Y_{i}" class="latex" /> is independent of <img src="http://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda" title="&#92;lambda" class="latex" /> given <img src="http://s0.wp.com/latex.php?latex=X_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="X_{i}" title="X_{i}" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Y_%7Bi%7D%7CX_%7Bi%7D%2C%5Csigma%5E%7B2%7D+%5Csim+N%28X_%7Bi%7D%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle Y_{i}|X_{i},&#92;sigma^{2} &#92;sim N(X_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (1)" title="&#92;displaystyle Y_{i}|X_{i},&#92;sigma^{2} &#92;sim N(X_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (1)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+X_%7Bi%7D%7C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%280%2C%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle X_{i}|&#92;sigma^{2},&#92;lambda^{2} &#92;sim N(0,&#92;lambda^{2}&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (2)" title="&#92;displaystyle X_{i}|&#92;sigma^{2},&#92;lambda^{2} &#92;sim N(0,&#92;lambda^{2}&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (2)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28+%5Clambda%5E%7B2%7D+%29+%5Cpropto+%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle p( &#92;lambda^{2} ) &#92;propto &#92;frac{1}{&#92;lambda^{2}} &#92; &#92; &#92; &#92; &#92; (3)" title="&#92;displaystyle p( &#92;lambda^{2} ) &#92;propto &#92;frac{1}{&#92;lambda^{2}} &#92; &#92; &#92; &#92; &#92; (3)" class="latex" /></p>
<h3>Full Conditionals</h3>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clambda%5E%7B2%7D%7CY_%7B1%3An%7D%2CX_%7B1%3An%7D%2C%5Csigma%5E%7B2%7D+%5Csim+%5CGamma%5E%7B-1%7D%5Cleft%28+%5Cfrac%7Bn%7D%7B2%7D%2C+%5Cfrac%7B%5Csum_%7Bi%7D%5E%7Bn%7D+X_%7Bi%7D%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%29+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;lambda^{2}|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim &#92;Gamma^{-1}&#92;left( &#92;frac{n}{2}, &#92;frac{&#92;sum_{i}^{n} X_{i}^{2}}{2&#92;sigma^{2}}&#92;right) &#92; &#92; &#92; &#92; &#92; (4)" title="&#92;displaystyle &#92;lambda^{2}|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim &#92;Gamma^{-1}&#92;left( &#92;frac{n}{2}, &#92;frac{&#92;sum_{i}^{n} X_{i}^{2}}{2&#92;sigma^{2}}&#92;right) &#92; &#92; &#92; &#92; &#92; (4)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+X_%7Bi%7D%7CY_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%5Cleft%28+%5Cfrac%7B%5Cfrac%7BY_%7Bi%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%7B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle X_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{Y_{i}}{&#92;sigma^{2}}}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (5)" title="&#92;displaystyle X_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{Y_{i}}{&#92;sigma^{2}}}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (5)" class="latex" /></p>
<h2>Non-Centered Parameterization</h2>
<p>Papaspiliopoulos gave a general definition of the non-centered parameterization to be when <img src="http://s0.wp.com/latex.php?latex=%5Ctilde%7BX%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;tilde{X}_{i}" title="&#92;tilde{X}_{i}" class="latex" /> and <img src="http://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda" title="&#92;lambda" class="latex" /> are a priori independent.</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Y_%7Bi%7D%7C%5Ctilde%7BX%7D_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda+%5Csim+N%28%5Clambda+%5Ctilde%7BX%7D_%7Bi%7D%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle Y_{i}|&#92;tilde{X}_{i},&#92;sigma^{2},&#92;lambda &#92;sim N(&#92;lambda &#92;tilde{X}_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (6)" title="&#92;displaystyle Y_{i}|&#92;tilde{X}_{i},&#92;sigma^{2},&#92;lambda &#92;sim N(&#92;lambda &#92;tilde{X}_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (6)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctilde%7BX%7D_%7Bi%7D%7C%5Csigma%5E%7B2%7D+%5Csim+N%280%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;tilde{X}_{i}|&#92;sigma^{2} &#92;sim N(0,&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (7)" title="&#92;displaystyle &#92;tilde{X}_{i}|&#92;sigma^{2} &#92;sim N(0,&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (7)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28%5Clambda%29+%5Cpropto+1+%5C+%5C+%5C+%5C+%5C+%288%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle p(&#92;lambda) &#92;propto 1 &#92; &#92; &#92; &#92; &#92; (8)" title="&#92;displaystyle p(&#92;lambda) &#92;propto 1 &#92; &#92; &#92; &#92; &#92; (8)" class="latex" /></p>
<h3>Full Conditionals</h3>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clambda%7CY_%7B1%3An%7D%2CX_%7B1%3An%7D%2C%5Csigma%5E%7B2%7D+%5Csim+N+%5Cleft%28+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7DY_%7Bi%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7D%5E%7B2%7D%7D%2C+%5Cfrac%7B%5Csigma%5E%7B2%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7D%5E%7B2%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%289%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;lambda|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim N &#92;left( &#92;frac{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}Y_{i}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}}, &#92;frac{&#92;sigma^{2}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}} &#92;right) &#92; &#92; &#92; &#92; &#92; (9)" title="&#92;displaystyle &#92;lambda|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim N &#92;left( &#92;frac{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}Y_{i}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}}, &#92;frac{&#92;sigma^{2}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}} &#92;right) &#92; &#92; &#92; &#92; &#92; (9)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctilde%7BX%7D_%7Bi%7D%7CY_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%5Cleft%28+%5Cfrac%7B%5Cfrac%7B%5Clambda+Y_%7Bi%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%7B%5Cfrac%7B%5Clambda%5E%7B2%7D%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Cfrac%7B%5Clambda%5E%7B2%7D%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%2810%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;tilde{X}_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{&#92;lambda Y_{i}}{&#92;sigma^{2}}}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (10)" title="&#92;displaystyle &#92;tilde{X}_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{&#92;lambda Y_{i}}{&#92;sigma^{2}}}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (10)" class="latex" /></p>
<h2>Interweaving Strategy</h2>
<p>Generally when the CP works well, the NCP works poorly and vice versa. Yaming Yu and Xiao-Li Meng[<a class="papercite_bibcite" href="index.html#paperkey_0">1</a>] present a way of combining both strategies by interweaving the Gibbs steps of both parameterizations at each iteration. The details can be read in their paper. I decided to test all three Gibbs samplers with the following R code:</p>
<pre class="brush: r; title: ; notranslate" title="">
#Generate Data
lam2=0.5
lam=sqrt(lam2)
sig2=1
n=1000
Xt=rnorm(n,0,sqrt(lam2*sig2))
Y=rnorm(n,Xt,sqrt(sig2))
nmc=2000
X=Xt

#Centered Parameterization
cp_lam2=rep(0,nmc)
cp_X=matrix(0,nmc,n)
for(i in 1:nmc){
	inv_lam2=rgamma(1,(n)/2,rate=(t(X)%*%X)/(2*sig2))
	lam2=1/inv_lam2
	X=rnorm(n,(1/(1/sig2 + 1/(sig2*lam2)))*Y/sig2, sqrt(1/(1/sig2 + 1/(sig2*lam2))))
	cp_lam2[i]=lam2
	cp_X[i,]=X
}
mean_cp_X=apply(cp_X,2,mean)

#Non-Centered Parameterization
X=Xt
ncp_lam2=rep(0,nmc)
ncp_X=matrix(0,nmc,n)
for(i in 1:nmc){
	lam=rnorm(1,t(X)%*%Y/(t(X)%*%X), sqrt(sig2/(t(X)%*%X)))
	lam2=lam*lam;
	X=rnorm(n, (1/(1/sig2 + lam2/sig2))*lam*Y/sig2, sqrt(1/(1/sig2+lam2/sig2))  )
	ncp_lam2[i]=lam2
	ncp_X[i,]=X
}
mean_ncp_X=apply(ncp_X,2,mean)

#Interweaving Strategy
int_lam2=rep(0,nmc)
int_X=matrix(0,nmc,n)
for(i in 1:nmc){
	X=rnorm(n,(1/(1/sig2 + 1/(sig2*lam2)))*Y/sig2, sqrt(1/(1/sig2 + 1/(sig2*lam2))))
	inv_lam2=rgamma(1,(n)/2,rate=(t(X)%*%X)/(2*sig2))
	half_lam2=1/inv_lam2
	X=X/sqrt(half_lam2)    #Transform to Xtilde
	lam=rnorm(1,t(X)%*%Y/(t(X)%*%X), sqrt(sig2/(t(X)%*%X)))
	lam2=lam*lam;
	int_lam2[i]=lam2
	int_X[i,]=X
}
mean_cp_X=apply(cp_X,2,mean)

#Remove Burnin
cp_lam2=cp_lam2[-(1:1000)]
ncp_lam2=ncp_lam2[-(1:1000)]
int_lam2=int_lam2[-(1:1000)]

#Plot Results
par(mfrow=c(3,3))
acf(cp_lam2)
plot(cp_lam2,type=&quot;l&quot;)
plot(cp_lam2[1:nmc-1],cp_lam2[2:nmc])
acf(ncp_lam2)
plot(ncp_lam2,type=&quot;l&quot;)
plot(ncp_lam2[1:nmc-1],ncp_lam2[2:nmc])
acf(int_lam2)
plot(int_lam2,type=&quot;l&quot;)
plot(int_lam2[1:nmc-1],int_lam2[2:nmc])
</pre>
<h2>Results</h2>
<h3><img src="http://s0.wp.com/latex.php?latex=%5Clambda%3D0.3&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda=0.3" title="&#92;lambda=0.3" class="latex" /></h3>
<div id="attachment_798" style="width: 910px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/08/lam0.3.png"><img class="size-full wp-image-798" alt="mcmc parameterization" src="../../../../wp-content/uploads/2013/08/lam0.3.png" width="900" height="900" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3.png 900w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3-300x300.png 300w" sizes="(max-width: 900px) 100vw, 900px" /></a><p class="wp-caption-text">Interweaving outperforms non-centered outperforms centered</p></div>
<h3><img src="http://s0.wp.com/latex.php?latex=%5Clambda%3D6&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda=6" title="&#92;lambda=6" class="latex" /></h3>
<div id="attachment_802" style="width: 910px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/08/lam6.png"><img class="size-full wp-image-802" alt="ncp poorly mixing" src="../../../../wp-content/uploads/2013/08/lam6.png" width="900" height="900" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/lam6.png 900w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam6-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam6-300x300.png 300w" sizes="(max-width: 900px) 100vw, 900px" /></a><p class="wp-caption-text">Interweaving outperforms centered outperforms non-centered</p></div>
<h2>Discussion</h2>
<p>As lambda gets small the centered parameterization becomes ever more autocorrelated and poorly mixing. When lambda becomes large the non-centered parameterization becomes ever more autocorrelated and poorly mixing. The interweaved Gibbs sampler exhibits great mixing in all cases.</p>
<div id="paperkey_0" class="papercite_entry">[1]           <a href='http://dx.doi.org/10.1198/jcgs.2011.203main' class='papercite_doi' title='View document in publisher site'><img src='../../../../wp-content/plugins/papercite/img/external.png' width='10' height='10' alt='[doi]' /></a>        Y. Yu and X. Meng, &#8220;To Center or Not to Center: That Is Not the Question&#8211;An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency,&#8221; <span style="font-style: italic">Journal of computational and graphical statistics</span>, vol. 20, iss. 3, pp. 531-570, 2011. <br />    <a href="javascript:void(0)" id="papercite_0" class="papercite_toggle">[Bibtex]</a></div>
<div class="papercite_bibtex" id="papercite_0_block">
<pre><code class="tex bibtex">@article{Yu11,
author = {Yu, Yaming and Meng, Xiao-Li},
citeulike-article-id = {10408757},
citeulike-linkout-0 = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2011.203main},
citeulike-linkout-1 = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2011.203main},
citeulike-linkout-2 = {http://dx.doi.org/10.1198/jcgs.2011.203main},
doi = {10.1198/jcgs.2011.203main},
journal = {Journal of Computational and Graphical Statistics},
number = {3},
pages = {531--570},
posted-at = {2012-03-03 18:10:07},
priority = {2},
title = {{To Center or Not to Center: That Is Not the Question--An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2011.203main},
volume = {20},
year = {2011}
}</code></pre>
</div>
<div id="paperkey_1" class="papercite_entry">[2]                   O. Papaspiliopoulos, G. O. Roberts, and M. Sköld, &#8220;A general framework for the parametrization of hierarchical models,&#8221; <span style="font-style: italic">Statistical science</span>, vol. 22, iss. 1, pp. 59-73, 2007. <br />    <a href="javascript:void(0)" id="papercite_1" class="papercite_toggle">[Bibtex]</a></div>
<div class="papercite_bibtex" id="papercite_1_block">
<pre><code class="tex bibtex">@article{Papaspiliopoulos07,
abstract = {{In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.}},
author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sk\&quot;{o}ld, Martin},
citeulike-article-id = {8977350},
citeulike-linkout-0 = {http://www.jstor.org/stable/27645805},
journal = {Statistical Science},
number = {1},
pages = {59--73},
posted-at = {2011-03-10 18:55:50},
priority = {2},
publisher = {Institute of Mathematical Statistics},
title = {{A general framework for the parametrization of hierarchical models}},
url = {http://www.jstor.org/stable/27645805},
volume = {22},
year = {2007}
}</code></pre>
</div>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1198%2Fjcgs.2011.203main&#038;rft.atitle=To+Center+or+Not+to+Center%3A+That+Is+Not+the+Question%E2%80%94An+Ancillarity%E2%80%93Sufficiency+Interweaving+Strategy+%28ASIS%29+for+Boosting+MCMC+Efficiency&#038;rft.jtitle=Journal+of+Computational+and+Graphical+Statistics&#038;rft.artnum=http%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1198%2Fjcgs.2011.203main&#038;rft.volume=20&#038;rft.issue=3&#038;rft.issn=1061-8600&#038;rft.spage=531&#038;rft.epage=570&#038;rft.date=2011&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Yu+Yaming&#038;rft.aulast=Yu&#038;rft.aufirst=Yaming&#038;rft.au=Meng+Xiao-Li&#038;rft.aulast=Meng&#038;rft.aufirst=Xiao-Li&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Computer+Science+%2F+Engineering%2CMathematics%2COther">Yu Y. &#038; Meng X.L. (2011). To Center or Not to Center: That Is Not the Question—An Ancillarity–Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency, <span style="font-style:italic;">Journal of Computational and Graphical Statistics, 20</span> (3) 531-570. DOI: <a rel="author" href="http://dx.doi.org/10.1198%2Fjcgs.2011.203main">10.1198/jcgs.2011.203main</a></span></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-762 -->

				
					
<article id="post-663" class="grid_6 post-663 post type-post status-publish format-standard hentry category-linear-algebra category-mathematics category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1248 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/woodbury-matrix-inverse-multivariate-normal/index.html#comments"><span class="dsq-postid" data-dsqidentifier="663 http://www.lindonslog.com/?p=663">3 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/woodbury-matrix-inverse-multivariate-normal/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Woodbury Matrix Inverse Identity</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<h1>Application in Conditional Distribution of Multivariate Normal</h1>
<p>The Sherman-Woodbury-Morrison matrix inverse identity can be regarded as a transform between Schur complements. That is, given <img src="http://s0.wp.com/latex.php?latex=V_%7B22.1%7D%5E%7B-1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{22.1}^{-1}" title="V_{22.1}^{-1}" class="latex" /> one can obtain <img src="http://s0.wp.com/latex.php?latex=V_%7B11.2%7D%5E%7B-1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{11.2}^{-1}" title="V_{11.2}^{-1}" class="latex" /> by using the Woodbury matrix identity and vice versa. Recall the <a title="Invert Matrix Woodbury Matrix Inverse Formula Identity" href="../../../../mathematics/invert-a-matrix-using-the-woodbury-matrix-inverse-formula-identity/index.html" target="_blank">Woodbury Identity</a>:</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=V_%7B11.2%7D%5E%7B-1%7D%3DV_%7B11%7D%5E%7B-1%7D%2BV_%7B11%7D%5E%7B-1%7DV_%7B12%7DV_%7B22.1%7D%5E%7B-1%7DV_%7B21%7DV_%7B11%7D%5E%7B-1%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{11.2}^{-1}=V_{11}^{-1}+V_{11}^{-1}V_{12}V_{22.1}^{-1}V_{21}V_{11}^{-1} " title="V_{11.2}^{-1}=V_{11}^{-1}+V_{11}^{-1}V_{12}V_{22.1}^{-1}V_{21}V_{11}^{-1} " class="latex" /></p>
<p>and</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=V_%7B22.1%7D%5E%7B-1%7D%3DV_%7B22%7D%5E%7B-1%7D%2BV_%7B22%7D%5E%7B-1%7DV_%7B21%7DV_%7B11.2%7D%5E%7B-1%7DV_%7B12%7DV_%7B22%7D%5E%7B-1%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{22.1}^{-1}=V_{22}^{-1}+V_{22}^{-1}V_{21}V_{11.2}^{-1}V_{12}V_{22}^{-1} " title="V_{22.1}^{-1}=V_{22}^{-1}+V_{22}^{-1}V_{21}V_{11.2}^{-1}V_{12}V_{22}^{-1} " class="latex" /></p>
<p>I recently stumbled across a neat application of this whilst deriving full conditionals for a multivariate normal. Recall that if the data are partitioned into two blocks, <img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%2CY_%7B2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1},Y_{2}" title="Y_{1},Y_{2}" class="latex" />, then the variance of the conditional distribution <img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%7CY_%7B2%7D%2C-&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}|Y_{2},-" title="Y_{1}|Y_{2},-" class="latex" /> is the Schur complement of the block <img src="http://s0.wp.com/latex.php?latex=V_%7B22%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{22}" title="V_{22}" class="latex" /> of total variance matrix <img src="http://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V" title="V" class="latex" />, that is, the variance of the conditional distribution is <img src="http://s0.wp.com/latex.php?latex=V_%7B11.2%7D%3DV_%7B11%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DV_%7B21%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{11.2}=V_{11}-V_{12}V_{22}^{-1}V_{21}" title="V_{11.2}=V_{11}-V_{12}V_{22}^{-1}V_{21}" class="latex" /> which is the variance of <img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}" title="Y_{1}" class="latex" /> subtracted by something corresponding to the reduction in uncertainty about <img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}" title="Y_{1}" class="latex" /> gained from the knowledge about <img src="http://s0.wp.com/latex.php?latex=Y_%7B2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{2}" title="Y_{2}" class="latex" />. If, however, <img src="http://s0.wp.com/latex.php?latex=V_%7B22%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{22}" title="V_{22}" class="latex" /> has the form of a Schur complement itself, then it may be possible to exploit the Woodbury identity above to considerably simplify the variance term. I came across this when I derived two very different-looking expressions for the conditional distribution and found them equivalent by the Woodbury identity. Consider the model</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+++Y_%7B1%7D%5C%5C+++Y_%7B2%7D++%5Cend%7Bbmatrix%7D++%3D++%5Cbegin%7Bbmatrix%7D+++X_%7B1%7D%5C%5C+++X_%7B2%7D++%5Cend%7Bbmatrix%7D%5Cbeta_%7B+%7D+%2B+%5Cvarepsilon++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{bmatrix}   Y_{1}&#92;&#92;   Y_{2}  &#92;end{bmatrix}  =  &#92;begin{bmatrix}   X_{1}&#92;&#92;   X_{2}  &#92;end{bmatrix}&#92;beta_{ } + &#92;varepsilon  " title="&#92;begin{bmatrix}   Y_{1}&#92;&#92;   Y_{2}  &#92;end{bmatrix}  =  &#92;begin{bmatrix}   X_{1}&#92;&#92;   X_{2}  &#92;end{bmatrix}&#92;beta_{ } + &#92;varepsilon  " class="latex" /></p>
<p>where</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cvarepsilon+%5Csim+N%5Cleft%28+%5Cbegin%7Bbmatrix%7D0%5C%5C+0%5Cend%7Bbmatrix%7D%2C+%5Csigma%5E%7B2%7D+%5Cbegin%7Bbmatrix%7DI_%7B1%7D+%26+0+%5C%5C+0+%26+I_%7B2%7D%5Cend%7Bbmatrix%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;varepsilon &#92;sim N&#92;left( &#92;begin{bmatrix}0&#92;&#92; 0&#92;end{bmatrix}, &#92;sigma^{2} &#92;begin{bmatrix}I_{1} &amp; 0 &#92;&#92; 0 &amp; I_{2}&#92;end{bmatrix} &#92;right) " title="&#92;varepsilon &#92;sim N&#92;left( &#92;begin{bmatrix}0&#92;&#92; 0&#92;end{bmatrix}, &#92;sigma^{2} &#92;begin{bmatrix}I_{1} &amp; 0 &#92;&#92; 0 &amp; I_{2}&#92;end{bmatrix} &#92;right) " class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cbeta_%7B+%7D%7C+%2C%5Csigma%5E%7B2%7D+%5Csim+N%280%2C+%5Csigma%5E%7B2%7D%5CLambda%5E%7B-1%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;beta_{ }| ,&#92;sigma^{2} &#92;sim N(0, &#92;sigma^{2}&#92;Lambda^{-1})" title="&#92;beta_{ }| ,&#92;sigma^{2} &#92;sim N(0, &#92;sigma^{2}&#92;Lambda^{-1})" class="latex" /></p>
<p>.<br />
I was seeking the distribution <img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%7C+Y_%7B2%7D%2C%5Csigma%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}| Y_{2},&#92;sigma^{2} " title="Y_{1}| Y_{2},&#92;sigma^{2} " class="latex" /> and arrived there through two different paths. The distributions derived looked very different, but they turned out to be equivalent upon considering the Woodbury identity.</p>
<h2>Method 1</h2>
<p>This simply manipulates properties of the multivariate normal. Marginalizing over <img src="http://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;beta" title="&#92;beta" class="latex" /> one gets</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=Cov+%5Cbegin%7Bbmatrix%7D+++Y_%7B1%7D+%5C%5C++Y_%7B2%7D++%5Cend%7Bbmatrix%7D+%3D+%5Cbegin%7Bbmatrix%7D+++X_%7B1%7D+%5C%5C++X_%7B2%7D++%5Cend%7Bbmatrix%7D+Cov+%28%5Cbeta_%7B+%7D%29++%5Cbegin%7Bbmatrix%7D+++X_%7B1%7D%5E%7BT%7D+%26++X_%7B2%7D%5E%7BT%7D++%5Cend%7Bbmatrix%7D+%2B+Cov%28%5Cvarepsilon%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Cov &#92;begin{bmatrix}   Y_{1} &#92;&#92;  Y_{2}  &#92;end{bmatrix} = &#92;begin{bmatrix}   X_{1} &#92;&#92;  X_{2}  &#92;end{bmatrix} Cov (&#92;beta_{ })  &#92;begin{bmatrix}   X_{1}^{T} &amp;  X_{2}^{T}  &#92;end{bmatrix} + Cov(&#92;varepsilon)  " title="Cov &#92;begin{bmatrix}   Y_{1} &#92;&#92;  Y_{2}  &#92;end{bmatrix} = &#92;begin{bmatrix}   X_{1} &#92;&#92;  X_{2}  &#92;end{bmatrix} Cov (&#92;beta_{ })  &#92;begin{bmatrix}   X_{1}^{T} &amp;  X_{2}^{T}  &#92;end{bmatrix} + Cov(&#92;varepsilon)  " class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=Cov+%5Cbegin%7Bbmatrix%7D+++Y_%7B1%7D+%5C%5C++Y_%7B2%7D++%5Cend%7Bbmatrix%7D+%3D+%5Csigma%5E%7B2%7D%5Cbegin%7Bbmatrix%7D+++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D+%26++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5C%5C++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D+%26++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D++%5Cend%7Bbmatrix%7D++%2B+%5Csigma%5E%7B2%7D++%5Cbegin%7Bbmatrix%7D++I_%7B1%7D+%26+0+%5C%5C+0+%26+I_%7B2%7D++%5Cend%7Bbmatrix%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Cov &#92;begin{bmatrix}   Y_{1} &#92;&#92;  Y_{2}  &#92;end{bmatrix} = &#92;sigma^{2}&#92;begin{bmatrix}   X_{1}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;&#92;  X_{2}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{2}&#92;Lambda^{-1} X_{2}^{T}  &#92;end{bmatrix}  + &#92;sigma^{2}  &#92;begin{bmatrix}  I_{1} &amp; 0 &#92;&#92; 0 &amp; I_{2}  &#92;end{bmatrix}  " title="Cov &#92;begin{bmatrix}   Y_{1} &#92;&#92;  Y_{2}  &#92;end{bmatrix} = &#92;sigma^{2}&#92;begin{bmatrix}   X_{1}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;&#92;  X_{2}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{2}&#92;Lambda^{-1} X_{2}^{T}  &#92;end{bmatrix}  + &#92;sigma^{2}  &#92;begin{bmatrix}  I_{1} &amp; 0 &#92;&#92; 0 &amp; I_{2}  &#92;end{bmatrix}  " class="latex" /></p>
<p>.<br />
Such that the distribution</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+++Y_%7B1%7D%5C%5C+++Y_%7B2%7D++%5Cend%7Bbmatrix%7D%7C+%2C%5Csigma%5E%7B2%7D+%5Csim+N+%5Cleft%28++%5Cbegin%7Bbmatrix%7D++0%5C%5C++0++%5Cend%7Bbmatrix%7D%2C++%5Csigma%5E%7B2%7D+%5Cleft%28+%5Cbegin%7Bbmatrix%7D++I_%7B1%7D%2B++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D+%26++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5C%5C++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D+%26+I_%7B2%7D%2B+X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D++%5Cend%7Bbmatrix%7D++%5Cright%29+%5Cright%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{bmatrix}   Y_{1}&#92;&#92;   Y_{2}  &#92;end{bmatrix}| ,&#92;sigma^{2} &#92;sim N &#92;left(  &#92;begin{bmatrix}  0&#92;&#92;  0  &#92;end{bmatrix},  &#92;sigma^{2} &#92;left( &#92;begin{bmatrix}  I_{1}+  X_{1}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;&#92;  X_{2}&#92;Lambda^{-1} X_{1}^{T} &amp; I_{2}+ X_{2}&#92;Lambda^{-1} X_{2}^{T}  &#92;end{bmatrix}  &#92;right) &#92;right)  " title="&#92;begin{bmatrix}   Y_{1}&#92;&#92;   Y_{2}  &#92;end{bmatrix}| ,&#92;sigma^{2} &#92;sim N &#92;left(  &#92;begin{bmatrix}  0&#92;&#92;  0  &#92;end{bmatrix},  &#92;sigma^{2} &#92;left( &#92;begin{bmatrix}  I_{1}+  X_{1}&#92;Lambda^{-1} X_{1}^{T} &amp;  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;&#92;  X_{2}&#92;Lambda^{-1} X_{1}^{T} &amp; I_{2}+ X_{2}&#92;Lambda^{-1} X_{2}^{T}  &#92;end{bmatrix}  &#92;right) &#92;right)  " class="latex" /></p>
<p>It follows that the conditional distribution is<br />
<img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%7C+Y_%7B2%7D+%2C%5Csigma%5E%7B2%7D+%5Csim+N+%5Cleft%28++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cleft%5B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%2B+I_%7B2%7D%5Cright%5D%5E%7B-1%7D+Y_%7B2%7D%2C+I_%7B1%7D+%2B++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D+-++X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cleft%5B+I_%7B2%7D+%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cright%5D%5E%7B-1%7D++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D%5Cright%29.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}| Y_{2} ,&#92;sigma^{2} &#92;sim N &#92;left(  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[  X_{2}&#92;Lambda^{-1} X_{2}^{T} + I_{2}&#92;right]^{-1} Y_{2}, I_{1} +  X_{1}&#92;Lambda^{-1} X_{1}^{T} -  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2} +  X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} X_{1}^{T}&#92;right).  " title="Y_{1}| Y_{2} ,&#92;sigma^{2} &#92;sim N &#92;left(  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[  X_{2}&#92;Lambda^{-1} X_{2}^{T} + I_{2}&#92;right]^{-1} Y_{2}, I_{1} +  X_{1}&#92;Lambda^{-1} X_{1}^{T} -  X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2} +  X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} X_{1}^{T}&#92;right).  " class="latex" /><br />
This looks a bit nasty, but notice that <img src="http://s0.wp.com/latex.php?latex=V_%7B22%7D%5E%7B-1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="V_{22}^{-1}" title="V_{22}^{-1}" class="latex" /> looks like it too could be a Schur complement of some matrix.</p>
<h2>Method 2</h2>
<p>An alternative route to this distribution is</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=f%28+Y_%7B1%7D%7C+Y_%7B2%7D%2C%5Csigma%5E%7B2%7D+%29%3D%5Cint+f%28+Y_%7B1%7D%7C%5Csigma%5E%7B2%7D+%2C%5Cbeta_%7B+%7D%29%5Cpi%28%5Cbeta_%7B+%7D%7C+Y_%7B2%7D%2C%5Csigma%5E%7B2%7D+%29d%5Cbeta_%7B+%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="f( Y_{1}| Y_{2},&#92;sigma^{2} )=&#92;int f( Y_{1}|&#92;sigma^{2} ,&#92;beta_{ })&#92;pi(&#92;beta_{ }| Y_{2},&#92;sigma^{2} )d&#92;beta_{ }  " title="f( Y_{1}| Y_{2},&#92;sigma^{2} )=&#92;int f( Y_{1}|&#92;sigma^{2} ,&#92;beta_{ })&#92;pi(&#92;beta_{ }| Y_{2},&#92;sigma^{2} )d&#92;beta_{ }  " class="latex" /></p>
<p>where</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cbeta_%7B+%7D%7C+Y_%7B2%7D+%2C%5Csigma%5E%7B2%7D%5Csim+N+%5Cleft%28+%28+X_%7B2%7D%5E%7BT%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+Y_%7B2%7D%2C+%5Csigma%5E%7B2%7D%28+X_%7B2%7D%5E%7BT%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D+%5Cright%29.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;beta_{ }| Y_{2} ,&#92;sigma^{2}&#92;sim N &#92;left( ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{2}^{T} Y_{2}, &#92;sigma^{2}( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} &#92;right).  " title="&#92;beta_{ }| Y_{2} ,&#92;sigma^{2}&#92;sim N &#92;left( ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{2}^{T} Y_{2}, &#92;sigma^{2}( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} &#92;right).  " class="latex" /></p>
<p>It follows that</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%7C+Y_%7B2%7D+%2C%5Csigma%5E%7B2%7D+%5Csim+N%5Cleft%28++X_%7B1%7D%28+X_%7B2%7D%5E%7BT%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+Y_%7B2%7D%2C+%5Csigma%5E%7B2%7D+%28I_%7B1%7D+%2B++X_%7B1%7D+%28+X_%7B2%7D%5E%7BT%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D%29+%5Cright%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}| Y_{2} ,&#92;sigma^{2} &#92;sim N&#92;left(  X_{1}( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{2}^{T} Y_{2}, &#92;sigma^{2} (I_{1} +  X_{1} ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{1}^{T}) &#92;right)  " title="Y_{1}| Y_{2} ,&#92;sigma^{2} &#92;sim N&#92;left(  X_{1}( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{2}^{T} Y_{2}, &#92;sigma^{2} (I_{1} +  X_{1} ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{1}^{T}) &#92;right)  " class="latex" /></p>
<p>which looks different from the distribution obtained through method 1. The expression for the variance is a lot neater. They are in fact identical by the Woodbury identity.</p>
<h2>Comparison</h2>
<h3>Mean (Submitted by Michelle Leigh)</h3>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cleft%5B%5CLambda%2B++X_%7B2%7D%5ETI_%7B2%7D+X_%7B2%7D%5Cright%5D%5E%7B-1%7D+X_%7B2%7D%5ET%5C%5C++%3D%5C%7B%5CLambda%5E%7B-1%7D-%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cleft%5BI_%7B2%7D%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cright%5D%5E%7B-1%7D+X_%7B2%7D%5CLambda%5E%7B-1%7D%5C%7D+X_%7B2%7D%5ET%5C%5C++%3D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cleft%5BI_%7B2%7D%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cright%5D%5E%7B-1%7D%5Cleft%5BI_%7B2%7D%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cright%5D-%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cleft%5BI_%7B2%7D%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cright%5D%5E%7B-1%7D+X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5C%5C++%3D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cleft%5BI_%7B2%7D%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5ET%5Cright%5D%5E%7B-1%7DI_%7B2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;left[&#92;Lambda+  X_{2}^TI_{2} X_{2}&#92;right]^{-1} X_{2}^T&#92;&#92;  =&#92;{&#92;Lambda^{-1}-&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1} X_{2}&#92;Lambda^{-1}&#92;} X_{2}^T&#92;&#92;  =&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1}&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]-&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1} X_{2}&#92;Lambda^{-1} X_{2}^T&#92;&#92;  =&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1}I_{2}" title="&#92;left[&#92;Lambda+  X_{2}^TI_{2} X_{2}&#92;right]^{-1} X_{2}^T&#92;&#92;  =&#92;{&#92;Lambda^{-1}-&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1} X_{2}&#92;Lambda^{-1}&#92;} X_{2}^T&#92;&#92;  =&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1}&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]-&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1} X_{2}&#92;Lambda^{-1} X_{2}^T&#92;&#92;  =&#92;Lambda^{-1} X_{2}^T&#92;left[I_{2}+  X_{2}&#92;Lambda^{-1} X_{2}^T&#92;right]^{-1}I_{2}" class="latex" /></p>
<p>So mean1=mean2.</p>
<h3>Variance</h3>
<p>By the Woodbury Identity it follows that</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5CLambda%5E%7B-1%7D+-+%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cleft%5B+I_%7B2%7D+%2B++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cright%5D%5E%7B-1%7D++X_%7B2%7D%5CLambda%5E%7B-1%7D+%3D+%28+X_%7B2%7D%5E%7BT%7DI_%7B2%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;Lambda^{-1} - &#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2} +  X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} = ( X_{2}^{T}I_{2} X_{2}+&#92;Lambda)^{-1}.  " title="&#92;Lambda^{-1} - &#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2} +  X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} = ( X_{2}^{T}I_{2} X_{2}+&#92;Lambda)^{-1}.  " class="latex" /></p>
<p>Therefore</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D-+X_%7B1%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cleft%5B+I_%7B2%7D%2B+X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B2%7D%5E%7BT%7D+%5Cright%5D%5E%7B-1%7D++X_%7B2%7D%5CLambda%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D%3D%7B+X_%7B1%7D+%28+X_%7B2%7D%5E%7BT%7D+X_%7B2%7D%2B%5CLambda%29%5E%7B-1%7D+X_%7B1%7D%5E%7BT%7D%7D%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="X_{1}&#92;Lambda^{-1} X_{1}^{T}- X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2}+ X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} X_{1}^{T}={ X_{1} ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{1}^{T}}&#92;&#92;  " title="X_{1}&#92;Lambda^{-1} X_{1}^{T}- X_{1}&#92;Lambda^{-1} X_{2}^{T} &#92;left[ I_{2}+ X_{2}&#92;Lambda^{-1} X_{2}^{T} &#92;right]^{-1}  X_{2}&#92;Lambda^{-1} X_{1}^{T}={ X_{1} ( X_{2}^{T} X_{2}+&#92;Lambda)^{-1} X_{1}^{T}}&#92;&#92;  " class="latex" /></p>
<p>and so variance1=variance2. The trick is recognizing the form of the formulas at the top of the page, then one can write the variance as a much neater expression.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-663 -->

				
					
<article id="post-605" class="grid_6 post-605 post type-post status-publish format-standard hentry category-openmp category-programming category-statistics tag-c tag-mcmc tag-openmp-2 tag-parallel-tempering">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1265 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/openmp/parallel-tempering-algorithm-c/index.html#comments"><span class="dsq-postid" data-dsqidentifier="605 http://www.lindonslog.com/?p=605">2 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/openmp/parallel-tempering-algorithm-c/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Parallel Tempering Algorithm with OpenMP / C++</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p><a href="index.html#theory">1.1. Parallel Tempering Theory</a><br />
<a href="index.html#physics">1.2. Physics Origins</a><br />
<a href="index.html#intra">2.1 Intra-Thread Metropolis Move</a><br />
<a href="index.html#inter">2.2. Inter-Thread Parallel Tempering</a><br />
<a href="index.html#openmp">2.3. OpenMP Parallelization</a><br />
<a href="index.html#fullcode">3. Full Code</a><br />
<a href="index.html#simstudy">4. Simulation Study<br />
<a href="index.html#futureuse">5. On the Future use of Parallel Tempering with OpenMP</a></p>
<p>Parallel tempering is one of my favourite sampling algorithms to improve MCMC mixing times. This algorithm seems to be used <em>exclusively</em> on distributed memory architectures using MPI and remains unexploited on shared memory architectures such as our office computers, which have up to eight cores. I&#8217;ve written parallel tempering algorithms in MPI and Rmpi but never in OpenMP. It turns out that the latter has substantial advantages. I guess when people think of parallel tempering they think of processors communicating with each other via MPI and swapping parameters directly.  If you are on a shared memory device, however, you can have processor A simply write to a shared array and have processor B read therefrom, which really saves a lot of aggro fiddling around with message numbers, blocking/non-blocking calls and deadlocks etc. Moreover, with OpenMP you can spawn more threads than you have processors, which translates to more parallel MCMC chains in the present context, whereas this becomes troublesome with MPI due to the danger of deadlocks. OpenMP is also much easier to use than MPI, with one line you can fork a serial thread into a desired and hardware-independent  number of parallel threads. The code looks as follows:</p>
<p><a name="theory"></a><br />
<h2>Parallel Tempering Theory</h2>
<p>Each thread simulates an MCMC trajectory from the posterior raised to a fractional power, B. When B=1, the MCMC draws are from the posterior from which we wish to sample. When B=0, the MCMC trajectory is just a realization of a Brownian motion random walk. To see this, consider the acceptance probability of the metropolis move. The density evaluated at the proposed parameters over the density evaluated at the current parameters all raised to the power of zero is unity, whatever the densities are, so the moves always get accepted. Similarly if B is close to zero, then the acceptance probability is near unity and the distribution from which this MCMC is sampling is quite uniform over the parameter space, so the trajectory explores a relatively larger part of the parameter space. As B is increased toward one, the features of the distribution from which we wish to sample start to become more prominent. In the other direction from B=1 to 0 one commonly says that the posterior is &#8220;melted down&#8221; and spreading out its mass. The terminology has remained from its origins in statistical physics where one would simulated particles at a hotter temperature, so that they would jostle around more and escape wells in the potential energy. The key to parallel tempering is to use the more diffuse, hotter or melted down MCMC chains as proposal distributions for the actual cold distribution we wish to sample from. One proceeds by performing a Metropolis-Hastings move because the proposal distributions are not symmetric. For illustration, thread j uses the hotter thread j+1 as its partner and as proposal distribution. Let theta j+1 be the proposed new position for thread j, being the current position of thread j+1.<br />
<img src="http://s0.wp.com/latex.php?latex=%5Calpha%3Dmin%281%2C%5Cfrac%7B++p_%7Bj%7D+%28%5Ctheta_%7Bj%2B1%7D+%29%7D++%7B+++p_%7Bj%7D%28%5Ctheta_%7Bj%7D+%29+%7D++%5Cfrac%7B++p_%7Bj%2B1%7D+%28%5Ctheta_%7Bj%7D+%29%7D++%7B+++p_%7Bj%2B1%7D%28%5Ctheta_%7Bj%2B1%7D+%29+%7D++++%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;alpha=min(1,&#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    )  " title="&#92;alpha=min(1,&#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    )  " class="latex" /><br />
The second fraction is the Hastings addition to the Metropolis algorithm and is required to satisfy detailed balance for an unsymmetrical proposal distribution. Now realise that<br />
<img src="http://s0.wp.com/latex.php?latex=p_%7Bj%7D%3D%5Cpi%28%5Ctheta%7CY%29%5E%7BB_%7Bj%7D%7D%5C%5C++p_%7Bj%2B1%7D%3D%5Cpi%28%5Ctheta%7CY%29%5E%7BB_%7Bj%2B1%7D%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="p_{j}=&#92;pi(&#92;theta|Y)^{B_{j}}&#92;&#92;  p_{j+1}=&#92;pi(&#92;theta|Y)^{B_{j+1}}  " title="p_{j}=&#92;pi(&#92;theta|Y)^{B_{j}}&#92;&#92;  p_{j+1}=&#92;pi(&#92;theta|Y)^{B_{j+1}}  " class="latex" /><br />
i.e. they are the same distribution raised to different fractional powers. Working now on the log scale, it can be shown that<br />
<img src="http://s0.wp.com/latex.php?latex=log+%5Cleft%28+%5Cfrac%7B++p_%7Bj%7D+%28%5Ctheta_%7Bj%2B1%7D+%29%7D++%7B+++p_%7Bj%7D%28%5Ctheta_%7Bj%7D+%29+%7D++%5Cfrac%7B++p_%7Bj%2B1%7D+%28%5Ctheta_%7Bj%7D+%29%7D++%7B+++p_%7Bj%2B1%7D%28%5Ctheta_%7Bj%2B1%7D+%29+%7D++++%5Cright%29+%3D++%28B_%7Bj%7D-B_%7Bj%2B1%7D%29+%5Cleft%28+log%28%5Cpi%5B%5Ctheta_%7Bj%2B1%7D%7CY%5D%29+-+log%28%5Cpi%5B%5Ctheta_%7Bj%7D%7CY%5D%29+%5Cright%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="log &#92;left( &#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    &#92;right) =  (B_{j}-B_{j+1}) &#92;left( log(&#92;pi[&#92;theta_{j+1}|Y]) - log(&#92;pi[&#92;theta_{j}|Y]) &#92;right)  " title="log &#92;left( &#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    &#92;right) =  (B_{j}-B_{j+1}) &#92;left( log(&#92;pi[&#92;theta_{j+1}|Y]) - log(&#92;pi[&#92;theta_{j}|Y]) &#92;right)  " class="latex" /><br />
<a name="physics"></a><br />
<h3>Physics Origins</h3>
<p>It is at this point where sometimes, in order to make things correspond to the earlier physics literature, one defines the &#8220;Energy&#8221; as<br />
 <img src="http://s0.wp.com/latex.php?latex=E_%7Bj%7D%3D-log%28%5Cpi%5B%5Ctheta_%7Bj%7D%7CY%5D%29.&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="E_{j}=-log(&#92;pi[&#92;theta_{j}|Y])." title="E_{j}=-log(&#92;pi[&#92;theta_{j}|Y])." class="latex" /><br />
So that the acceptance probability becomes<br />
<img src="http://s0.wp.com/latex.php?latex=%5Calpha%3Dmin%281%2Ce%5E%7B-%28B_%7Bj%7D-B_%7Bj%2B1%7D%29%28E_%7Bj%2B1%7D+-+E_%7Bj%7D%29++%7D%29.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;alpha=min(1,e^{-(B_{j}-B_{j+1})(E_{j+1} - E_{j})  }).  " title="&#92;alpha=min(1,e^{-(B_{j}-B_{j+1})(E_{j+1} - E_{j})  }).  " class="latex" /><br />
It&#8217;s not necessary to define this energy, it only defines an equivalence mapping between statistics and physics. In physics particles get stuck in the local minima of the energy landscape and in statistics the MCMC gets stuck in the local peaks of the posterior. The reason for this is that in a canonical ensemble lower energy states are more probable (recall that nature tries to minimize the potential energy and that force is the negative gradient of the potential energy), so regions of the parameter space with low potential energy, physically, correspond to regions of high probability density, statistically. To be more precise, a result from statistical physics is that the distribution of energy is exponential with scale parameter kT, where k is Boltzmann&#8217;s constant and T is temperature (this condition holds only for a canonical ensemble). An exponential distribution with this scale parameter is called the Boltzmann distribution by physicists. As the temperature increases, higher energy states become more probable and the particle jumps out of the minima more. If you are a statistician you don&#8217;t need to worry about this, but sometimes this notation crops up in the literature. Its also the same acceptance probability now as in physics when sampling energies from a Boltzmann distribution. I have decided not to adopt the physics notation for this post.</p>
<p><a name="intra"></a><br />
<h2>Intra-Thread Metropolis Move</h2>
<p>Each thread, within itself, performs a normal vanilla metropolis move:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
//Propose Candidate Position//
			t1new=t1[rank*nmc+i-1] + normal(stream[rank]);
			t2new=t2[rank*nmc+i-1] + normal(stream[rank]);

			//Calculate log-Density at Newly-Proposed and Current Position//
			lpost_new[rank]=lLikelihood(t1new,t2new) + lprior(t1new,t2new);
			lpost[rank]=lLikelihood(t1[rank*nmc+i-1],t2[rank*nmc+i-1]) + lprior(t1[rank*nmc+i-1],t2[rank*nmc+i-1]);

			//Melt Density and Calculate log-Acceptance Probability//
			lalpha=B[rank]*(lpost_new[rank]-lpost[rank]);

			//Perform Metropolis Accept-Reject Step//
			if( log(u(stream[rank])) &lt; lalpha ){
				//Accept
				//Proposed as Current Position
				t1[rank*nmc+i]=t1new;
				t2[rank*nmc+i]=t2new;
			}else{
				//Do not Accept
				//Propogate Current Position
				t1[rank*nmc+i]=t1[rank*nmc+i-1];
				t2[rank*nmc+i]=t2[rank*nmc+i-1];
			}
</pre>
<p>A few comments about the variables. &#8220;nmc&#8221; is the number of mcmc draws I wish to generate. I have two parameters which I have denoted t1 and t2, because t is closest to theta. Moreover, each processor stores its <em>nmc</em> draws of t1 and t2 in a contiguous array in the memory of length nmc times number of threads. &#8220;Rank&#8221; Identifies the thread and &#8220;lpost&#8221; and &#8220;B&#8221; are arrays of length equal to the number of threads in which to store the log posterior density at the current position and the fractional melting power. All of these variables are defined at the top of the code.</p>
<p><a name="inter"></a><br />
<h2>Inter-Thread Metropolis-Hastings Move</h2>
<pre class="brush: cpp; title: ; notranslate" title="">

				if(u(stream[rank]) &lt; 0.5){
					rank_partner=rank+1;
					if(rank_partner &lt; size){
						//Inter-Thread Metropolis-Hastings Part
						lalpha = (B[rank]-B[rank_partner])*(lpost[rank_partner]-lpost[rank]);
						if(log(u(stream[rank])) &lt; lalpha){
							//accept swap
							swap(t1[rank*nmc+i],t1[rank_partner*nmc+i]);
							swap(t2[rank*nmc+i],t2[rank_partner*nmc+i]);
						}

					}
				}
</pre>
<p>The only additional thing to add is that each chain attempts a swap with its neighbour at each iteration with probability 1/2. There is nothing special about 1/2, you could choose what you like, but there are pros and cons. How this made parallel in OpenMP is shown below.</p>
<p><a name="openmp"></a><br />
<h2>OpenMP Parallelization</h2>
<p>The OpenMP parallel implementation of the above algorithm is very simple!</p>
<pre class="brush: plain; title: ; notranslate" title="">
#pragma omp parallel private(i,t1new,t2new,rank,lalpha,rank_partner) shared(B, lpost, lpost_new,t1,t2,swapt1,swapt2)
	{
		//Identify Each Thread
		rank=omp_get_thread_num();

		for (i = 1; i &lt; nmc; ++i)
		{

                 //***Intra-Thread Metropolis Part***//
	
#pragma omp barrier      //Synchronise Threads
#pragma omp critical     //Executed Critical Code Block Oney Thread at a Time. 
			{

                 //***Inter-Thread Parallel Tempering Part***//

			}
#pragma omp barrier   //Synchronise Threads
		}
	}
</pre>
<p>The first parallel pragma simply forks the master thread into a number of threads whereby each thread executes the following code block independently i.e. a number of independent parallel mcmcs. Specifying variables as private means that each thread gets a copy of that variable in its own seperate location in the memory. Shared is the opposite, although I think variables are shared by default. The barrier pragma means that each thread halts until all threads have reached this point. The critical pragma means the following code block is executed by one thread at a time only. This prevents thread j swapping with thread j+1 whilst thread j+1 is attempting a swap with thread j+2, nasty things such as race conditions can occur. The last pragma barrier waits for all threads to have reached the end and then the next iteration of the for loop proceeds.</p>
<p><a name="fullcode"></a><br />
<h2>Full code</h2>
<p>The full code can be found <a href="../../../../example_code/tempering.cpp">here</a>. It depends on <a href="../../../programming/openmp/index.html">OpenMP</a> and the <a href="../../../../programming/parallel-random-number-generation-trng/index.html" title="Parallel Random Number Generation using TRNG">TRNG</a> library in order to generate multiple independent streams of random numbers. It takes the number of mcmc draws as a command-line argument.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael tempering]$ wget http://www.lindonslog.com/example_code/tempering.cpp
[michael@michael tempering]$ g++ tempering.cpp -fopenmp -o tempering  -ltrng4 -lm
[michael@michael tempering]$ ./tempering 10000
Thread 0 has fractional power 1
Thread 1 has fractional power 0.469117
Thread 2 has fractional power 0.220071
Thread 3 has fractional power 0.103239
Thread 4 has fractional power 0.0484313
Thread 5 has fractional power 0.0227199
Thread 6 has fractional power 0.0106583
Thread 7 has fractional power 0.005
[michael@michael tempering]$
</pre>
<p><a name="simstudy"></a><br />
<h2>Simulation Study</h2>
<p>I chose the likelihood to be 5 sharply peaked normal distributions located at the corners of a sort-of unit square plus one at the origin with variances of 0.001. The prior was a normal of variance 1000 centered at the origin. The parallel tempering algorithm was run with 8 threads. The posterior draws and mixing results are below:<br />
<div id="attachment_632" style="width: 490px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/07/partempdraws.png"><img src="../../../../wp-content/uploads/2013/07/partempdraws.png" alt="Posterior Draws" width="480" height="480" class="size-full wp-image-632" srcset="http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws.png 480w, http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws-300x300.png 300w" sizes="(max-width: 480px) 100vw, 480px" /></a><p class="wp-caption-text">Posterior Draws from Parallel Tempering</p></div><br />
<div id="attachment_630" style="width: 610px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/07/parallel_tempering.png"><img src="../../../../wp-content/uploads/2013/07/parallel_tempering.png" alt="parallel tempering mixing" width="600" height="900" class="size-full wp-image-630" srcset="http://www.lindonslog.com/wp-content/uploads/2013/07/parallel_tempering.png 600w, http://www.lindonslog.com/wp-content/uploads/2013/07/parallel_tempering-200x300.png 200w" sizes="(max-width: 600px) 100vw, 600px" /></a><p class="wp-caption-text">Mixing of parallel tempering algorithm</p></div></p>
<p><a name="futureuse"></a><br />
<h2>On the Future use of Parallel Tempering with OpenMP</h2>
<p>I hope the code exemplifies how easy it is to run parallel MCMC chains with OpenMP. I would argue that the metropolis moves are the hardest part. If you can write them for a single serial chain, then it is only a few extra steps to run parallel chains and imlement that parallel tempering algorithm. My laptop has four cores and my office computer has eight. Given the trajectory of technology that shared memory devices have an ever increasing number of cores, it seems to me that parallel tempering is becoming an ever-more valuable algorithm to improve mixing times of MCMC runs. Afterall, had I not used the extra 3 cores on my laptop, they would have remained idle. If you have extra cores, why not use them! Moreover with OpenMP you can spawn as many parallel MCMCs as you desire, avoiding the pitalls of MPI.</p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1039%2Fb509983h&#038;rft.atitle=Parallel+tempering%3A+Theory%2C+applications%2C+and+new+perspectives&#038;rft.jtitle=Physical+Chemistry+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Fxlink.rsc.org%2F%3FDOI%3Db509983h&#038;rft.volume=7&#038;rft.issue=23&#038;rft.issn=1463-9076&#038;rft.spage=3910&#038;rft.date=2005&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Earl+David+J.&#038;rft.aulast=Earl&#038;rft.aufirst=David+J.&#038;rft.au=Deem+Michael+W.&#038;rft.aulast=Deem&#038;rft.aufirst=Michael+W.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CMathematics%2CPhysics">Earl D.J. &#038; Deem M.W. (2005). Parallel tempering: Theory, applications, and new perspectives, <span style="font-style:italic;">Physical Chemistry Chemical Physics, 7</span> (23) 3910. DOI: <a rel="author" href="http://dx.doi.org/10.1039%2Fb509983h">10.1039/b509983h</a></span></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-605 -->

				
					
<article id="post-318" class="grid_6 post-318 post type-post status-publish format-standard hentry category-statistics tag-bayesian tag-linear-model tag-marginal-likelihood tag-model-selection tag-regression">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1293 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/statistics/marginal-likelihood-model-evidence-bayesian-regression/index.html#respond"><span class="dsq-postid" data-dsqidentifier="318 http://www.lindonslog.com/?p=318">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/statistics/marginal-likelihood-model-evidence-bayesian-regression/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Marginal Likelihood and Model Evidence in Bayesian Regression</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>The marginal likelihood or the model evidence is the probability of observing the data given a specific model. This is used in Bayesian model selection and comparison when computing Bayes factor between models, which is simply the ratio of the two respective marginal likelihoods. This can be used to select which covariates to include in a linear model for describing data. Consider the usual normal-inverse gamma prior specification on the regression parameters and variance.</p>
<h2>Normal Inverse-Gamma Priors</h2>
<p><img src="http://s0.wp.com/latex.php?latex=%5Cpi%28%5Cbeta%2C%5Csigma%5E%7B2%7D%29%3D%5Cpi%28%5Cbeta%7C%5Csigma%5E%7B2%7D%29%5Cpi%28%5Csigma%5E%7B2%7D%29%5C%5C++%5Cpi%28%5Cbeta%7C%5Csigma%5E%7B2%7D%29%3DN%28%5Cmu_%7B0%7D%2C%5Csigma%5E%7B2%7D+%5CLambda_%7B0%7D%5E%7B-1%7D%29%5C%5C++%5Cpi%28%5Csigma%5E%7B2%7D%29%3DIG%28a_%7B0%7D%2Cb_%7B0%7D%29%5C%5C++f%28Y%7C%5Cbeta%2C%5Csigma%5E%7B2%7D%29%3DN%28X%5Cbeta%2C%5Csigma%5E%7B2%7D%29%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="&#92;pi(&#92;beta,&#92;sigma^{2})=&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})&#92;&#92;  &#92;pi(&#92;beta|&#92;sigma^{2})=N(&#92;mu_{0},&#92;sigma^{2} &#92;Lambda_{0}^{-1})&#92;&#92;  &#92;pi(&#92;sigma^{2})=IG(a_{0},b_{0})&#92;&#92;  f(Y|&#92;beta,&#92;sigma^{2})=N(X&#92;beta,&#92;sigma^{2})&#92;&#92;  " title="&#92;pi(&#92;beta,&#92;sigma^{2})=&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})&#92;&#92;  &#92;pi(&#92;beta|&#92;sigma^{2})=N(&#92;mu_{0},&#92;sigma^{2} &#92;Lambda_{0}^{-1})&#92;&#92;  &#92;pi(&#92;sigma^{2})=IG(a_{0},b_{0})&#92;&#92;  f(Y|&#92;beta,&#92;sigma^{2})=N(X&#92;beta,&#92;sigma^{2})&#92;&#92;  " class="latex" /></p>
<p>Now construct the joint, out of which the regression coefficients and variance will be integrated.</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28Y%7C%5Cbeta%2C%5Csigma%5E%7B2%7D%29%5Cpi%28%5Cbeta%7C%5Csigma%5E%7B2%7D%29%5Cpi%28%5Csigma%5E%7B2%7D%29%3D++%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%28Y-X%5Cbeta%29%27%28Y-X%5Cbeta%29%7D+++%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D%5Cright%29+%5E%7B%5Cfrac%7Bk%7D%7B2%7D%7D%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D+%5Cright%29%5E%7B%5Cfrac%7Bk%7D%7B2%7D%7D+%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D+e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%28%5Cbeta-%5Cmu_%7B0%7D%29%27%5CLambda_%7B0%7D%28%5Cbeta-%5Cmu_%7B0%7D%29%7D++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7B%5CGamma%28a_%7B0%7D%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7Ba_%7B0%7D-1%7De%5E%7B-%5Cfrac%7Bb_%7B0%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="f(Y|&#92;beta,&#92;sigma^{2})&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})=  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y-X&#92;beta)&#039;(Y-X&#92;beta)}   &#92;left( &#92;frac{1}{2&#92;pi}&#92;right) ^{&#92;frac{k}{2}}&#92;left( &#92;frac{1}{&#92;sigma^{2}} &#92;right)^{&#92;frac{k}{2}} |&#92;Lambda_{0}|^{&#92;frac{1}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(&#92;beta-&#92;mu_{0})&#039;&#92;Lambda_{0}(&#92;beta-&#92;mu_{0})}  &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " title="f(Y|&#92;beta,&#92;sigma^{2})&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})=  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y-X&#92;beta)&#039;(Y-X&#92;beta)}   &#92;left( &#92;frac{1}{2&#92;pi}&#92;right) ^{&#92;frac{k}{2}}&#92;left( &#92;frac{1}{&#92;sigma^{2}} &#92;right)^{&#92;frac{k}{2}} |&#92;Lambda_{0}|^{&#92;frac{1}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(&#92;beta-&#92;mu_{0})&#039;&#92;Lambda_{0}(&#92;beta-&#92;mu_{0})}  &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " class="latex" /></p>
<p>This can be made neater by rearrangement. Consider first the two exponentials above. The exponents can be combined and rewritten in such a manner that a normal kernel in Beta is recognizable. </p>
<p><img src="http://s0.wp.com/latex.php?latex=%28Y-X%5Cbeta%29%27%28Y-X%5Cbeta%29%2B%28%5Cbeta-%5Cmu_%7B0%7D%29%5CLambda_%7B0%7D%28%5Cbeta_%7B0%7D-%5Cmu_%7B0%7D%29%3D%5C%5C++%5Cbeta%27%28X%27X%2B%5CLambda_%7B0%7D%29%5Cbeta-2%5Cbeta%27%28X%27X%5Chat%7B%5Cbeta%7D%2B%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29+++%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%2B%5Chat%7B%5Cbeta%7DX%27X%5Chat%7B%5Cbeta%7D%2BY%27%28I-P%29Y%3D%5C%5C++%5Cbeta%27%28X%27X%2B%5CLambda_%7B0%7D%29%5Cbeta-2%5Cbeta%27%28X%27X%5Chat%7B%5Cbeta%7D%2B%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29+++%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%2BY%27Y++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="(Y-X&#92;beta)&#039;(Y-X&#92;beta)+(&#92;beta-&#92;mu_{0})&#92;Lambda_{0}(&#92;beta_{0}-&#92;mu_{0})=&#92;&#92;  &#92;beta&#039;(X&#039;X+&#92;Lambda_{0})&#92;beta-2&#92;beta&#039;(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})   +&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+&#92;hat{&#92;beta}X&#039;X&#92;hat{&#92;beta}+Y&#039;(I-P)Y=&#92;&#92;  &#92;beta&#039;(X&#039;X+&#92;Lambda_{0})&#92;beta-2&#92;beta&#039;(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})   +&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y  " title="(Y-X&#92;beta)&#039;(Y-X&#92;beta)+(&#92;beta-&#92;mu_{0})&#92;Lambda_{0}(&#92;beta_{0}-&#92;mu_{0})=&#92;&#92;  &#92;beta&#039;(X&#039;X+&#92;Lambda_{0})&#92;beta-2&#92;beta&#039;(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})   +&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+&#92;hat{&#92;beta}X&#039;X&#92;hat{&#92;beta}+Y&#039;(I-P)Y=&#92;&#92;  &#92;beta&#039;(X&#039;X+&#92;Lambda_{0})&#92;beta-2&#92;beta&#039;(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})   +&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y  " class="latex" /></p>
<p>Define:<br />
<img src="http://s0.wp.com/latex.php?latex=%5CLambda_%7Bn%7D%3DX%27X%2B%5CLambda_%7B0%7D%5C%5C++%5Cmu_%7Bn%7D%3D%28X%27X%2B%5CLambda_%7B0%7D%29%5E%7B-1%7D%28X%27X%5Chat%7B%5Cbeta%7D%2B%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29%3D%5CLambda_%7Bn%7D%5E%7B-1%7D%28X%27X%5Chat%7B%5Cbeta%7D%2B%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="&#92;Lambda_{n}=X&#039;X+&#92;Lambda_{0}&#92;&#92;  &#92;mu_{n}=(X&#039;X+&#92;Lambda_{0})^{-1}(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})=&#92;Lambda_{n}^{-1}(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})  " title="&#92;Lambda_{n}=X&#039;X+&#92;Lambda_{0}&#92;&#92;  &#92;mu_{n}=(X&#039;X+&#92;Lambda_{0})^{-1}(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})=&#92;Lambda_{n}^{-1}(X&#039;X&#92;hat{&#92;beta}+&#92;Lambda_{0}&#92;mu_{0})  " class="latex" /><br />
then the above can written as:</p>
<p><img src="http://s0.wp.com/latex.php?latex=%28Y-X%5Cbeta%29%27%28Y-X%5Cbeta%29%2B%28%5Cbeta-%5Cmu_%7B0%7D%29%5CLambda_%7B0%7D%28%5Cbeta_%7B0%7D-%5Cmu_%7B0%7D%29%3D%5C%5C++%5Cbeta%27%5CLambda_%7Bn%7D%5Cbeta-2%5Cbeta%27%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%2BY%27Y%3D%5C%5C++%28%5Cbeta-%5Cmu_%7Bn%7D%29%5CLambda_%7Bn%7D%28%5Cbeta-%5Cmu_%7Bn%7D%29-%5Cmu_%7Bn%7D%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%2BY%27Y%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="(Y-X&#92;beta)&#039;(Y-X&#92;beta)+(&#92;beta-&#92;mu_{0})&#92;Lambda_{0}(&#92;beta_{0}-&#92;mu_{0})=&#92;&#92;  &#92;beta&#039;&#92;Lambda_{n}&#92;beta-2&#92;beta&#039;&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y=&#92;&#92;  (&#92;beta-&#92;mu_{n})&#92;Lambda_{n}(&#92;beta-&#92;mu_{n})-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y&#92;&#92;  " title="(Y-X&#92;beta)&#039;(Y-X&#92;beta)+(&#92;beta-&#92;mu_{0})&#92;Lambda_{0}(&#92;beta_{0}-&#92;mu_{0})=&#92;&#92;  &#92;beta&#039;&#92;Lambda_{n}&#92;beta-2&#92;beta&#039;&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y=&#92;&#92;  (&#92;beta-&#92;mu_{n})&#92;Lambda_{n}(&#92;beta-&#92;mu_{n})-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0}+Y&#039;Y&#92;&#92;  " class="latex" /><br />
where the last line was achieved by completing the square.</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28Y%7C%5Cbeta%2C%5Csigma%5E%7B2%7D%29%5Cpi%28%5Cbeta%7C%5Csigma%5E%7B2%7D%29%5Cpi%28%5Csigma%5E%7B2%7D%29%3Df%28Y%2C%5Cbeta%2C%5Csigma%5E%7B2%7D%29%3D%5C%5C++%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%28Y%27Y-%5Cmu_%7Bn%7D%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29%7D++%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D%5Cright%29+%5E%7B%5Cfrac%7Bk%7D%7B2%7D%7D%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D+%5Cright%29%5E%7B%5Cfrac%7Bk%7D%7B2%7D%7D+%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D+e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%28%5Cbeta-%5Cmu_%7Bn%7D%29%5CLambda_%7Bn%7D%28%5Cbeta-%5Cmu_%7Bn%7D%29%7D+++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7B%5CGamma%28a_%7B0%7D%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7Ba_%7B0%7D-1%7De%5E%7B-%5Cfrac%7Bb_%7B0%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="f(Y|&#92;beta,&#92;sigma^{2})&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})=f(Y,&#92;beta,&#92;sigma^{2})=&#92;&#92;  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}  &#92;left( &#92;frac{1}{2&#92;pi}&#92;right) ^{&#92;frac{k}{2}}&#92;left( &#92;frac{1}{&#92;sigma^{2}} &#92;right)^{&#92;frac{k}{2}} |&#92;Lambda_{0}|^{&#92;frac{1}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(&#92;beta-&#92;mu_{n})&#92;Lambda_{n}(&#92;beta-&#92;mu_{n})}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " title="f(Y|&#92;beta,&#92;sigma^{2})&#92;pi(&#92;beta|&#92;sigma^{2})&#92;pi(&#92;sigma^{2})=f(Y,&#92;beta,&#92;sigma^{2})=&#92;&#92;  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}  &#92;left( &#92;frac{1}{2&#92;pi}&#92;right) ^{&#92;frac{k}{2}}&#92;left( &#92;frac{1}{&#92;sigma^{2}} &#92;right)^{&#92;frac{k}{2}} |&#92;Lambda_{0}|^{&#92;frac{1}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(&#92;beta-&#92;mu_{n})&#92;Lambda_{n}(&#92;beta-&#92;mu_{n})}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " class="latex" /></p>
<p>Recognizing the kernel of a normal for Beta and knowing the normalizing constant the integral over Beta is now simple. Integrating out Beta yields:</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28Y%2C%5Csigma%5E%7B2%7D%29%3D++%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%28Y%27Y-%5Cmu_%7Bn%7D%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29%7D+++%5Cfrac%7B%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%7B%7C%5CLambda_%7Bn%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D+++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7B%5CGamma%28a_%7B0%7D%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7Ba_%7B0%7D-1%7De%5E%7B-%5Cfrac%7Bb_%7B0%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="f(Y,&#92;sigma^{2})=  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}   &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " title="f(Y,&#92;sigma^{2})=  &#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}} e^{-&#92;frac{1}{2&#92;sigma^{2}}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}   &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}&#92;left(&#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{0}-1}e^{-&#92;frac{b_{0}}{&#92;sigma^{2}}}&#92;&#92;  " class="latex" /></p>
<p>Following a similar strategy as that for Beta, this can be rearranged into something containing a Gamma kernel for sigma squared.</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28Y%2C%5Csigma%5E%7B2%7D%29%3D%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%2Ba_%7B0%7D-1%7D+e%5E%7B-%5Cfrac%7Bb_%7B0%7D%2B%5Cfrac%7B1%7D%7B2%7D%28Y%27Y-%5Cmu_%7Bn%7D%27%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29%7D%7B%5Csigma%5E%7B2%7D%7D%7D++++%5Cfrac%7B%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%7B%7C%5CLambda_%7Bn%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D+++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7B%5CGamma%28a_%7B0%7D%29%7D++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="f(Y,&#92;sigma^{2})=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}+a_{0}-1} e^{-&#92;frac{b_{0}+&#92;frac{1}{2}(Y&#039;Y-&#92;mu_{n}&#039;&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}{&#92;sigma^{2}}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}  " title="f(Y,&#92;sigma^{2})=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}} &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{&#92;frac{n}{2}+a_{0}-1} e^{-&#92;frac{b_{0}+&#92;frac{1}{2}(Y&#039;Y-&#92;mu_{n}&#039;&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})}{&#92;sigma^{2}}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}   &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}  " class="latex" /></p>
<p>Define<br />
<img src="http://s0.wp.com/latex.php?latex=a_%7Bn%7D%3Da_%7B0%7D%2B%5Cfrac%7Bn%7D%7B2%7D%5C%5C++b_%7Bn%7D%3Db_%7B0%7D%2B%5Cfrac%7B1%7D%7B2%7D%28Y%27Y-%5Cmu_%7Bn%7D%5CLambda_%7Bn%7D%5Cmu_%7Bn%7D%2B%5Cmu_%7B0%7D%27%5CLambda_%7B0%7D%5Cmu_%7B0%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="a_{n}=a_{0}+&#92;frac{n}{2}&#92;&#92;  b_{n}=b_{0}+&#92;frac{1}{2}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})  " title="a_{n}=a_{0}+&#92;frac{n}{2}&#92;&#92;  b_{n}=b_{0}+&#92;frac{1}{2}(Y&#039;Y-&#92;mu_{n}&#92;Lambda_{n}&#92;mu_{n}+&#92;mu_{0}&#039;&#92;Lambda_{0}&#92;mu_{0})  " class="latex" /><br />
Then the above reads</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28Y%2C%5Csigma%5E%7B2%7D%29%3D%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D++++%5Cfrac%7B%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%7B%7C%5CLambda_%7Bn%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7B%5CGamma%28a_%7B0%7D%29%7D+++%5Cleft%28+%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%5Cright%29%5E%7Ba_%7Bn%7D-1%7D+e%5E%7B-%5Cfrac%7Bb_%7Bn%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%5C%5C+++&amp;bg=ffffff&amp;fg=000&amp;s=1" alt="f(Y,&#92;sigma^{2})=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}  &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}   &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{n}-1} e^{-&#92;frac{b_{n}}{&#92;sigma^{2}}}&#92;&#92;   " title="f(Y,&#92;sigma^{2})=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}  &#92;frac{b_{0}^{a_{0}}}{&#92;Gamma(a_{0})}   &#92;left( &#92;frac{1}{&#92;sigma^{2}}&#92;right)^{a_{n}-1} e^{-&#92;frac{b_{n}}{&#92;sigma^{2}}}&#92;&#92;   " class="latex" /></p>
<p>Recognizing the kernel of a Gamma makes the integration easy, which yields the final result for the marginal likelihood below.</p>
<h2>Marginal Likelihood/Model Evidence</h2>
<p><img src="http://s0.wp.com/latex.php?latex=m%28Y%29%3D%5Cleft%28+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cright%29%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D++++%5Cfrac%7B%7C%5CLambda_%7B0%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%7B%7C%5CLambda_%7Bn%7D%7C%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D++%5Cfrac%7Bb_%7B0%7D%5E%7Ba_%7B0%7D%7D%7D%7Bb_%7Bn%7D%5E%7Ba_%7Bn%7D%7D%7D++%5Cfrac%7B%5CGamma%28a_%7Bn%7D%29%7D%7B%5CGamma%28a_%7B0%7D%29%7D%5C%5C+&amp;bg=ffffff&amp;fg=000&amp;s=2" alt="m(Y)=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}  &#92;frac{b_{0}^{a_{0}}}{b_{n}^{a_{n}}}  &#92;frac{&#92;Gamma(a_{n})}{&#92;Gamma(a_{0})}&#92;&#92; " title="m(Y)=&#92;left( &#92;frac{1}{2&#92;pi} &#92;right)^{&#92;frac{n}{2}}    &#92;frac{|&#92;Lambda_{0}|^{&#92;frac{1}{2}}}{|&#92;Lambda_{n}|^{&#92;frac{1}{2}}}  &#92;frac{b_{0}^{a_{0}}}{b_{n}^{a_{n}}}  &#92;frac{&#92;Gamma(a_{n})}{&#92;Gamma(a_{0})}&#92;&#92; " class="latex" /></p>
<p>An example application of this can be found here in <a href="http://www.lindonslog.com/mathematics/statistics/model-selection-in-bayesian-linear-regression/" title="Model Selection in Bayesian Linear Regression">linear model selection</a>.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-318 -->

				
					
<article id="post-306" class="grid_6 post-306 post type-post status-publish format-standard hentry category-mathematics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1295 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/polynomial-regression-linear-model-in-r/index.html#respond"><span class="dsq-postid" data-dsqidentifier="306 http://www.lindonslog.com/?p=306">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/polynomial-regression-linear-model-in-r/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Polynomial Regression Linear Model in R</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>This post is just a brief example of how linear model theory can be used to perform polynomial regression. Consider getting some bivariate data that looks like this <a href="../../../../example_code/polydata.dat">(downloadable here)</a><br />
<a href="../../../../wp-content/uploads/2013/06/polyreg.bmp"><img src="../../../../wp-content/uploads/2013/06/polyreg.bmp" alt="Polynomial Regression" title="Polynomial Regression" class="alignnone size-medium wp-image-307" /></a></p>
<p>and suppose we wish to fit a 3rd degree polynomial to this data. We can write it as a linear model<br />
<img src="http://s0.wp.com/latex.php?latex=y_%7Bi%7D%3DB_%7B0%7D%2BB_%7B1%7Dx_%7Bi%7D%2BB_%7B2%7Dx_%7Bi%7D%5E%7B2%7D%2BB_%7B3%7Dx_%7Bi%7D%5E%7B3%7D+%2B+%5Cepsilon_%7Bi%7D%5C%5C++Y%3DXB+%2B+%5Cepsilon%2C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="y_{i}=B_{0}+B_{1}x_{i}+B_{2}x_{i}^{2}+B_{3}x_{i}^{3} + &#92;epsilon_{i}&#92;&#92;  Y=XB + &#92;epsilon,  " title="y_{i}=B_{0}+B_{1}x_{i}+B_{2}x_{i}^{2}+B_{3}x_{i}^{3} + &#92;epsilon_{i}&#92;&#92;  Y=XB + &#92;epsilon,  " class="latex" /><br />
which is a linear model because it is linear in the regression coefficients. The ordinary least squares estimator of the regression coefficients is then<br />
<img src="http://s0.wp.com/latex.php?latex=%5Chat%7BB%7D_%7BOLS%7D%3D%28X%5E%7B%27%7DX%29%5E%7B-1%7DX%27Y.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;hat{B}_{OLS}=(X^{&#039;}X)^{-1}X&#039;Y.  " title="&#92;hat{B}_{OLS}=(X^{&#039;}X)^{-1}X&#039;Y.  " class="latex" /></p>
<p>This is implemented in the below R code</p>
<pre class="brush: r; title: ; notranslate" title="">
plot(x,y)
n=length(y)
int=rep(1,n)
X=cbind(x,x^2,x^3,int)
B=solve(t(X)%*%X)%*%t(X)%*%y
u=seq(-100,100,1)
v=B[4]+B[1]*u+B[2]*u^2+B[3]*u^3
lines(u,v,col=&quot;red&quot;)
</pre>
<p>The array B contains the OLS estimates of the regression coefficients. The fitted polynomial is fitted below in red:<br />
<a href="../../../../wp-content/uploads/2013/06/polyregfit.bmp"><img src="../../../../wp-content/uploads/2013/06/polyregfit.bmp" alt="Fitted Polynomial" title="Fitted Polynomial" class="alignnone size-medium wp-image-313" /></a></p>
<p>The extension to higher degree polynomials is simple, just add columns to the design matrix X.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-306 -->

				
					
<article id="post-291" class="grid_6 post-291 post type-post status-publish format-standard hentry category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1296 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/statistics/ages-10-12-toy-exoplanet-detection/index.html#respond"><span class="dsq-postid" data-dsqidentifier="291 http://www.lindonslog.com/?p=291">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/statistics/ages-10-12-toy-exoplanet-detection/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Ages 10-12 Toy Exoplanet Detection</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>A major objection with the previous simulated light curves is that the baseline is rarely constnat. Instead, from what I have learned, it is a horrible mess of discontinuities and curves due to the telescope rotating and instruments heating up. I spoke to someone who said that there is some periodicity in the curve. It is easy to adapt the previous code to generalize it for a periodic background flux. Consider the model:<br />
<img src="http://s0.wp.com/latex.php?latex=f_%7Bt%7D+%5Csim+N%28b_%7B1%7D+%2B+a%5Ctext%7Bsin%7D%28%5Comega+t%29-+b_%7B2%7D1_%7B+%5Cleft%5B+%5Ctau_%7B1%7D%2C%5Ctau_%7B2%7D+%5Cright%5D+%7D%28t%29%2C+%5Csigma%5E%7B2%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="f_{t} &#92;sim N(b_{1} + a&#92;text{sin}(&#92;omega t)- b_{2}1_{ &#92;left[ &#92;tau_{1},&#92;tau_{2} &#92;right] }(t), &#92;sigma^{2})  " title="f_{t} &#92;sim N(b_{1} + a&#92;text{sin}(&#92;omega t)- b_{2}1_{ &#92;left[ &#92;tau_{1},&#92;tau_{2} &#92;right] }(t), &#92;sigma^{2})  " class="latex" /><br />
which has two additional parameters, the amplitude a and the angular frequency omega, upon which I put priors:<br />
<img src="http://s0.wp.com/latex.php?latex=a+%5Csim+unif%280%2Cb_%7B1%7D%29%5C%5C++%5Comega+%5Csim+unif%280%2C1%29%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="a &#92;sim unif(0,b_{1})&#92;&#92;  &#92;omega &#92;sim unif(0,1)&#92;&#92;  " title="a &#92;sim unif(0,b_{1})&#92;&#92;  &#92;omega &#92;sim unif(0,1)&#92;&#92;  " class="latex" /><br />
I&#8217;m not sure if the last one is justified, I&#8217;d need to talk to an exoplanet expert, but hopefully the background frequency is a lot less than the frequency at which measurements are being made.<br />
The JAGS code looks like this:</p>
<pre class="brush: r; title: ; notranslate" title="">
model
{
for(t in 1 : N)
{
D[t] ~ dnorm(mu[t],s2)
mu[t] &lt;- b[1]+ amp*sin(omega*t) - step(t - tau1)*step(tau2-t)*step(tau2-tau1) * b[2]
}
b[1] ~ dnorm(0.0,1.0E-6)T(0,)
b[2] ~ dnorm(0.0,1.0E-6)T(0,b[1])
tau1 ~ dunif(1,N)
tau2 ~ dunif(tau1,N)
s2 ~ dunif(0,100)
amp ~ dunif(0,b[1])
omega ~ dunif(0,1)
}
</pre>
<p>The code to call JAGS from R and make plots can be found <a href="../../../../example_code/ages1012exoplanet.R">here</a>.</p>
<pre class="brush: r; title: ; notranslate" title="">
rm(list=ls())
library('rjags')

s2=5
bn=1000
dn=400
amp=1
omega=0.005
base=rnorm(bn,100,sqrt(s2))
drop=rnorm(dn,97,sqrt(s2))
t=seq(1,length(c(base,drop,base,base)),1)
sin=amp*sin(omega*t)
trace=c(base,drop,base,base)
trace=trace+sin

plot(trace,type=&quot;l&quot;,col=&quot;red&quot;,xlab=&quot;Time t&quot;,ylab=&quot;Flux&quot;)

datalist=list(
  D=trace,
  N=length(trace)
  )

jags &lt;- jags.model(file.path('/home/grad/msl33/Dropbox/samsi/cp.bugs'),data = datalist, n.chains = 1, n.adapt = 500)
mcmc.samples=coda.samples(jags,c('b','tau1','tau2','amp','omega'),2000)
plot(mcmc.samples)
</pre>
<p>and I generated some fake data that looks like this:<br />
<a href="../../../../wp-content/uploads/2013/06/per_flux.bmp"><img src="../../../../wp-content/uploads/2013/06/per_flux.bmp" alt="Periodic Background Flux with Exoplanet Transit Light Curve" class="alignnone size-medium wp-image-297" /></a><br />
based on the parameters:<br />
<img src="http://s0.wp.com/latex.php?latex=b_%7B1%7D%3D100%5C%5C++b_%7B2%7D%3D3%5C%5C++a%3D1%5C%5C++%5Ctau_%7B1%7D%3D1000%5C%5C++%5Ctau_%7B2%7D%3D1400%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="b_{1}=100&#92;&#92;  b_{2}=3&#92;&#92;  a=1&#92;&#92;  &#92;tau_{1}=1000&#92;&#92;  &#92;tau_{2}=1400&#92;&#92;  " title="b_{1}=100&#92;&#92;  b_{2}=3&#92;&#92;  a=1&#92;&#92;  &#92;tau_{1}=1000&#92;&#92;  &#92;tau_{2}=1400&#92;&#92;  " class="latex" /></p>
<p>The posterior density estimates are below:<br />
<a href="../../../../wp-content/uploads/2013/06/per_per.bmp"><img src="../../../../wp-content/uploads/2013/06/per_per.bmp" alt="Posterior Angular Frequency and Amplitude" title="Posterior Angular Frequency and Amplitude" class="alignnone size-medium wp-image-299" /></a><br />
<a href="../../../../wp-content/uploads/2013/06/per_tau.bmp"><img src="../../../../wp-content/uploads/2013/06/per_tau.bmp" alt="Posterior Exoplanet Transit Times" class="alignnone size-medium wp-image-300" /></a><br />
<a href="../../../../wp-content/uploads/2013/06/per_b.bmp"><img src="../../../../wp-content/uploads/2013/06/per_b.bmp" alt="Background Flux and Dip in Flux" class="alignnone size-medium wp-image-301" /></a></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-291 -->

				
					
<article id="post-262" class="grid_6 post-262 post type-post status-publish format-standard hentry category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1297 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/statistics/toy-exoplanet-change-point-detection/index.html#respond"><span class="dsq-postid" data-dsqidentifier="262 http://www.lindonslog.com/?p=262">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/statistics/toy-exoplanet-change-point-detection/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Toy Exoplanet Change Point Light Curve Transit Detection</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>I&#8217;ve been attending an exoplanet data conference this week, a gathering between astrophysicists and statisticians. One way to look for exoplanets is by the <a href="http://www.nasa.gov/mission_pages/kepler/multimedia/images/kepler-transit-graph.html">&#8220;Transit&#8221; method</a>. Basically a dip in the flux from a star is observed as an orbiting planet passes across the line of sight between the observer and the star. There was a lot of talk about automated methods of detecting this dip in brightness. I decided to code this up for the lulz:</p>
<p>First I generated some fake data.<br />
<a href="../../../../wp-content/uploads/2013/06/transit.bmp"><img src="../../../../wp-content/uploads/2013/06/transit.bmp" alt="fake exoplanet transit" title="fake exoplanet transit" class="alignnone size-medium wp-image-264" /></a></p>
<p>The dip in flux is meant to be as the exoplanet passes between the observer and the star. Interesting parameters are the dip in flux and the time interval of the reduced flux as these correspond to physical parameters of the planet-star system. I decided to model this as a simple changepoint problem:</p>
<p><img src="http://s0.wp.com/latex.php?latex=f%28t%29+%5Csim+N%28b_%7B1%7D+-+b_%7B2%7D1_%7B%5Cleft%5B+%5Ctau_%7B1%7D%2C%5Ctau_%7B2%7D+%5Cright%5D%7D%28t%29%2C+%5Csigma%5E%7B2%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="f(t) &#92;sim N(b_{1} - b_{2}1_{&#92;left[ &#92;tau_{1},&#92;tau_{2} &#92;right]}(t), &#92;sigma^{2})  " title="f(t) &#92;sim N(b_{1} - b_{2}1_{&#92;left[ &#92;tau_{1},&#92;tau_{2} &#92;right]}(t), &#92;sigma^{2})  " class="latex" /></p>
<p>Parameters of interest are b_1, b_2, tau_1 and tau_2, which are the baseline, the drop, the time at which the drop occurs and the time at which the drop finishes respectively. I proceeded using the following JAGS code:</p>
<pre class="brush: r; title: ; notranslate" title="">
model
{
for(t in 1 : N)
{
D[t] ~ dnorm(mu[t],s2)
mu[t] &lt;- b[1] - step(t - tau1)*step(tau2-t)*step(tau2-tau1) * b[2]
}
b[1] ~ dnorm(0.0,1.0E-6)T(0,)
b[2] ~ dnorm(0.0,1.0E-6)T(0,b[1])
tau1 ~ dunif(1,N)
tau2 ~ dunif(tau1,N)
s2 ~ dunif(0,100)
}
</pre>
<p>The script to generate the data and call JAGS from R can be found here. The posterior density estimates are displayed below:<br />
<a href="../../../../wp-content/uploads/2013/06/exo_post.bmp"><img src="../../../../wp-content/uploads/2013/06/exo_post.bmp" alt="exoplanet posterior summaries" title="exoplanet posterior summaries" class="alignnone size-medium wp-image-273" /></a></p>
<p>The Bayesian change point model correctly identifies a drop of 40 and the times at which the transit begin and finishes. One might argue that such a large drop in flux is unphysical for detecting Earth like planets, so here is a further example to detect how robust this approach is. Consider the following light curve:<br />
<a href="../../../../wp-content/uploads/2013/06/exo_tiny_trace.bmp"><img src="../../../../wp-content/uploads/2013/06/exo_tiny_trace.bmp" alt="exoplanet light curve" title="exoplanet light curve" class="alignnone size-medium wp-image-278" /></a><br />
Can you tell by eye how deep the drop is, when it occurs and for how long? Here are the posterior density estimates:</p>
<p><a href="../../../../wp-content/uploads/2013/06/exo_post_b.bmp"><img src="../../../../wp-content/uploads/2013/06/exo_post_b.bmp" alt="exoplanet posterior distribution" title="exoplanet posterior distribution"class="alignnone size-medium wp-image-280" /></a><br />
<a href="../../../../wp-content/uploads/2013/06/exo_post_tau.bmp"><img src="../../../../wp-content/uploads/2013/06/exo_post_tau.bmp" alt="exoplanet posterior distribution" title="exoplanet posterior distribution" class="alignnone size-medium wp-image-279" /></a><br />
The true valies with which the light curve was generated are<br />
<img src="http://s0.wp.com/latex.php?latex=b_%7B1%7D%3D100%5C%5C++b_%7B2%7D%3D0.5%5C%5C++%5Ctau_%7B1%7D%3D2000%5C%5C++%5Ctau_%7B2%7D%3D2400%5C%5C++%5Csigma%5E%7B2%7D%3D5++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="b_{1}=100&#92;&#92;  b_{2}=0.5&#92;&#92;  &#92;tau_{1}=2000&#92;&#92;  &#92;tau_{2}=2400&#92;&#92;  &#92;sigma^{2}=5  " title="b_{1}=100&#92;&#92;  b_{2}=0.5&#92;&#92;  &#92;tau_{1}=2000&#92;&#92;  &#92;tau_{2}=2400&#92;&#92;  &#92;sigma^{2}=5  " class="latex" /></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-262 -->

				
					
<article id="post-244" class="grid_6 post-244 post type-post status-publish format-standard hentry category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1297 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/statistics/an-introduction-to-importance-sampling/index.html#comments"><span class="dsq-postid" data-dsqidentifier="244 http://www.lindonslog.com/?p=244">1 Comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/statistics/an-introduction-to-importance-sampling/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">An Introduction to Importance Sampling</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>Importance Sampling is a Monte Carlo integration technique for getting (very accurate) approximations to integrals. Consider the integral<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cint+e%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx%5E%7B2%7D%7Ddx%2C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;int e^{-&#92;frac{1}{2}x^{2}}dx,  " title="&#92;int e^{-&#92;frac{1}{2}x^{2}}dx,  " class="latex" /><br />
and suppose we wish to approximate this without doing any calculus. Statistically speaking we want to compute the normalizing constant for a standard normal, which we know to be<br />
<img src="http://s0.wp.com/latex.php?latex=%5Csqrt%7B2+%5Cpi%7D+%5Capprox+2.506628.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;sqrt{2 &#92;pi} &#92;approx 2.506628.  " title="&#92;sqrt{2 &#92;pi} &#92;approx 2.506628.  " class="latex" /><br />
We can rewrite the above integral as<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7Be%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx%5E%7B2%7D%7D%7D%7Bg%28x%29%7Dg%28x%29dx%2C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;int &#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)}g(x)dx,  " title="&#92;int &#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)}g(x)dx,  " class="latex" /><br />
because this is just multiplying the integrand by 1. One can now make the observation that<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7Be%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx%5E%7B2%7D%7D%7D%7Bg%28x%29%7Dg%28x%29dx%3D%5Cmathbb%7BE%7D%5Cleft%5B%5Cfrac%7Be%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx%5E%7B2%7D%7D%7D%7Bg%28x%29%7D+%5Cright%5D%2C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;int &#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)}g(x)dx=&#92;mathbb{E}&#92;left[&#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)} &#92;right],  " title="&#92;int &#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)}g(x)dx=&#92;mathbb{E}&#92;left[&#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)} &#92;right],  " class="latex" /><br />
and realise that<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Cfrac%7Be%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx%5E%7B2%7D%7D%7D%7Bg%28x%29%7D+%5Cright%5D+%5Capprox+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cfrac%7Be%5E%7B-%5Cfrac%7B1%7D%7B2%7Dx_%7Bi%7D%5E%7B2%7D%7D%7D%7Bg%28x_%7Bi%7D%29%7D%2C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;mathbb{E}&#92;left[&#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)} &#92;right] &#92;approx &#92;frac{1}{n}&#92;sum_{i=1}^{n} &#92;frac{e^{-&#92;frac{1}{2}x_{i}^{2}}}{g(x_{i})},  " title="&#92;mathbb{E}&#92;left[&#92;frac{e^{-&#92;frac{1}{2}x^{2}}}{g(x)} &#92;right] &#92;approx &#92;frac{1}{n}&#92;sum_{i=1}^{n} &#92;frac{e^{-&#92;frac{1}{2}x_{i}^{2}}}{g(x_{i})},  " class="latex" /><br />
where<br />
<img src="http://s0.wp.com/latex.php?latex=x_%7Bi%7D+%5Csim+g%28x%29&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="x_{i} &#92;sim g(x)" title="x_{i} &#92;sim g(x)" class="latex" />.<br />
The code below in R performs this summation.</p>
<pre class="brush: r; title: ; notranslate" title="">
N=10000
cumsum=rep(0,N)
for(i in 2:N){
x=rt(1,3)
cumsum[i]=cumsum[i-1]+((exp((-0.5)*(x^2)))/dt(x,3))
}

for(i in 1:N) cumsum[i]=cumsum[i]/i
plot(cumsum[50:N],type=&quot;l&quot;)
</pre>
<p><a href="../../../../wp-content/uploads/2013/06/nconstant.bmp"><img src="../../../../wp-content/uploads/2013/06/nconstant.bmp" alt="Importance Sampling Normalizing Constant" title="Importance Sampling Normalizing Constant" class="alignnone size-medium wp-image-258" /></a><br />
At n=10000 the normalizing constant given by importance sampling is 2.516303. Increasing n to n=100000 the normalizing constant is 2.505983</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-244 -->

				
					
<article id="post-218" class="grid_6 post-218 post type-post status-publish format-standard hentry category-mathematics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1325 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/multivariate-normal-conditional-distribution/index.html#respond"><span class="dsq-postid" data-dsqidentifier="218 http://www.lindonslog.com/?p=218">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/multivariate-normal-conditional-distribution/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Multivariate Normal Conditional Distribution</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>Previously I made a post about<a href="../../../../mathematics/diagonalize-positive-definite-symmetric-matrix-schur-complement-ldu-decomposition/index.html" title="Diagonalize a Positive-Definite Symmetric Matrix using the Schur Complement and LDU Decomposition"> diagonalizing a positive definite symmetric matrix </a>. One of the applications of this within statistics is to diagonalize the variance matrix of a multivariate normal in order to derive conditional distributions. Let</p>
<p><img src="http://s0.wp.com/latex.php?latex=Y%5Csim+N_%7Bp%7D%28m%2CV%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y&#92;sim N_{p}(m,V)  " title="Y&#92;sim N_{p}(m,V)  " class="latex" /><br />
Consider multiplying Y by the following matrix<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++1+%26+-V_%7B12%7DV_%7B22%7D%5E%7B-1%7D%5C%5C++++++++++0+%26+1%5C%5C++%5Cend%7Bpmatrix%7DY+%5Csim+N_%7Bp%7D%5Cleft%28+%5Cbegin%7Bpmatrix%7D+m_%7B1%7D+-+V_%7B12%7DV_%7B22%7D%5E%7B-1%7Dm_%7B2%7D+%5C%5C+m_%7B2%7D%5C%5C+%5Cend%7Bpmatrix%7D%2C%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DV_%7B21%7D+%26+0+%5C%5C++++++++++0+%26+V_%7B22%7D%5C%5C++%5Cend%7Bpmatrix%7D%5Cright%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          1 &amp; -V_{12}V_{22}^{-1}&#92;&#92;          0 &amp; 1&#92;&#92;  &#92;end{pmatrix}Y &#92;sim N_{p}&#92;left( &#92;begin{pmatrix} m_{1} - V_{12}V_{22}^{-1}m_{2} &#92;&#92; m_{2}&#92;&#92; &#92;end{pmatrix},&#92;begin{pmatrix}          V_{11}-V_{12}V_{22}^{-1}V_{21} &amp; 0 &#92;&#92;          0 &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}&#92;right)  " title="&#92;begin{pmatrix}          1 &amp; -V_{12}V_{22}^{-1}&#92;&#92;          0 &amp; 1&#92;&#92;  &#92;end{pmatrix}Y &#92;sim N_{p}&#92;left( &#92;begin{pmatrix} m_{1} - V_{12}V_{22}^{-1}m_{2} &#92;&#92; m_{2}&#92;&#92; &#92;end{pmatrix},&#92;begin{pmatrix}          V_{11}-V_{12}V_{22}^{-1}V_{21} &amp; 0 &#92;&#92;          0 &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}&#92;right)  " class="latex" /><br />
i.e.<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++Y_%7B1%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DY_%7B2%7D+%5C%5C++++++++++Y_%7B2%7D%5C%5C++%5Cend%7Bpmatrix%7D%5Csim++N%5Cleft%28++%5Cbegin%7Bpmatrix%7D+m_%7B1%7D+-+V_%7B12%7DV_%7B22%7D%5E%7B-1%7Dm_%7B2%7D+%5C%5C+m_%7B2%7D%5C%5C+%5Cend%7Bpmatrix%7D%2C%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DV_%7B21%7D+%26+0+%5C%5C++++++++++0+%26+V_%7B22%7D%5Cend%7Bpmatrix%7D+%5Cright%29%5C%5C++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          Y_{1}-V_{12}V_{22}^{-1}Y_{2} &#92;&#92;          Y_{2}&#92;&#92;  &#92;end{pmatrix}&#92;sim  N&#92;left(  &#92;begin{pmatrix} m_{1} - V_{12}V_{22}^{-1}m_{2} &#92;&#92; m_{2}&#92;&#92; &#92;end{pmatrix},&#92;begin{pmatrix}          V_{11}-V_{12}V_{22}^{-1}V_{21} &amp; 0 &#92;&#92;          0 &amp; V_{22}&#92;end{pmatrix} &#92;right)&#92;&#92;  " title="&#92;begin{pmatrix}          Y_{1}-V_{12}V_{22}^{-1}Y_{2} &#92;&#92;          Y_{2}&#92;&#92;  &#92;end{pmatrix}&#92;sim  N&#92;left(  &#92;begin{pmatrix} m_{1} - V_{12}V_{22}^{-1}m_{2} &#92;&#92; m_{2}&#92;&#92; &#92;end{pmatrix},&#92;begin{pmatrix}          V_{11}-V_{12}V_{22}^{-1}V_{21} &amp; 0 &#92;&#92;          0 &amp; V_{22}&#92;end{pmatrix} &#92;right)&#92;&#92;  " class="latex" /><br />
The covariance matrix has now been diagonalized. This is useful because zero covariance implies independence for normally distributed random variables and so it follows that</p>
<p><img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DY_%7B2%7D+%5Csim+N%28+m_%7B1%7D+-+V_%7B12%7DV_%7B22%7D%5E%7B-1%7Dm_%7B2%7D+%2CV_%7B11%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DV_%7B21%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}-V_{12}V_{22}^{-1}Y_{2} &#92;sim N( m_{1} - V_{12}V_{22}^{-1}m_{2} ,V_{11}-V_{12}V_{22}^{-1}V_{21})  " title="Y_{1}-V_{12}V_{22}^{-1}Y_{2} &#92;sim N( m_{1} - V_{12}V_{22}^{-1}m_{2} ,V_{11}-V_{12}V_{22}^{-1}V_{21})  " class="latex" /><br />
Conditiong on Y2 it follows that the conditional distribution is<br />
<img src="http://s0.wp.com/latex.php?latex=Y_%7B1%7D%7CY_%7B2%7D+%5Csim+N%28+m_%7B1%7D+%2B+V_%7B12%7DV_%7B22%7D%5E%7B-1%7D%28Y_%7B2%7D-m_%7B2%7D+%29%2CV_%7B11%7D-V_%7B12%7DV_%7B22%7D%5E%7B-1%7DV_%7B21%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{1}|Y_{2} &#92;sim N( m_{1} + V_{12}V_{22}^{-1}(Y_{2}-m_{2} ),V_{11}-V_{12}V_{22}^{-1}V_{21})  " title="Y_{1}|Y_{2} &#92;sim N( m_{1} + V_{12}V_{22}^{-1}(Y_{2}-m_{2} ),V_{11}-V_{12}V_{22}^{-1}V_{21})  " class="latex" /><br />
Notice how the variance of the conditional normal distribution is the marginal variance of Y1 minus something. That is to say the variance of Y1 is reduced given the knowledge of Y2. The variance can be recognized as the Schur complement of the covariance matrix with respect to V22. A similar treatment yields<br />
<img src="http://s0.wp.com/latex.php?latex=Y_%7B2%7D%7CY_%7B1%7D+%5Csim+N%28+m_%7B2%7D+%2B+V_%7B21%7DV_%7B11%7D%5E%7B-1%7D%28Y_%7B1%7D-m_%7B1%7D+%29%2CV_%7B22%7D-V_%7B21%7DV_%7B11%7D%5E%7B-1%7DV_%7B12%7D%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{2}|Y_{1} &#92;sim N( m_{2} + V_{21}V_{11}^{-1}(Y_{1}-m_{1} ),V_{22}-V_{21}V_{11}^{-1}V_{12})  " title="Y_{2}|Y_{1} &#92;sim N( m_{2} + V_{21}V_{11}^{-1}(Y_{1}-m_{1} ),V_{22}-V_{21}V_{11}^{-1}V_{12})  " class="latex" /></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-218 -->

				
					
<article id="post-205" class="grid_6 post-205 post type-post status-publish format-standard hentry category-linear-algebra category-mathematics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1325 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/block-matrix-inverse/index.html#respond"><span class="dsq-postid" data-dsqidentifier="205 http://www.lindonslog.com/?p=205">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/block-matrix-inverse/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Block Matrix Inverse</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>Previously I wrote a post on the<a href="../../../../mathematics/invert-a-matrix-using-the-woodbury-matrix-inverse-formula-identity/index.html"> Woodbury matrix inverse formula</a>. One of the results derived was<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D+%26+V_%7B12%7D%5C%5C++++++++++V_%7B21%7D+%26+V_%7B22%7D%5C%5C++%5Cend%7Bpmatrix%7D%5E%7B-1%7D++%3D++%5Cbegin%7Bpmatrix%7D++V_%7B11.2%7D%5E%7B-1%7D+%26+-V_%7B11.2%7D%5E%7B-1%7DV_%7B12%7DV_%7B22%7D%5E%7B-1%7D%5C%5C++-V_%7B22%7D%5E%7B-1%7DV_%7B21%7DV_%7B11.2%7D%5E%7B-1%7D+%26+V_%7B22%7D%5E%7B-1%7D%2BV_%7B22%7D%5E%7B-1%7DV_%7B21%7DV_%7B11.2%7D%5E%7B-1%7DV_%7B12%7DV_%7B22%7D%5E%7B-1%7D++%5Cend%7Bpmatrix%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =  &#92;begin{pmatrix}  V_{11.2}^{-1} &amp; -V_{11.2}^{-1}V_{12}V_{22}^{-1}&#92;&#92;  -V_{22}^{-1}V_{21}V_{11.2}^{-1} &amp; V_{22}^{-1}+V_{22}^{-1}V_{21}V_{11.2}^{-1}V_{12}V_{22}^{-1}  &#92;end{pmatrix}  " title="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =  &#92;begin{pmatrix}  V_{11.2}^{-1} &amp; -V_{11.2}^{-1}V_{12}V_{22}^{-1}&#92;&#92;  -V_{22}^{-1}V_{21}V_{11.2}^{-1} &amp; V_{22}^{-1}+V_{22}^{-1}V_{21}V_{11.2}^{-1}V_{12}V_{22}^{-1}  &#92;end{pmatrix}  " class="latex" /></p>
<p><img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D+%26+V_%7B12%7D%5C%5C++++++++++V_%7B21%7D+%26+V_%7B22%7D%5C%5C++%5Cend%7Bpmatrix%7D%5E%7B-1%7D++%3D%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D%5E%7B-1%7D%2BV_%7B11%7D%5E%7B-1%7DV_%7B12%7DV_%7B22.1%7D%5E%7B-1%7DV_%7B21%7DV_%7B11%7D%5E%7B-1%7D+%26+-V_%7B11%7D%5E%7B-1%7DV_%7B12%7DV_%7B22.1%7D%5E%7B-1%7D%5C%5C++++++++++-V_%7B22.1%7D%5E%7B-1%7DV_%7B21%7DV_%7B11%7D%5E%7B-1%7D+%26+V_%7B22.1%7D%5E%7B-1%7D++%5Cend%7Bpmatrix%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =&#92;begin{pmatrix}          V_{11}^{-1}+V_{11}^{-1}V_{12}V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; -V_{11}^{-1}V_{12}V_{22.1}^{-1}&#92;&#92;          -V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; V_{22.1}^{-1}  &#92;end{pmatrix}  " title="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =&#92;begin{pmatrix}          V_{11}^{-1}+V_{11}^{-1}V_{12}V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; -V_{11}^{-1}V_{12}V_{22.1}^{-1}&#92;&#92;          -V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; V_{22.1}^{-1}  &#92;end{pmatrix}  " class="latex" /></p>
<p>Hence the inverse of a block matrix can be written neatly in one of two ways:<br />
<img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D+%26+V_%7B12%7D%5C%5C++++++++++V_%7B21%7D+%26+V_%7B22%7D%5C%5C++%5Cend%7Bpmatrix%7D%5E%7B-1%7D++%3D++%5Cbegin%7Bpmatrix%7D++V_%7B11.2%7D%5E%7B-1%7D+%26+-V_%7B11.2%7D%5E%7B-1%7DV_%7B12%7DV_%7B22%7D%5E%7B-1%7D%5C%5C++-V_%7B22%7D%5E%7B-1%7DV_%7B21%7DV_%7B11.2%7D%5E%7B-1%7D+%26++V_%7B22.1%7D%5E%7B-1%7D++%5Cend%7Bpmatrix%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =  &#92;begin{pmatrix}  V_{11.2}^{-1} &amp; -V_{11.2}^{-1}V_{12}V_{22}^{-1}&#92;&#92;  -V_{22}^{-1}V_{21}V_{11.2}^{-1} &amp;  V_{22.1}^{-1}  &#92;end{pmatrix}  " title="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =  &#92;begin{pmatrix}  V_{11.2}^{-1} &amp; -V_{11.2}^{-1}V_{12}V_{22}^{-1}&#92;&#92;  -V_{22}^{-1}V_{21}V_{11.2}^{-1} &amp;  V_{22.1}^{-1}  &#92;end{pmatrix}  " class="latex" /></p>
<p><img src="http://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11%7D+%26+V_%7B12%7D%5C%5C++++++++++V_%7B21%7D+%26+V_%7B22%7D%5C%5C++%5Cend%7Bpmatrix%7D%5E%7B-1%7D++%3D%5Cbegin%7Bpmatrix%7D++++++++++V_%7B11.2%7D%5E%7B-1%7D+%26+-V_%7B11%7D%5E%7B-1%7DV_%7B12%7DV_%7B22.1%7D%5E%7B-1%7D%5C%5C++++++++++-V_%7B22.1%7D%5E%7B-1%7DV_%7B21%7DV_%7B11%7D%5E%7B-1%7D+%26+V_%7B22.1%7D%5E%7B-1%7D++%5Cend%7Bpmatrix%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =&#92;begin{pmatrix}          V_{11.2}^{-1} &amp; -V_{11}^{-1}V_{12}V_{22.1}^{-1}&#92;&#92;          -V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; V_{22.1}^{-1}  &#92;end{pmatrix}  " title="&#92;begin{pmatrix}          V_{11} &amp; V_{12}&#92;&#92;          V_{21} &amp; V_{22}&#92;&#92;  &#92;end{pmatrix}^{-1}  =&#92;begin{pmatrix}          V_{11.2}^{-1} &amp; -V_{11}^{-1}V_{12}V_{22.1}^{-1}&#92;&#92;          -V_{22.1}^{-1}V_{21}V_{11}^{-1} &amp; V_{22.1}^{-1}  &#92;end{pmatrix}  " class="latex" /></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-205 -->

				
				<div class="clear"></div>
				<div class='pagination cf'><a href='../../index.html' class='inactive' >1</a><span class='current'>2</span><a href='../3/index.html' class='inactive' >3</a></div>

			
			</div><!-- #content .site-content -->
		</section><!-- #primary .content-area -->


	</div><!-- #main .site-main -->

<div id="bottom">
<div class="container_6 cf">

<li class="botwid grid_2 widget_categories"><h3 class="bothead">Categories</h3>		<ul>
	<li class="cat-item cat-item-3"><a href="../../../linux-unix/index.html" >Linux/Unix</a> (16)
</li>
	<li class="cat-item cat-item-5 current-cat"><a href="../../index.html" >Mathematics</a> (22)
<ul class='children'>
	<li class="cat-item cat-item-10"><a href="../../linear-algebra/index.html" >Linear Algebra</a> (5)
</li>
	<li class="cat-item cat-item-7"><a href="../../statistics/index.html" >Statistics</a> (17)
</li>
</ul>
</li>
	<li class="cat-item cat-item-6"><a href="../../../programming/index.html" >Programming</a> (24)
<ul class='children'>
	<li class="cat-item cat-item-48"><a href="../../../programming/clojure/index.html" >clojure</a> (1)
</li>
	<li class="cat-item cat-item-51"><a href="../../../programming/functional-programming/index.html" >functional programming</a> (1)
</li>
	<li class="cat-item cat-item-50"><a href="../../../programming/haskell-programming/index.html" >haskell</a> (1)
</li>
	<li class="cat-item cat-item-40"><a href="../../../programming/julia/index.html" >julia</a> (2)
</li>
	<li class="cat-item cat-item-4"><a href="../../../programming/openmp/index.html" >OpenMP</a> (6)
</li>
	<li class="cat-item cat-item-11"><a href="../../../programming/r/index.html" >R</a> (12)
</li>
	<li class="cat-item cat-item-43"><a href="../../../programming/scala/index.html" >scala</a> (1)
</li>
</ul>
</li>
		</ul>
</li>		<li class="botwid grid_2 widget_recent_entries">		<h3 class="bothead">Recent Posts</h3>		<ul>
					<li>
				<a href="../../../../programming/partial-application-functions-julia/index.html">Partial Application for Functions in Julia</a>
						</li>
					<li>
				<a href="../../../../programming/newtons-iteration-scala-clojure-haskell/index.html">Newtons Iteration in Scala, Clojure and Haskell Comparison</a>
						</li>
					<li>
				<a href="../../../../mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/index.html">MALA &#8211; Metropolis Adjusted Langevin Algorithm in Julia</a>
						</li>
					<li>
				<a href="../../../../programming/passing-julia-type-to-c-function-as-struct/index.html">Passing Julia Type to C Function as Struct</a>
						</li>
					<li>
				<a href="../../../../linux-unix/send-lines-code-vim-r-julia-python-repl-slime/index.html">Send Lines of Code from Vim to R/Julia/Python REPL</a>
						</li>
					<li>
				<a href="../../../../linux-unix/c-merge-sort-algorithm/index.html">C++ Merge Sort Algorithm</a>
						</li>
					<li>
				<a href="../../../../programming/generate-random-inverse-gaussian-r/index.html">Generate Random Inverse Gaussian in R</a>
						</li>
					<li>
				<a href="../../../../mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/index.html">Generalized Double Pareto Priors for Regression</a>
						</li>
					<li>
				<a href="../../../../mathematics/em-algorithm-bayesian-lasso-r-cpp-code/index.html">EM Algorithm for Bayesian Lasso R Cpp Code</a>
						</li>
				</ul>
		</li>			<div class="squarebanner grid_2 cf">
	<h3 class="bothead"> Sponsors </h3>
<ul><li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>			

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li></ul>
</div></div>
</div>


	<footer id="colophon" class="site-footer" role="contentinfo">
	<div class="container_6">
	<div class="site-info">
			<div class="fcred">
			Copyright &copy; 2016 <a href="../../../../index.html" title="Ive Moved">Ive Moved</a> - .<br />
 | <a href="https://michaellindon.github.io/" >Ive Moved</a>
			</div>		
		</div><!-- .site-info -->	
		</footer><!-- #colophon .site-footer -->
	
</div><!-- #page .hfeed .site -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40536457-1', 'lindonslog.com');
  ga('send', 'pageview');

</script><!-- MathJax Latex Plugin installed: Disabled as no shortcodes on this page -->	<div style="display:none">
	</div>
<script type="text/javascript">var elLogo = document.getElementById("ft_logo"); if (elLogo) {elLogo.style.maxHeight = elLogo.getAttribute("relHeight") ? elLogo.getAttribute("relHeight") + "px" : "100px";} if (elLogo) {elLogo.style.maxWidth = elLogo.getAttribute("relWidth") ? elLogo.getAttribute("relWidth") + "px" : "100px";}</script><script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shCore.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/third-party-brushes/shBrushR.js%3Fver=20100919'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushCpp.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushPlain.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushBash.js%3Fver=3.0.9b'></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "http://www.lindonslog.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.getElementsByTagName("head")[0].insertBefore( corecss, document.getElementById("syntaxhighlighteranchor") );
		var themecssurl = "http://www.lindonslog.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		//document.getElementById("syntaxhighlighteranchor").appendChild(themecss);
		document.getElementsByTagName("head")[0].insertBefore( themecss, document.getElementById("syntaxhighlighteranchor") );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();
</script>
<script type='text/javascript' src='http://s0.wp.com/wp-content/js/devicepx-jetpack.js?ver=201652'></script>
<script type='text/javascript' src='http://s.gravatar.com/js/gprofiles.js?ver=2016Decaa'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type='text/javascript' src='../../../../wp-content/plugins/jetpack/modules/wpgroho.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/superfish.js%3Fver=20120206'></script>
<script type='text/javascript' src='../../../../wp-includes/js/wp-embed.min.js%3Fver=4.6.1'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var countVars = {"disqusShortname":"michaellindon"};
/* ]]> */
</script>
<script type='text/javascript' src='../../../../wp-content/plugins/disqus-comment-system/media/js/count.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='http://stats.wp.com/e-201652.js' async defer></script>
<script type='text/javascript'>
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:3.9.7',blog:'27325203',post:'0',tz:'-4',srv:'www.lindonslog.com'} ]);
	_stq.push([ 'clickTrackerInit', '27325203', '0' ]);
</script>

</body>
</html>
