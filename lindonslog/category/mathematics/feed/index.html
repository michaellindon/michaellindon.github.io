<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Mathematics &#8211; Lindons Log</title>
	<atom:link href="http://www.lindonslog.com/category/mathematics/feed/" rel="self" type="application/rss+xml" />
	<link>http://www.lindonslog.com</link>
	<description></description>
	<lastBuildDate>Thu, 03 Nov 2016 17:07:09 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>

<image>
	<url>http://www.lindonslog.com/wp-content/uploads/2015/09/cropped-L-32x32.png</url>
	<title>Mathematics &#8211; Lindons Log</title>
	<link>http://www.lindonslog.com</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>MALA &#8211; Metropolis Adjusted Langevin Algorithm in Julia</title>
		<link>http://www.lindonslog.com/mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/</link>
		<comments>http://www.lindonslog.com/mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/#respond</comments>
		<pubDate>Mon, 03 Oct 2016 17:28:24 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[Julia]]></category>
		<category><![CDATA[MALA]]></category>
		<category><![CDATA[mcmc]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=1156</guid>
		<description><![CDATA[<p>Metropolis Adjusted Langevin Algorithm (MALA) Haven&#8217;t dumped much code here in a while. Here&#8217;s a Julia implementation of MALA with an arbitrary preconditioning matrix M. Potentially I might use this in the future. Generic Julia Implementation Arguments are a function to evaluate the logdensity, function to evaluate the gradient, a step size h, a preconditioning [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/">MALA &#8211; Metropolis Adjusted Langevin Algorithm in Julia</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<h1>Metropolis Adjusted Langevin Algorithm (MALA)</h1>
<p>Haven&#8217;t dumped much code here in a while. Here&#8217;s a Julia implementation of MALA with an arbitrary preconditioning matrix M. Potentially I might use this in the future.</p>
<h2>Generic Julia Implementation</h2>
<p>Arguments are a function to evaluate the logdensity, function to evaluate the gradient, a step size h, a preconditioning matrix M, number of iterations and an initial parameter value.</p>
<pre class="brush: python; title: ; notranslate">
function mala(logdensity,gradient,h,M,niter,θinit)                                                                                                                                                                 
        function gradientStep(θ,t)                                                                                                                                                                                 
                θ-t*M*gradient(θ)                                                                                                                                                                                  
        end                                                                                                                                                                                                        
        θtrace=Array{Float64}(length(θinit),niter)                                                                                                                                                                 
        θ=θinit                                                                                                                                                                                                    
        θtrace[:,1]=θinit                                                                                                                                                                                          
        for i=2:niter                                                                                                                                                                                              
                θold=θ                                                                                                                                                                                             
                θ=rand(MvNormal(gradientStep(θ,0.5*h),h*M))                                                                                                                                                        
                d=logdensity(θ) - logdensity(θold) + logpdf(MvNormal(gradientStep(θ,0.5*h),h*M),θold) - logpdf(MvNormal(gradientStep(θold,0.5*h),h*M),θ)                                                           
                if(!(log(rand(Uniform(0,1)))&lt;d))                                                                                                                                                                   
                        θ=θold                                                                                                                                                                                     
                end                                                                                                                                                                                                
                θtrace[:,i]=θ                                                                                                                                                                                      
        end                                                                                                                                                                                                        
        θtrace                                                                                                                                                                                                     
end     
</pre>
<p>Don&#8217;t forget to use the Distributions package of Julia.</p>
<h2>Bivariate Normal Example</h2>
<p>Consider a Bivariate Normal as an example</p>
<pre class="brush: python; title: ; notranslate">
ρ²=0.8                                                                                                                                                                                                             
Σ=[1 ρ²;ρ² 1]                                                                                                                                                                                                      
                                                                                                                                                                                                                   
function logdensity(θ)                                                                                                                                                                                             
        logpdf(MvNormal(Σ),θ)                                                                                                                                                                                      
end                                                                                                                                                                                                                
                                                                                                                                                                                                                   
function gradient(θ)                                                                                                                                                                                               
        Σ\θ                                                                                                                                                                                                        
end   
</pre>
<p>and now the code to generate the plots and results</p>
<pre class="brush: python; title: ; notranslate">
niter=1000                                                                                                                                                                                                         
h=1/eigs(inv(Σ),nev=1)[1][1]                                                                                                                                                                                       
@time draws=mala(logdensity,gradient,h,eye(2),niter,[5,50]);                                                                                                                                                       
sum(map(t -&gt; draws[:,t]!=draws[:,t-1],2:niter))/(niter-1)                                                                                                                                                          
@time pdraws=mala(logdensity,gradient,h,Σ,niter,[5,50]);                                                                                                                                                           
sum(map(t -&gt; pdraws[:,t]!=pdraws[:,t-1],2:niter))/(niter-1)                                                                                                                                                        
                                                                                                                                                                                                                   
function logdensity2d(x,y)                                                                                                                                                                                         
        logdensity([x,y])                                                                                                                                                                                          
end                                                                                                                                                                                                                
x = -30:0.1:30                                                                                                                                                                                                     
y = -30:0.1:50                                                                                                                                                                                                     
X = repmat(x',length(y),1)                                                                                                                                                                                         
Y = repmat(y,1,length(x))                                                                                                                                                                                          
Z = map(logdensity2d,Y,X)                                                                                                                                                                                          
p1 = contour(x,y,Z,200)                                                                                                                                                                                            
plot(vec(draws[1,:]),vec(draws[2,:]))                                                                                                                                                                              
plot(vec(pdraws[1,:]),vec(pdraws[2,:])) 
</pre>
<p>pdraws uses the covariance matrix Σ as the preconditioning matrix, whereas the first uses an identity matrix, resulting in the original MALA algorithm. The traceplot of draws from MALA and preconditioned MALA are shown in blue and green respectively&#8230;<br />
<div id="attachment_1163" style="width: 822px" class="wp-caption alignnone"><a href="http://www.lindonslog.com/wp-content/uploads/2016/10/mala.jpeg" rel="attachment wp-att-1163"><img src="http://www.lindonslog.com/wp-content/uploads/2016/10/mala.jpeg" alt="mala" width="812" height="612" class="size-full wp-image-1163" srcset="http://www.lindonslog.com/wp-content/uploads/2016/10/mala.jpeg 812w, http://www.lindonslog.com/wp-content/uploads/2016/10/mala-300x226.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2016/10/mala-768x579.jpeg 768w" sizes="(max-width: 812px) 100vw, 812px" /></a><p class="wp-caption-text">Traceplots for MALA and preconditioned MALA</p></div></p>
<h2>Effective Sample Sizes in R</h2>
<p>We can use the julia &#8220;RCall&#8221; package to switch over to R and use the coda library to evaluate the minimum effective sample size for both of these MCMC algorithms.</p>
<pre class="brush: r; title: ; notranslate">
julia&gt; library(coda)

R&gt; library(coda)

R&gt; min(effectiveSize($(draws')))
[1] 22.02418

R&gt; min(effectiveSize($(pdraws')))
[1] 50.85163
</pre>
<p>I didn&#8217;t tune the step size h in this example at all (you should).</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/">MALA &#8211; Metropolis Adjusted Langevin Algorithm in Julia</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Generate Random Inverse Gaussian in R</title>
		<link>http://www.lindonslog.com/programming/generate-random-inverse-gaussian-r/</link>
		<comments>http://www.lindonslog.com/programming/generate-random-inverse-gaussian-r/#comments</comments>
		<pubDate>Sat, 20 Sep 2014 21:57:19 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=1090</guid>
		<description><![CDATA[<p>Needed to generate draws from an inverse Gaussian today, so I wrote the following Rcpp code: It seems to be faster than existing implementations such as rig from mgcv and rinvgauss from statmod packages. rename rrinvgauss as desired.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/generate-random-inverse-gaussian-r/">Generate Random Inverse Gaussian in R</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Needed to generate draws from an inverse Gaussian today, so I wrote the following Rcpp code:</p>
<pre class="brush: cpp; title: ; notranslate">
#include &lt;RcppArmadillo.h&gt;
// [[Rcpp::depends(RcppArmadillo)]]

using namespace Rcpp;
using namespace arma;

// [[Rcpp::export]]
Col&lt;double&gt; rrinvgauss(int n, double mu, double lambda){

	Col&lt;double&gt; random_vector(n);
	double z,y,x,u;

	for(int i=0; i&lt;n; ++i){
		z=R::rnorm(0,1);
		y=z*z;
		x=mu+0.5*mu*mu*y/lambda - 0.5*(mu/lambda)*sqrt(4*mu*lambda*y+mu*mu*y*y);
		u=R::runif(0,1);
		if(u &lt;= mu/(mu+x)){
			random_vector(i)=x;
		}else{
			random_vector(i)=mu*mu/x;
		};
	}
	return(random_vector);
}
</pre>
<p>It seems to be faster than existing implementations such as rig from <a href="http://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/rig.html">mgcv</a> and rinvgauss from <a href="http://ugrad.stat.ubc.ca/R/library/statmod/html/invgauss.html">statmod</a> packages.</p>
<pre class="brush: r; title: ; notranslate">
library(Rcpp) 
library(RcppArmadillo)
library(rbenchmark)
library(statmod)
library(mgcv)
sourceCpp(&quot;rrinvgauss.cpp&quot;)
n=10000 
benchmark(rig(n,1,1),rinvgauss(n,1,1),rrinvgauss(n,1,1),replications=100)
</pre>
<p><a href="http://www.lindonslog.com/wp-content/uploads/2014/09/inversegaussian.png"><img src="http://www.lindonslog.com/wp-content/uploads/2014/09/inversegaussian.png" alt="inverse gaussian" width="923" height="97" class="aligncenter size-full wp-image-1092" srcset="http://www.lindonslog.com/wp-content/uploads/2014/09/inversegaussian.png 923w, http://www.lindonslog.com/wp-content/uploads/2014/09/inversegaussian-300x31.png 300w" sizes="(max-width: 923px) 100vw, 923px" /></a></p>
<p>rename rrinvgauss as desired.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/generate-random-inverse-gaussian-r/">Generate Random Inverse Gaussian in R</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/programming/generate-random-inverse-gaussian-r/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>Generalized Double Pareto Priors for Regression</title>
		<link>http://www.lindonslog.com/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/</link>
		<comments>http://www.lindonslog.com/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/#respond</comments>
		<pubDate>Thu, 11 Sep 2014 01:45:58 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=1067</guid>
		<description><![CDATA[<p>This post is a review of the &#8220;GENERALIZED DOUBLE PARETO SHRINKAGE&#8221; Statistica Sinica (2012) paper by Armagan, Dunson and Lee. Consider the regression model \(Y=X\beta+\varepsilon\) where we put a generalized double pareto distribution as the prior on the regression coefficients \(\beta\). The GDP distribution has density $$\begin{equation} f(\beta&#124;\xi,\alpha)=\frac{1}{2\xi}\left( 1+\frac{&#124;\beta&#124;}{\alpha\xi} \right)^{-(\alpha+1)}. \label{} \end{equation}$$ GDP as Scale [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/">Generalized Double Pareto Priors for Regression</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[
<p>This post is a review of the &#8220;GENERALIZED DOUBLE PARETO SHRINKAGE&#8221; Statistica Sinica (2012) paper by Armagan, Dunson and Lee. </p>
<p>Consider the regression model \(Y=X\beta+\varepsilon\) where we put a generalized double pareto distribution as the prior on the regression coefficients \(\beta\). The GDP distribution has density<br />
$$\begin{equation}<br />
	f(\beta|\xi,\alpha)=\frac{1}{2\xi}\left( 1+\frac{|\beta|}{\alpha\xi} \right)^{-(\alpha+1)}.<br />
	\label{}<br />
\end{equation}$$</p>
<h2>GDP as Scale Mixture of Normals</h2>
<p>The GDP distribution can be conveniently represented as a scale mixture of normals. Let<br />
$$\begin{align*}<br />
\beta_{i}|\phi,\tau_{i} &#038;\sim N(0,\phi^{-1}\tau_{i})\\<br />
\tau_{i}|\lambda_{i}&#038;\sim Exp(\frac{\lambda_{i}^{2}}{2})\\<br />
\lambda_{i}&#038;\sim Ga(\alpha,\eta)\\<br />
\end{align*}$$<br />
 then \(\beta|\phi \sim GDP(\xi=\frac{\eta}{\sqrt{\phi}\alpha},\alpha)\).<br />
To see this first note that \(\beta_{i}|\phi,\lambda_{i}\) has a Laplace or Double Exponential distribution with rate parameter \(\sqrt{\phi}\lambda_{i}\).<br />
$$\begin{align*}<br />
	p(\beta_{i}|\phi,\lambda_{i})&#038;=\int p(\beta_{i}|\phi,\tau_{i})p(\tau_{i}|\lambda_{i})d\tau_{i}\\<br />
	\psi(t)&#038;=\int e^{it\beta_{i}} \int p(\beta_{i}|\phi,\tau_{i})p(\tau_{i}|\lambda_{i})d\tau_{i} d\beta_{i}\\<br />
	&#038;=\int \int e^{it\beta_{i}}p(\beta_{i}|\phi,\tau_{i})d\beta_{i}p(\tau_{i}|\lambda_{i})d\tau_{i}\\<br />
	&#038;=\int e^{-\frac{1}{2}\frac{\tau_{i}}{\phi}t^{2}}p(\tau_{i}|\lambda_{i})d\tau_{i}\\<br />
	&#038;=\frac{\lambda_{i}^{2}}{2} \int e^{-\frac{1}{2}(\frac{t^{2}}{\phi}+\frac{\lambda_{i}^{2}}{2})\tau_{i}}d\tau_{i}\\<br />
	&#038;=\frac{\phi\lambda_{i}^{2}}{t^{2}+\phi\lambda_{i}^{2}},<br />
\end{align*}$$<br />
which is the characteristic function of a Double Exponential distribution with rate parameter \(\sqrt{\phi}\lambda_{i}\).<br />
Lastly<br />
$$\begin{align*}<br />
	p(\beta_{i}|\phi)&#038;=\int p(\beta_{i}|\phi,\lambda_{i})p(\lambda_{i})d\lambda_{i}\\<br />
	&#038;=\frac{1}{2}\sqrt{\phi}\frac{\eta^{\alpha}}{\Gamma(\alpha)}\frac{\Gamma(\alpha+1)}{(\eta+\sqrt{\phi}|\beta_{i}|)^{\alpha+1}}\\<br />
	&#038;=\frac{1}{2}\frac{\sqrt{\phi}\alpha}{\eta}\left( 1+\frac{\sqrt{\phi}\alpha}{\eta}\frac{|\beta_{i}|}{\alpha} \right)^{-(\alpha+1)},<br />
\end{align*}$$<br />
which is the density of a \(GDP(\xi=\frac{\eta}{\sqrt{\phi}\alpha},\alpha)\).</p>
<h2>EM Algorithm</h2>
<p>\(\tau_{i}\) and \(\lambda_{i}\) are treated as missing data for each \(i\).<br />
\begin{align*}<br />
	Q(\beta,\phi||\beta^{(t)},\phi^{(t)})&#038;=c+\mathbb{E}_{\tau,\lambda}\left[ \log p(\beta,\phi|Y,\tau,\lambda)|\beta^{(t)},\phi^{(t)} \right]\\<br />
	&#038;=\frac{n+p-3}{2}\log\phi &#8211; \frac{\phi}{2}||Y-X\beta||^{2}-\frac{\phi}{2}\sum_{i=1}^{p}\beta_{i}^{2}\mathbb{E}\left[ \frac{1}{\tau_{i}} \right]\\<br />
\end{align*}</p>
<h2>Expectation</h2>
<p>For the iterated expectation one needs the distribution \(\tau_{i}|\lambda_{i},\beta_{i},\phi\) and \(\lambda_{i}|\beta_{i},\phi\).<br />
\begin{align*}<br />
	p(\tau_{i}|\beta_{i},\lambda_{i},\phi)&#038;\propto p(\beta_{i}|\phi,\tau_{i})p(\tau_{i}|\lambda_{i})\\<br />
	&#038;\propto \left( \frac{1}{\tau_{i}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}(\frac{\phi \beta_{i}^{2}}{\tau_{i}}+\lambda_{i}^{2}\tau_{i})}<br />
\end{align*}<br />
This is the kernel of a Generalized Inverse Gaussian distribution, specifically \(p(\tau_{i}|\beta_{i},\lambda_{i},\phi)=GIG(\tau_{i}:\lambda_{i}^{2},\phi \beta_{i}^{2},\frac{1}{2})\).<br />
 By a standard change of variables it follows that \(p(\frac{1}{\tau_{i}}|\beta_{i},\lambda_{i},\phi)=IG(\frac{1}{\tau_{i}}:\sqrt{\frac{\lambda_{i}^{2}}{\phi \beta_{i}^{2}}},\lambda_{i}^{2})\) and so \(\mathbb{E}\left[ \frac{1}{\tau_{i}}|\lambda_{i},\beta^{(t)},\phi^{(t)} \right]=\frac{\lambda_{i}}{\sqrt{\phi^{(t)}}|\beta_{i}^{(t)}|}\).</p>
<p>Recall that \(p(\beta_{i}|\phi,\lambda_{i})\) has a double exponential distribution with rate \(\sqrt{\phi}\lambda_{i}\).<br />
 Hence from \(p(\lambda_{i}|\beta_{i},\phi)\propto p(\beta_{i}|\lambda_{i},\phi)p(\lambda_{i})\) it follows that \(\lambda_{i}|\beta_{i},\phi \sim Ga(\alpha+1,\eta+\sqrt{\phi}|\beta_{i}|)\), then performing the expectation with respect to \(\lambda_{i}\) yields<br />
\begin{align*}<br />
	\mathbb{E}\left[ \frac{1}{\tau_{i}}|\beta^{(t)},\phi^{(t)} \right]=\left( \frac{\alpha+1}{\eta+\sqrt{\phi^{t}}|\beta_{i}^{(t)}|} \right)\left( \frac{1}{\sqrt{\phi^{(t)}}|\beta_{i}^{(t)}|} \right)<br />
\end{align*}</p>
<h2>Maximization</h2>
<p>Writing \(D^{(t)}=diag(\mathbb{E}[\frac{1}{\tau_{1}}],\dots,\mathbb{E}[\frac{1}{\tau_{p}}])\) the function to maximize is<br />
\begin{align*}<br />
	Q(\beta,\phi||\beta^{(t)},\phi^{(t)})&#038;=c+\mathbb{E}_{\tau,\lambda}\left[ \log p(\beta,\phi|Y,\tau,\lambda)|\beta^{(t)},\phi^{(t)} \right]\\<br />
	&#038;=\frac{n+p-3}{2}\log\phi &#8211; \frac{\phi}{2}||Y-X\beta||^{2}-\frac{\phi}{2}\beta^{&#8216;}D^{(t)}\beta,\\<br />
\end{align*}<br />
which is maximized by letting<br />
\begin{align*}<br />
	\beta^{(t+1)}&#038;=(X^{&#8216;}X+D^{(t)})^{-1}X^{&#8216;}Y\\<br />
	\phi^{(t+1)}&#038;=\frac{n+p-3}{Y^{&#8216;}(I-X(X^{&#8216;}X+D^{(t)})^{-1}X^{&#8216;})Y}\\<br />
	&#038;=\frac{n+p-3}{||Y-X\beta^{(t+1)}||^{2}+\beta^{(t+1)&#8217;}D^(t)\beta^{(t+1)}}\\<br />
\end{align*}</p>
<h2>R CPP Code</h2>
<pre class="brush: cpp; title: ; notranslate">
#include &lt;RcppArmadillo.h&gt;
// [[Rcpp::depends(RcppArmadillo)]]

using namespace Rcpp;
using namespace arma;

double gdp_log_posterior_density(int no, int p, double alpha, double eta, const Col&lt;double&gt;&amp; yo, const Mat&lt;double&gt;&amp; xo, const Col&lt;double&gt;&amp; B,double phi);

// [[Rcpp::export]]
List gdp_em(NumericVector ryo, NumericMatrix rxo, SEXP ralpha, SEXP reta){

	//Define Variables//
	int p=rxo.ncol();
	int no=rxo.nrow();
	double eta=Rcpp::as&lt;double &gt;(reta);
	double alpha=Rcpp::as&lt;double &gt;(ralpha);

	//Create Data//
	arma::mat xo(rxo.begin(), no, p, false);
	arma::colvec yo(ryo.begin(), ryo.size(), false);
	yo-=mean(yo);

	//Pre-Processing//
	Col&lt;double&gt; xoyo=xo.t()*yo;
	Col&lt;double&gt; B=xoyo/no;
	Col&lt;double&gt; Babs=abs(B);
	Mat&lt;double&gt; xoxo=xo.t()*xo;
	Mat&lt;double&gt; D=eye(p,p);
	Mat&lt;double&gt; Ip=eye(p,p);
	double yoyo=dot(yo,yo);
	double deltaB;
	double deltaphi;
	double phi=no/dot(yo-xo*B,yo-xo*B);
	double lp;

	//Create Trace Matrices
	Mat&lt;double&gt; B_trace(p,20000);
	Col&lt;double&gt; phi_trace(20000);
	Col&lt;double&gt; lpd_trace(20000);

	//Run EM Algorithm//
	cout &lt;&lt; &quot;Beginning EM Algorithm&quot; &lt;&lt; endl;
	int t=0;
	B_trace.col(t)=B;
	phi_trace(t)=phi;
	lpd_trace(t)=gdp_log_posterior_density(no,p,alpha,eta,yo,xo,B,phi);
	do{
		t=t+1;


		Babs=abs(B);
		D=diagmat(sqrt(((eta+sqrt(phi)*Babs)%(sqrt(phi)*Babs))/(alpha+1)));
		B=D*solve(D*xoxo*D+Ip,D*xoyo);

		phi=(no+p-3)/(yoyo-dot(xoyo,B));

		//Store Values//
		B_trace.col(t)=B;
		phi_trace(t)=phi;
		lpd_trace(t)=gdp_log_posterior_density(no,p,alpha,eta,yo,xo,B,phi);

		deltaB=dot(B_trace.col(t)-B_trace.col(t-1),B_trace.col(t)-B_trace.col(t-1));
		deltaphi=phi_trace(t)-phi_trace(t-1);
	} while((deltaB&gt;0.00001 || deltaphi&gt;0.00001) &amp;&amp; t&lt;19999);
	cout &lt;&lt; &quot;EM Algorithm Converged in &quot; &lt;&lt; t &lt;&lt; &quot; Iterations&quot; &lt;&lt; endl;

	//Resize Trace Matrices//
	B_trace.resize(p,t);
	phi_trace.resize(t);
	lpd_trace.resize(t);

	return Rcpp::List::create(
			Rcpp::Named(&quot;B&quot;) = B,
			Rcpp::Named(&quot;B_trace&quot;) = B_trace,
			Rcpp::Named(&quot;phi&quot;) = phi,
			Rcpp::Named(&quot;phi_trace&quot;) = phi_trace,
			Rcpp::Named(&quot;lpd_trace&quot;) = lpd_trace
			) ;

}



double gdp_log_posterior_density(int no, int p, double alpha, double eta, const Col&lt;double&gt;&amp; yo, const Mat&lt;double&gt;&amp; xo, const Col&lt;double&gt;&amp; B,double phi){

	double lpd;
	double xi=eta/(sqrt(phi)*alpha);
	lpd=(double)0.5*((double)no-1)*log(phi/(2*M_PI))-p*log(2*xi)-(alpha+1)*sum(log(1+abs(B)/(alpha*xi)))-0.5*phi*dot(yo-xo*B,yo-xo*B)-log(phi);
	return(lpd);

}
</pre>
<h2>An Example in R</h2>
<pre class="brush: r; title: ; notranslate">
rm(list=ls())
library(Rcpp)
library(RcppArmadillo)
sourceCpp(&quot;src/gdp_em.cpp&quot;)

#Generate Design Matrix
set.seed(3)
no=100
foo=rnorm(no,0,1)
sd=4
xo=cbind(foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd))
for(i in 1:40) xo=cbind(xo,foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd))

#Scale and Center Design Matrix
xo=scale(xo,center=T,scale=F)
var=apply(xo^2,2,sum)
xo=scale(xo,center=F,scale=sqrt(var/no))

#Generate Data under True Model
p=dim(xo)[2]
b=rep(0,p)
b[1]=1
b[2]=2
b[3]=3
b[4]=4
b[5]=5
xo%*%b
yo=xo%*%b+rnorm(no,0,1)
yo=yo-mean(yo)

#Run GDP
gdp=gdp_em(yo,xo,100,100)

#Posterior Density Increasing at Every Iteration?
gdp$lpd_trace[2:dim(gdp$lpd_trace)[1],1]-gdp$lpd_trace[1:(dim(gdp$lpd_trace)[1]-1),1]&gt;=0
mean(gdp$lpd_trace[2:dim(gdp$lpd_trace)[1],1]-gdp$lpd_trace[1:(dim(gdp$lpd_trace)[1]-1),1]&gt;=0)

#Plot Results
plot(gdp$B,ylab=expression(beta[GDP]),main=&quot;GDP MAP Estimate of Regression Coefficients&quot;)
</pre>
<p><a href="http://www.lindonslog.com/wp-content/uploads/2014/09/gdp.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2014/09/gdp.jpeg" alt="Generalized Double Pareto Estimated Coefficients" width="815" height="534" class="aligncenter size-full wp-image-1081" srcset="http://www.lindonslog.com/wp-content/uploads/2014/09/gdp.jpeg 815w, http://www.lindonslog.com/wp-content/uploads/2014/09/gdp-300x196.jpeg 300w" sizes="(max-width: 815px) 100vw, 815px" /></a></p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2Fhttp%3A%2F%2Fdx.doi.org%2F10.1093%2Fbiomet%2F74.3.646&#038;rft.atitle=On+scale+mixtures+of+normal+distributions&#038;rft.jtitle=Biometrika&#038;rft.volume=74&#038;rft.issue=3&#038;rft.spage=646&#038;rft.epage=648&#038;rft.date=1987&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=WEST+MIKE&#038;rft.aulast=WEST&#038;rft.aufirst=MIKE&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1">WEST M. (1987). On scale mixtures of normal distributions, <span style="font-style:italic;">Biometrika, 74</span> (3) 646-648. DOI: <a rev="review" href="http://dx.doi.org/10.1093/biomet/74.3.646">http://dx.doi.org/10.1093/biomet/74.3.646</a></span></p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft.jtitle=Statistica+Sinica+23+%282013%29%2C+119-143&#038;rft_id=info%3Aarxiv%2F1104.0861v4&#038;rfr_id=info%3Asid%2Fresearchblogging.org&#038;rft.atitle=Generalized+double+Pareto+shrinkage&#038;rft.issn=&#038;rft.date=2011&#038;rft.volume=&#038;rft.issue=&#038;rft.spage=&#038;rft.epage=&#038;rft.artnum=&#038;rft.au=Artin+Armagan&#038;rft.au=David+Dunson&#038;rft.au=Jaeyong+Lee&#038;rfe_dat=bpr3.included=1;bpr3.tags=Mathematics%2CApplied+Mathematics%2C+Probability+and+Statistics%2C+Parallel+and+Distributed+Computing%2C+Algorithms%2C+Operating+Systems">Artin Armagan, David Dunson, &#038; Jaeyong Lee (2011). Generalized double Pareto shrinkage <span style="font-style: italic;">Statistica Sinica 23 (2013), 119-143</span> arXiv: <a rev="review" href="http://arxiv.org/abs/1104.0861v4">1104.0861v4</a></span></p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2Fhttp%3A%2F%2Fdx.doi.org%2F10.1109%2Ftpami.2003.1227989&#038;rft.atitle=Adaptive+sparseness+for+supervised+learning&#038;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&#038;rft.volume=25&#038;rft.issue=9&#038;rft.spage=1150&#038;rft.epage=1159&#038;rft.date=2003&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Figueiredo+M.A.T.&#038;rft.aulast=Figueiredo&#038;rft.aufirst=M.A.T.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1">Figueiredo M.A.T. (2003). Adaptive sparseness for supervised learning, <span style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence, 25</span> (9) 1150-1159. DOI: <a rev="review" href="http://dx.doi.org/10.1109/tpami.2003.1227989">http://dx.doi.org/10.1109/tpami.2003.1227989</a></span></p>
<p>Also see this <a href="http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/" title="lasso" target="_blank">similar post on the Bayesian lasso</a>.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/">Generalized Double Pareto Priors for Regression</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>EM Algorithm for Bayesian Lasso R Cpp Code</title>
		<link>http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/</link>
		<comments>http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/#comments</comments>
		<pubDate>Fri, 05 Sep 2014 16:56:55 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=1035</guid>
		<description><![CDATA[<p>Bayesian Lasso $$\begin{align*} p(Y_{o}&#124;\beta,\phi)&#038;=N(Y_{o}&#124;1\alpha+X_{o}\beta,\phi^{-1} I_{n{o}})\\ \pi(\beta_{i}&#124;\phi,\tau_{i}^{2})&#038;=N(\beta_{i}&#124;0, \phi^{-1}\tau_{i}^{2})\\ \pi(\tau_{i}^{2})&#038;=Exp \left( \frac{\lambda}{2} \right)\\ \pi(\phi)&#038;\propto \phi^{-1}\\ \pi(\alpha)&#038;\propto 1\\ \end{align*}$$ Marginalizing over \(\alpha\) equates to centering the observations and losing a degree of freedom and working with the centered \( Y_{o} \). Mixing over \(\tau_{i}^{2}\) leads to a Laplace or Double Exponential prior on \(\beta_{i}\) with rate parameter \(\sqrt{\phi\lambda}\) [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/">EM Algorithm for Bayesian Lasso R Cpp Code</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[
<h1>Bayesian Lasso </h1>
<p>$$\begin{align*}<br />
	p(Y_{o}|\beta,\phi)&#038;=N(Y_{o}|1\alpha+X_{o}\beta,\phi^{-1} I_{n{o}})\\<br />
	\pi(\beta_{i}|\phi,\tau_{i}^{2})&#038;=N(\beta_{i}|0, \phi^{-1}\tau_{i}^{2})\\<br />
	\pi(\tau_{i}^{2})&#038;=Exp \left( \frac{\lambda}{2} \right)\\<br />
	\pi(\phi)&#038;\propto \phi^{-1}\\<br />
	\pi(\alpha)&#038;\propto 1\\<br />
\end{align*}$$</p>
<p>Marginalizing over \(\alpha\) equates to centering the observations and losing a degree of freedom and working with the centered \( Y_{o} \).<br />
Mixing over \(\tau_{i}^{2}\) leads to a Laplace or Double Exponential prior on \(\beta_{i}\) with rate parameter \(\sqrt{\phi\lambda}\) as seen by considering the characteristic function<br />
$$\begin{align*}<br />
	\varphi_{\beta_{i}|\phi}(t)&#038;=\int e^{jt\beta_{i}}\pi(\beta_{i}|\phi)d\beta_{i}\\<br />
	&#038;=\int \int e^{jt\beta_{i}}\pi(\beta_{i}|\phi,\tau_{i}^{2})\pi(\tau_{i}^{2})d\tau_{i} d\beta_{i}\\<br />
	&#038;=\frac{\lambda}{2} \int  e^{-\frac{1}{2}\frac{t^{2}}{\phi}\tau_{i}^{2}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}\\<br />
	&#038;=\frac{\lambda}{\frac{t^{2}}{\phi}+\lambda}=\frac{\lambda\phi}{t^{2}+\lambda\phi}<br />
\end{align*}$$.</p>
<h1>EM Algorithm</h1>
<p>The objective is to find the mode of the joint posterior \(\pi(\beta,\phi|Y_{o})\). It is easier, however, to find the joint mode of \(\pi(\beta,\phi|Y_{o},\tau^{2})\) and use EM to exploit the scale mixture representation.</p>
<p>$$\begin{align*}<br />
	\log \pi(\beta,\phi|Y_{o},\tau^{2})=c+ \frac{n_o+p-3}{2}\log \phi -\frac{\phi}{2}||Y_{o}-X_{o}\beta||^{2}-\sum_{i=1}^{p}\frac{\phi}{2}\frac{1}{\tau_{i}^{2}}\beta^{2}_{i}<br />
\end{align*}$$</p>
<h2>Expectation</h2>
<p>The expecation w.r.t. \(\tau_{i}^{2}\) is handled as by<br />
$$<br />
\begin{align*}<br />
	&#038;\frac{\lambda}{2}\int \frac{1}{\tau_{i}^{2}}\left( \frac{\phi}{2\pi\tau_{i}^{2}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}^{2}\\<br />
	&#038;\frac{\lambda}{2}\int \left( \frac{\phi}{2\pi[\tau_{i}^{2}]^{3}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}^{2}\\<br />
\end{align*}$$</p>
<p>This has the kernel of an Inverse Gaussian distribution with shape parameter \(\phi \beta_{i}^{2}\) and mean \(\sqrt{\frac{\phi}{\lambda}}|\beta_{i}|\)</p>
<p>$$\begin{align*}<br />
	&#038;\frac{{\lambda}}{2|\beta_{i}|}\int \left( \frac{\beta_{i}^{2}\phi}{2\pi[\tau_{i}^{2}]^{3}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}^{2}\\<br />
	&#038;\frac{\lambda}{2|\beta_{i}|}e^{-\sqrt{\lambda\phi\beta_{i}^{2}}}\int \left( \frac{\beta_{i}^{2}\phi}{2\pi[\tau_{i}^{2}]^{3}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}e^{\sqrt{\lambda\phi\beta_{i}^{2}}}d\tau_{i}^{2}\\<br />
	&#038;\frac{\lambda}{2|\beta_{i}|}e^{-\sqrt{\lambda\phi\beta_{i}^{2}}}\\<br />
\end{align*}$$</p>
<p>Normalization as follows</p>
<p>$$\begin{align*}<br />
	&#038;\frac{\lambda}{2}\int \left( \frac{\phi}{2\pi\tau_{i}^{2}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}^{2}\\<br />
	&#038;\frac{\lambda}{2}\int \tau_{i}^{2}\left( \frac{\phi}{2\pi[\tau_{i}^{2}]^{3}} \right)^{\frac{1}{2}}e^{-\frac{1}{2}\phi\beta_{i}^{2}\frac{1}{\tau_{i}^{2}}}e^{-\frac{\lambda}{2}\tau_{i}^{2}}d\tau_{i}^{2}\\<br />
\end{align*}$$<br />
$$\begin{align*}<br />
	&#038;\frac{\lambda}{2|\beta_{i}|}e^{-\sqrt{\lambda\phi\beta_{i}^{2}}}\sqrt{\frac{\phi}{\lambda}}|\beta_{i}|\\<br />
\end{align*}$$</p>
<p>\( \Rightarrow \mathbb{E}\left[ \frac{1}{\tau_{i}^{2}} \right]=\sqrt{\frac{\lambda}{\phi^{t}}}\frac{1}{|\beta_{i}^{t}|}\).<br />
 Let \(\Lambda^{t}=diag(\sqrt{\frac{\lambda}{\phi^{t}}}\frac{1}{|\beta_{1}^{t}|}, \dots, \sqrt{\frac{\lambda}{\phi^{t}}}\frac{1}{|\beta_{p}^{t}|})\).</p>
<h2>Maximization</h2>
<p>$$\begin{align*}<br />
	&#038;Q(\beta,\phi||\beta^{t},\phi^{t})=c+ \frac{n_o+p-3}{2}\log \phi -\frac{\phi}{2}||Y_{o}-X_{o}\beta||^{2} &#8211; \frac{\phi}{2}\beta^{T}\Lambda^{t}\beta\\<br />
	&#038;=c+ \frac{n_o+p-3}{2}\log \phi -\frac{\phi}{2}||\beta-(X_{o}^{T}X_{o}+\Lambda^{t})^{-1}X_{o}^{T}Y_{o}||^{2}_{(X_{o}^{T}X_{o}+\Lambda^{t})}-\frac{\phi}{2}Y_{o}^{T}(I_{n_{o}}-X_{o}^{T}(X_{o}^{T}X_{o}+\Lambda^{t})^{-1}X_{o})Y_{o}\\<br />
\end{align*}$$</p>
<p>$$\begin{align*}<br />
	\beta^{t+1}&#038;=(X_{o}^{T}X_{o}+\Lambda^{t})^{-1}X_{o}^{T}Y_{o}\\<br />
\end{align*}$$</p>
<p>$$\begin{align*}<br />
	\phi^{t+1}=\frac{n_{o}+p-3}{Y_{o}^{T}(I_{n_{o}}-X_{o}^{T}(X_{o}^{T}X_{o}+\Lambda^{t})^{-1}X_{o})Y_{o}}<br />
\end{align*}$$</p>
<h2>RCpp C++ Code</h2>
<pre class="brush: cpp; title: ; notranslate">
#include &lt;RcppArmadillo.h&gt;
// [[Rcpp::depends(RcppArmadillo)]]

using namespace Rcpp;
using namespace arma;

double or_log_posterior_density(int no, int p, double lasso, const Col&lt;double&gt;&amp; yo, const Mat&lt;double&gt;&amp; xo, const Col&lt;double&gt;&amp; B,double phi);

// [[Rcpp::export]]
List or_lasso_em(NumericVector ryo, NumericMatrix rxo, SEXP rlasso){

	//Define Variables//
	int p=rxo.ncol();
	int no=rxo.nrow();
	double lasso=Rcpp::as&lt;double &gt;(rlasso);

	//Create Data//
	arma::mat xo(rxo.begin(), no, p, false);
	arma::colvec yo(ryo.begin(), ryo.size(), false);
	yo-=mean(yo);

	//Pre-Processing//
	Col&lt;double&gt; xoyo=xo.t()*yo;
	Col&lt;double&gt; B=xoyo/no;
	Col&lt;double&gt; Babs=abs(B);
	Mat&lt;double&gt; xoxo=xo.t()*xo;
	Mat&lt;double&gt; D=eye(p,p);
	Mat&lt;double&gt; Ip=eye(p,p);
	double yoyo=dot(yo,yo);
	double deltaB;
	double deltaphi;
	double phi=no/dot(yo-xo*B,yo-xo*B);
	double lp;

	//Create Trace Matrices
	Mat&lt;double&gt; B_trace(p,20000);
	Col&lt;double&gt; phi_trace(20000);
	Col&lt;double&gt; lpd_trace(20000);

	//Run EM Algorithm//
	cout &lt;&lt; &quot;Beginning EM Algorithm&quot; &lt;&lt; endl;
	int t=0;
	B_trace.col(t)=B;
	phi_trace(t)=phi;
	lpd_trace(t)=or_log_posterior_density(no,p,lasso,yo,xo,B,phi);
	do{
		t=t+1;

		lp=sqrt(lasso/phi);

		Babs=abs(B);
		D=diagmat(sqrt(Babs));
		B=D*solve(D*xoxo*D+lp*Ip,D*xoyo);

		phi=(no+p-3)/(yoyo-dot(xoyo,B));

		//Store Values//
		B_trace.col(t)=B;
		phi_trace(t)=phi;
		lpd_trace(t)=or_log_posterior_density(no,p,lasso,yo,xo,B,phi);

		deltaB=dot(B_trace.col(t)-B_trace.col(t-1),B_trace.col(t)-B_trace.col(t-1));
		deltaphi=phi_trace(t)-phi_trace(t-1);
	} while((deltaB&gt;0.00001 || deltaphi&gt;0.00001) &amp;&amp; t&lt;19999);
	cout &lt;&lt; &quot;EM Algorithm Converged in &quot; &lt;&lt; t &lt;&lt; &quot; Iterations&quot; &lt;&lt; endl;

	//Resize Trace Matrices//
	B_trace.resize(p,t);
	phi_trace.resize(t);
	lpd_trace.resize(t);

	return Rcpp::List::create(
			Rcpp::Named(&quot;B&quot;) = B,
			Rcpp::Named(&quot;B_trace&quot;) = B_trace,
			Rcpp::Named(&quot;phi&quot;) = phi,
			Rcpp::Named(&quot;phi_trace&quot;) = phi_trace,
			Rcpp::Named(&quot;lpd_trace&quot;) = lpd_trace
			) ;

}




double or_log_posterior_density(int no, int p, double lasso, const Col&lt;double&gt;&amp; yo, const Mat&lt;double&gt;&amp; xo, const Col&lt;double&gt;&amp; B,double phi){

	double lpd;
	lpd=(double)0.5*((double)no-1)*log(phi/(2*M_PI))-0.5*phi*dot(yo-xo*B,yo-xo*B)+0.5*(double)p*log(phi*lasso)-sqrt(phi*lasso)*sum(abs(B))-log(phi);
	return(lpd);

}
</pre>
<h2>An Example in R</h2>
<pre class="brush: r; title: ; notranslate">
rm(list=ls())

#Generate Design Matrix
set.seed(3)
no=100
foo=rnorm(no,0,1)
sd=4
xo=cbind(foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd))
for(i in 1:40) xo=cbind(xo,foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd),foo+rnorm(no,0,sd))

#Scale and Center Design Matrix
xo=scale(xo,center=T,scale=F)
var=apply(xo^2,2,sum)
xo=scale(xo,center=F,scale=sqrt(var/no))

#Generate Data under True Model
p=dim(xo)[2]
b=rep(0,p)
b[1]=1
b[2]=2
b[3]=3
b[4]=4
b[5]=5
xo%*%b
yo=xo%*%b+rnorm(no,0,1)
yo=yo-mean(yo)

#Run Lasso
or_lasso=or_lasso_em(yo,xo,100)

#Posterior Density Increasing at Every Iteration?
or_lasso$lpd_trace[2:dim(or_lasso$lpd_trace)[1],1]-or_lasso$lpd_trace[1:(dim(or_lasso$lpd_trace)[1]-1),1]&gt;=0
mean(or_lasso$lpd_trace[2:dim(or_lasso$lpd_trace)[1],1]-or_lasso$lpd_trace[1:(dim(or_lasso$lpd_trace)[1]-1),1]&gt;=0)

#Plot Results
plot(or_lasso$B,ylab=expression(beta[lasso]),main=&quot;Lasso MAP Estimate of Regression Coefficients&quot;)
</pre>
<p><a href="http://www.lindonslog.com/wp-content/uploads/2014/09/lasso.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2014/09/lasso.jpeg" alt="MAP regression coefficients" width="815" height="566" class="aligncenter size-full wp-image-1062" srcset="http://www.lindonslog.com/wp-content/uploads/2014/09/lasso.jpeg 815w, http://www.lindonslog.com/wp-content/uploads/2014/09/lasso-300x208.jpeg 300w" sizes="(max-width: 815px) 100vw, 815px" /></a></p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft.jtitle=Journal+of+the+American+Statistical+Association&#038;rft_id=info%3Adoi%2F10.1198%2F016214508000000337&#038;rfr_id=info%3Asid%2Fresearchblogging.org&#038;rft.atitle=The+Bayesian+Lasso&#038;rft.issn=0162-1459&#038;rft.date=2008&#038;rft.volume=103&#038;rft.issue=482&#038;rft.spage=681&#038;rft.epage=686&#038;rft.artnum=http%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1198%2F016214508000000337&#038;rft.au=Park%2C+T.&#038;rft.au=Casella%2C+G.&#038;rfe_dat=bpr3.included=1;bpr3.tags=Mathematics%2CApplied+Mathematics%2C+Probability+and+Statistics%2C+Algorithms">Park, T., &#038; Casella, G. (2008). The Bayesian Lasso <span style="font-style: italic;">Journal of the American Statistical Association, 103</span> (482), 681-686 DOI: <a rev="review" href="http://dx.doi.org/10.1198/016214508000000337">10.1198/016214508000000337</a></span><br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2Fhttp%3A%2F%2Fdx.doi.org%2F10.1109%2Ftpami.2003.1227989&#038;rft.atitle=Adaptive+sparseness+for+supervised+learning&#038;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&#038;rft.volume=25&#038;rft.issue=9&#038;rft.spage=1150&#038;rft.epage=1159&#038;rft.date=2003&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Figueiredo+M.A.T.&#038;rft.aulast=Figueiredo&#038;rft.aufirst=M.A.T.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1">Figueiredo M.A.T. (2003). Adaptive sparseness for supervised learning, <span style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence, 25</span> (9) 1150-1159. DOI: <a rev="review" href="http://dx.doi.org/10.1109/tpami.2003.1227989">http://dx.doi.org/10.1109/tpami.2003.1227989</a></span><br />
Better Shrinkage Priors:<br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Apmid%2F24478567&#038;rft.atitle=GENERALIZED+DOUBLE+PARETO+SHRINKAGE.&#038;rft.jtitle=Statistica+Sinica&#038;rft.issn=1017-0405&#038;rfr_id=info%3Asid%2Fmathblogging.org&#038;rft.au=Armagan+Artin&#038;rft.aulast=Armagan&#038;rft.aufirst=Artin&#038;rft.au=Dunson+David+B&#038;rft.aulast=Dunson&#038;rft.aufirst=David+B&#038;rft.au=Lee+Jaeyong&#038;rft.aulast=Lee&#038;rft.aufirst=Jaeyong&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1">Armagan A., Dunson D.B. &#038; Lee J.  GENERALIZED DOUBLE PARETO SHRINKAGE., <span style="font-style:italic;">Statistica Sinica, </span>   PMID: <a rel="author" href="http://www.ncbi.nlm.nih.gov/pubmed/24478567">24478567</a></span></p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/">EM Algorithm for Bayesian Lasso R Cpp Code</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/mathematics/em-algorithm-bayesian-lasso-r-cpp-code/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>Compile R and OpenBLAS from Source Guide</title>
		<link>http://www.lindonslog.com/linux-unix/compile-r-openblas-source-guide/</link>
		<comments>http://www.lindonslog.com/linux-unix/compile-r-openblas-source-guide/#comments</comments>
		<pubDate>Wed, 16 Jul 2014 20:54:15 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>
		<category><![CDATA[Linux/Unix]]></category>
		<category><![CDATA[OpenMP]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=995</guid>
		<description><![CDATA[<p>1. Get OpenBLAS 2.1 Get R 2.2 Specific Instructions for DSS Users 3. Validation 4. Benchmark This guide is intended to aid any R and Linux user who desires a threaded version of BLAS. In particular I hope this will allow other grad students, who like me do not have many user privileges on their [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/linux-unix/compile-r-openblas-source-guide/">Compile R and OpenBLAS from Source Guide</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p><a href="#openblas">1. Get OpenBLAS</a><br />
<a href="#R">2.1 Get R</a><br />
<a href="#duke">2.2 Specific Instructions for DSS Users</a><br />
<a href="#validation">3. Validation</a><br />
<a href="#benchmark">4. Benchmark</a></p>
<p>This guide is intended to aid any R and Linux user who desires a threaded version of BLAS. In particular I hope this will allow other grad students, who like me do not have many user privileges on their office computer, to follow suit and exploit multiple cores to speed up their linear algebra computations within R. The following will be performed on <strong>Scientific Linux 6.4</strong> but has should be completely general. If you are a <strong>Ubuntu</strong> user, then there is an elegant and streamlined process for changing BLAS libraries and a recommended post about it <a href="http://www.stat.cmu.edu/~nmv/2013/07/09/for-faster-r-use-openblas-instead-better-than-atlas-trivial-to-switch-to-on-ubuntu/" title="ubuntu blas">here</a>. I use <strong>Fedora</strong> on my laptop, and the following has also been tested thereupon. </p>
<p>My office computer has a quadcore processor with two threads per core but I also have access to a departmental computer with 4 sockets and 12 cores per socket (1 thread per core), so it really makes sense to use a threaded version of BLAS. If you are curious about the hardware on your own computer you can run the command &#8220;cat /proc/cpuinfo&#8221; or &#8220;lscpu&#8221;.</p>
<p>Unfortunately my office computer is part of a network upon which I do not have permissions to change &#8216;/usr/lib64/R/lib/libRblas.so&#8217;. Moreover R appears to be running serially: if you start up R and get the PID (process ID) from &#8216;top&#8217; or &#8216;ps aux | grep R&#8217; or something and then execute &#8216;cat /proc/PID/status | grep Threads&#8217; you can see there is only one thread available.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~]$ cat /proc/13605/status | grep Threads
Threads:	1

</pre>
<p>(where 13605 was the process ID of my R process. That is using the default R on the network. One could appeal to the network administrator to change things for you but they probably won&#8217;t because a parallel BLAS implementation may cause problems for other users who require a serial BLAS, such as those that use the multicore environment to perform inherently parallel algorithms such as parallel tempering instead of using idle cores to speed up the linear algebra. There are also some known conflicts with the multicore package in R. There is, however, nothing stopping the user from compiling one&#8217;s own custom R build in one&#8217;s home directory and just changing the executable path thereto. In addition, you then have the power and freedom customize R to your needs &#8211; at the moment I have some very large matrices which would benefit from a threaded BLAS but at some point I may want to revert to a tuned serial BLAS such at ATLAS for certain parallel algorithms. </p>
<p>Firstly, go ahead and create a directory in which to keep all your custom software.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~]$ pwd
/home/grad/msl33
[msl33@cabbage ~]$ mkdir software

</pre>
<p><a name="openblas"></a><br />
<h1>Download OpenBLAS</h1>
<p>Make a directory &#8220;openblas&#8221; in the &#8220;software directory.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~]$ cd software/
[msl33@cabbage software]$ mkdir openblas

</pre>
<p>Next, grab the tarball from the <a href="http://www.openblas.net/" title="openblas homepage">OpenBLAS homepage</a>. Change directory into where you downloaded the tarball and extract the files from it.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~]$ cd Downloads/
[msl33@cabbage Downloads]$ tar -xvf xianyi-OpenBLAS-v0.2.9-0-gf773f49.tar.gz 

</pre>
<p>While this is running, fill a kettle with some water and turn it on, this stage is very important.</p>
<p>Change directory into where you extracted the files and verify that NO_AFFINITY=1 is uncommented in the Makefile.rule. If so proceed and run make.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~/Downloads]$ cd xianyi-OpenBLAS-347dded/
[msl33@cabbage xianyi-OpenBLAS-347dded]$ cat Makefile.rule | grep NO_AFFINITY
NO_AFFINITY = 1
[msl33@cabbage xianyi-OpenBLAS-347dded]$ make

</pre>
<p>Now is a good time to &#8220;make&#8221; some tea with the water prepared earlier. When done successfully one will see<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_complete.png"><img src="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_complete.png" alt="openblas confirmation" width="656" height="255" class="size-full wp-image-1002" srcset="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_complete.png 656w, http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_complete-300x116.png 300w" sizes="(max-width: 656px) 100vw, 656px" /></a><br />
Now, as instructed above, install to the &#8220;software&#8221; directory made earlier.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage xianyi-OpenBLAS-347dded]$ make PREFIX=/home/grad/msl33/software/openblas install
...
Install OK!
</pre>
<p>In  openblas/lib there will be a file &#8220;libopenblas.so&#8221;, needed for later. That&#8217;s it for openblas, next we will do R.</p>
<p><a name="R"></a><br />
<h1>Download R</h1>
<p>Let&#8217;s create an R directory in software. Go onto the R homepage, then download, then choose a <a href="http://mirrors.ebi.ac.uk/CRAN/">mirror</a> and grab the tarball of the latest version. Download it to your &#8220;software&#8221; directory and extract it as before with &#8220;tar -xvf R-3.1.1.tar.gz&#8221;. Once extracted, remove the tarball and change directory into R-3.1.1. Before running the configure script one might bring some customizations into consideration in the name of efficiency. One might consider upping the optimization level from 2 to 3 and adding march or mtune by editing &#8220;config.site&#8221; and changing &#8220;## CFLAGS=&#8221; on line 53 to &#8220;CFLAGS=&#8217;-O3 -march=native'&#8221; and making similar changes for FFLAGS and CXXFLAGS. It is noted in the <a href="http://cran.r-project.org/doc/manuals/r-release/R-admin.html#Compilation-flags">R Installation and Administration</a> documentation that these can produce worthwhile speedups but come with a warning that the build will be less reliable, with segfaults and numerical errors creeping in. It is safest to leave things <a href="http://www.youtube.com/watch?v=aYBkDxao3wg&#038;t=2m28s">regular</a> (reccommended link) but I&#8217;ll take the risk. Now, if you are not using a computer on the duke statistical science network, run the configure script, otherwise see the additional instructions before running configure.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage R-3.1.1]$ ./configure --prefix=/home/grad/msl33/software/R --enable-R-shlib --enable-BLAS-shlib --enable-memory-profiling --with-tcltk=no

</pre>
<p><a name="duke"></a><br />
<h3>BEGIN ADDITIONAL INSTRUCTIONS FOR DUKE STATISTICAL SCIENCE STUDENTS</h3>
<p>[On the DSS computers some further instructions are required to locate headers and libraries. The first time I tried to make on my office computer I encountered this <a href="http://stackoverflow.com/questions/17570586/unable-to-compile-jni-program-rjava">error</a>. &#8220;jni.h&#8221; could not be found. The error was resolved by locating it and then export the environment variable JAVA_HOME.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage software]$ locate jni.h
/usr/lib/jvm/java-1.7.0-sun-1.7.0.11/include/jni.h
[msl33@cabbage software]$ export JAVA_HOME=/usr/lib/jvm/java-1.7.0-sun-1.7.0.11/

</pre>
<p>In addition, when running the configure script the readline headers/libs could not be found. We&#8217;ll just borrow them from some other software. Add to CFLAGS, FFLAGS, CXXFLAGS &#8220;-I/opt/EPD_Free/include -L/opt/EPD_Free/lib&#8221; in addition to any other flags that you have set. Also make a lib directory and copy them in.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage R-3.1.1]$ mkdir lib
[msl33@cabbage R-3.1.1]$ cp /opt/EPD_Free/lib/libreadline.* lib/
[msl33@cabbage R-3.1.1]$ cp /opt/EPD_Free/lib/libncurses* lib/
</pre>
<p> Now run the configure line above.]</p>
<h3>END ADDITIONAL INSTRUCTIONS FOR DUKE STATISTICAL SCIENCE STUDENTS</h3>
<p>Once the configure has completed, you&#8217;ll see a summary below like<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2014/07/configure.png"><img src="http://www.lindonslog.com/wp-content/uploads/2014/07/configure-1024x383.png" alt="openblas configure" width="640" height="239" class="aligncenter size-large wp-image-1013" srcset="http://www.lindonslog.com/wp-content/uploads/2014/07/configure-1024x383.png 1024w, http://www.lindonslog.com/wp-content/uploads/2014/07/configure-300x112.png 300w, http://www.lindonslog.com/wp-content/uploads/2014/07/configure.png 1026w" sizes="(max-width: 640px) 100vw, 640px" /></a><br />
Now issue the command &#8220;make&#8221;, it will take some time. Once make has finished, you can execute &#8220;make install&#8221; to populate the software/R directory created earlier but you don&#8217;t need to. Change directories to lib and make a backup of libRblas.so and create a symbolic link to the openblas library that was made earlier.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage ~]$ cd software/R-3.1.1/lib
[msl33@cabbage lib]$ pwd
/home/grad/msl33/software/R-3.1.1/lib
[msl33@cabbage lib]$ mv libRblas.so libRblas.so.keep
[msl33@cabbage lib]$ ln -s /home/grad/msl33/software/openblas/lib/libopenblas.so libRblas.so

</pre>
<p>That was the last step. </p>
<p><a name="validation"></a><br />
<h2>Setup Validation</h2>
<p>The R executable in the bin directory should now use openblas. Note this is the R executable you now need to run in order to use the custom built R with openblas. Just typing R in terminal will load the old /usr/lib64&#8230; which we students didn&#8217;t have the permissions to alter. You can, however, create an alias in your .bashrc file by inserting the line &#8216;alias R=&#8221;/home/grad/msl33/software/R-3.1.1/bin/./R&#8221;&#8216;. Now when you type R in a terminal it will load the new R and not the old one.  One can check that R executable depends on the correct linked shared blas library with the &#8220;ldd&#8221; command.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage bin]$ pwd
/home/grad/msl33/software/R-3.1.1/bin
[msl33@cabbage bin]$ ./R CMD ldd exec/./R | grep blas
	libRblas.so =&gt; /home/grad/msl33/software/R-3.1.1/lib/libRblas.so (0x00007f62e3fb7000)
[msl33@cabbage bin]$ ls -lt ../lib | grep openblas
lrwxrwxrwx  1 msl33 grad      53 Jul 16 15:35 libRblas.so -&gt; /home/grad/msl33/software/openblas/lib/libopenblas.so

</pre>
<p>In addition, execute &#8220;./R&#8221; from the &#8220;bin&#8221; directory  (or just R if you set up the alias) and grab the process id. </p>
<pre class="brush: bash; title: ; notranslate">
[msl33@cabbage bin]$ ps aux | grep R | grep software | awk '{print $2}'
2412
[msl33@cabbage bin]$ cat /proc/`ps aux | grep R | grep software | awk '{print $2}'`/status | grep Threads
Threads:	8
[msl33@cabbage bin]$ 

</pre>
<p>Evidently the R session now has 8 threads available. Finally, lets perform an eigen-decomposition and look at the cpu usage using top. You&#8217;ll see it light up all of your cores.<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_cpu.png"><img src="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_cpu-1024x358.png" alt="openblas cpu usage" width="640" height="223" class="aligncenter size-large wp-image-1012" srcset="http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_cpu-1024x358.png 1024w, http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_cpu-300x105.png 300w, http://www.lindonslog.com/wp-content/uploads/2014/07/openblas_cpu.png 1321w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>
<p><a name="benchmark"></a><br />
<h2>Benchmark</h2>
<p>Using this <a href="http://r.research.att.com/benchmarks/R-benchmark-25.R">benchmark</a> the reference BLAS took 32.1 seconds whilst openBLAS took 7.1 seconds.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/linux-unix/compile-r-openblas-source-guide/">Compile R and OpenBLAS from Source Guide</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/linux-unix/compile-r-openblas-source-guide/feed/</wfw:commentRss>
		<slash:comments>3</slash:comments>
		</item>
		<item>
		<title>Monotonicity of EM Algorithm Proof</title>
		<link>http://www.lindonslog.com/mathematics/monotonicity-em-algorithm-proof/</link>
		<comments>http://www.lindonslog.com/mathematics/monotonicity-em-algorithm-proof/#respond</comments>
		<pubDate>Sat, 19 Apr 2014 20:25:41 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=973</guid>
		<description><![CDATA[<p>Here the monotonicity of the EM algorithm is established. $$ f_{o}(Y_{o}&#124;\theta)=f_{o,m}(Y_{o},Y_{m}&#124;\theta)/f_{m&#124;o}(Y_{m}&#124;Y_{o},\theta)$$ $$ \log L_{o}(\theta)=\log L_{o,m}(\theta)-\log f_{m&#124;o}(Y_{m}&#124;Y_{o},\theta) \label{eq:loglikelihood} $$ where \( L_{o}(\theta)\) is the likelihood under the observed data and \(L_{o,m}(\theta)\) is the likelihood under the complete data. Taking the expectation of the second line with respect to the conditional distribution of \(Y_{m}\) given \(Y_{o}\) and [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/monotonicity-em-algorithm-proof/">Monotonicity of EM Algorithm Proof</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p><br />
Here the monotonicity of the EM algorithm is established.<br />
$$	f_{o}(Y_{o}|\theta)=f_{o,m}(Y_{o},Y_{m}|\theta)/f_{m|o}(Y_{m}|Y_{o},\theta)$$<br />
$$	\log L_{o}(\theta)=\log L_{o,m}(\theta)-\log f_{m|o}(Y_{m}|Y_{o},\theta) \label{eq:loglikelihood} $$<br />
where \( L_{o}(\theta)\) is the likelihood under the observed data and \(L_{o,m}(\theta)\) is the likelihood under the complete data. Taking the expectation of the second line with respect to the conditional distribution of \(Y_{m}\) given \(Y_{o}\) and the current parameters \(\theta^{k}\) yields<br />
$$\log L_{o}(\theta)= \mathbb{E}_{Y_{m}}\left[\log L_{o,m}(\theta)|Y_{o},\theta^{k}\right]-\mathbb{E}_{Y_{m}}\left[\log f_{m|o}(Y_{m}|Y_{o},\theta)|Y_{o},\theta^{k} \right]$$<br />
which is used to construct the difference between the log-likelihood of a new value of \(\theta\) and the current value \(\theta^{k}\) as<br />
\begin{equation}<br />
	\begin{split}<br />
		\log L_{o}(\theta)-&#038;\log L_{o}(\theta^{k})=\mathbb{E}_{Y_{m}}\left[ \log L_{o,m}(\theta)|Y_{o},\theta^{k}\right]-\mathbb{E}_{Y_{m}}\left[ \log L_{o,m}(\theta^{k})|Y_{o},\theta^{k}\right] \\<br />
		+&#038;\mathbb{E}_{Y_{m}}\left[ \log f_{m|o}(Y_{m}|Y_{o},\theta^{k})|Y_{o},\theta^{k} \right]-\mathbb{E}_{Y_{m}}\left[ \log f_{m|o}(Y_{m}|Y_{o},\theta)|Y_{o},\theta^{k} \right],\\<br />
\end{split}<br />
\end{equation}<br />
or by adopting common notation as<br />
\begin{equation}<br />
		\log L_{o}(\theta)-\log L_{o}(\theta^{k})=Q(\theta;\theta^{k})-Q(\theta^{k};\theta^{k})+H(\theta^{k};\theta^{k})-H(\theta;\theta^{k}).\\<br />
\end{equation}<br />
Consider the last two &#8220;\( H\)&#8221; terms, then by Jensen&#8217;s inequality</p>
<p>\begin{align*}<br />
	       &#038;-\mathbb{E}_{Y_{m}}\left[ \log f_{m|o}(Y_{m}|Y_{o},\theta)- \log f_{m|o}(Y_{m}|Y_{o},\theta^{k})|Y_{o},\theta^{k} \right]\\<br />
	       &#038;=-\mathbb{E}_{Y_{m}}\left[\log \frac{ f_{m|o}(Y_{m}|Y_{o},\theta)}{ f_{m|o}(Y_{m}|Y_{o},\theta^{k})}|Y_{o},\theta^{k} \right]\\<br />
	       &#038;\geq-\log \mathbb{E}_{Y_{m}}\left[ \frac{ f_{m|o}(Y_{m}|Y_{o},\theta)}{ f_{m|o}(Y_{m}|Y_{o},\theta^{k})}|Y_{o},\theta^{k} \right]\\<br />
	       &#038;=-\log \int f_{m|o}(Y_{m}|Y_{o},\theta)dY_{m}\\<br />
	       &#038;=0 \; \; \; \; \; \; \;\forall \theta\in \Theta.<br />
\end{align*}<br />
It follows that \(\log L_{o}(\theta)-\log L_{o}(\theta^{k})\geq 0\) by choosing \(\theta\) such that \(Q(\theta;\theta^{k})-Q(\theta^{k};\theta^{k})\geq 0\).</p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft.jtitle=arXiv&#038;rft_id=info%3Aarxiv%2F1212.2490v1&#038;rfr_id=info%3Asid%2Fresearchblogging.org&#038;rft.atitle=On+the+Convergence+of+Bound+Optimization+Algorithms&#038;rft.issn=&#038;rft.date=2012&#038;rft.volume=&#038;rft.issue=&#038;rft.spage=&#038;rft.epage=&#038;rft.artnum=&#038;rft.au=Ruslan+R+Salakhutdinov&#038;rft.au=Sam+T+Roweis&#038;rft.au=Zoubin+Ghahramani&#038;rfe_dat=bpr3.included=1;bpr3.tags=Mathematics%2CApplied+Mathematics%2C+Probability+and+Statistics%2C+Parallel+and+Distributed+Computing%2C+Algorithms%2C+Operating+Systems">Ruslan R Salakhutdinov, Sam T Roweis, &#038; Zoubin Ghahramani (2012). On the Convergence of Bound Optimization Algorithms <span style="font-style: italic;">arXiv</span> arXiv: <a rev="review" href="http://arxiv.org/abs/1212.2490v1">1212.2490v1</a></span><br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176346060&#038;rft.atitle=On+the+Convergence+Properties+of+the+EM+Algorithm&#038;rft.jtitle=The+Annals+of+Statistics&#038;rft.artnum=http%3A%2F%2Fprojecteuclid.org%2Feuclid.aos%2F1176346060&#038;rft.volume=11&#038;rft.issue=1&#038;rft.issn=0090-5364&#038;rft.spage=95&#038;rft.epage=103&#038;rft.date=1983&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Wu+C.+F.+Jeff&#038;rft.aulast=Wu&#038;rft.aufirst=C.+F.+Jeff&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Mathematics">Wu C.F.J. (1983). On the Convergence Properties of the EM Algorithm, <span style="font-style:italic;">The Annals of Statistics, 11</span> (1) 95-103. DOI: <a rel="author" href="http://dx.doi.org/10.1214%2Faos%2F1176346060">10.1214/aos/1176346060</a></span><br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1002%2F0471721182&#038;rft.artnum=http%3A%2F%2Fdoi.wiley.com%2F10.1002%2F0471721182&#038;rfr_id=info%3Asid%2Fmathblogging.org&#038;rft.au=McLachlan+Geoffrey&#038;rft.aulast=McLachlan&#038;rft.aufirst=Geoffrey&#038;rft.au=Peel+David&#038;rft.aulast=Peel&#038;rft.aufirst=David&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Mathematics">McLachlan G. &#038; Peel D.   <span style="font-style:italic;"> </span>   DOI: <a rel="author" href="http://dx.doi.org/10.1002%2F0471721182">10.1002/0471721182</a></span></p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/monotonicity-em-algorithm-proof/">Monotonicity of EM Algorithm Proof</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/mathematics/monotonicity-em-algorithm-proof/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>C++11 versus R Standalone Random Number Generation Performance Comparison</title>
		<link>http://www.lindonslog.com/linux-unix/c11-r-standalone-random-number-generation-performance-comparison/</link>
		<comments>http://www.lindonslog.com/linux-unix/c11-r-standalone-random-number-generation-performance-comparison/#respond</comments>
		<pubDate>Sat, 01 Mar 2014 20:00:25 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Linux/Unix]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[C++11]]></category>
		<category><![CDATA[PRNG]]></category>
		<category><![CDATA[pseudo random number generation]]></category>
		<category><![CDATA[RNG]]></category>
		<category><![CDATA[standalone]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=960</guid>
		<description><![CDATA[<p>If you are writing some C++ code with the intent of calling it from R or even developing it into a package you might wonder whether it is better to use the pseudo random number library native to C++11 or the R standalone library. On the one hand users of your package might have an [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/linux-unix/c11-r-standalone-random-number-generation-performance-comparison/">C++11 versus R Standalone Random Number Generation Performance Comparison</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>If you are writing some C++ code with the intent of calling it from R or even developing it into a package you might wonder whether it is better to use the pseudo random number library native to C++11 or the R standalone library. On the one hand users of your package might have an outdated compiler which doesn&#8217;t support C++11 but on the other hand perhaps there are potential speedups to be won by using the <random> library native to C++11. I decided to compare the performance of these two libraries.</p>
<pre class="brush: cpp; title: ; notranslate">
#define MATHLIB_STANDALONE
#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;random&gt;
#include &lt;chrono&gt;
#include &quot;Rmath.h&quot;

int main(int argc, char *argv[])
{
        int ndraws=100000000;
        std::vector&lt;double&gt; Z(ndraws);
        std::mt19937 engine;
        std::normal_distribution&lt;double&gt; N(0,1);

        auto start = std::chrono::steady_clock::now();
        for(auto &amp; z : Z ) {
                z=N(engine);
        }
        auto end = std::chrono::steady_clock::now();
        std::chrono::duration&lt;double&gt; elapsed=end-start;

        std::cout &lt;&lt;  elapsed.count() &lt;&lt; &quot; seconds - C++11&quot; &lt;&lt; std::endl;

        start = std::chrono::steady_clock::now();
        GetRNGstate();
        for(auto &amp; z : Z ) {
                z=rnorm(0,1);
        }
        PutRNGstate();
        end = std::chrono::steady_clock::now();
        elapsed=end-start;

        std::cout &lt;&lt;  elapsed.count() &lt;&lt; &quot; seconds - R Standalone&quot; &lt;&lt; std::endl;

        return 0;
}
</pre>
<p>Compiling and run with:</p>
<pre class="brush: bash; title: ; notranslate">
[michael@michael coda]$ g++ normal.cpp -o normal -std=c++11 -O3 -lRmath
[michael@michael coda]$ ./normal 
</pre>
<h2>Normal Generation</h2>
<pre class="brush: bash; title: ; notranslate">
5.2252 seconds - C++11
6.0679 seconds - R Standalone
</pre>
<h2>Gamma Generation</h2>
<pre class="brush: bash; title: ; notranslate">
11.2132 seconds - C++11
12.4486 seconds - R Standalone
</pre>
<h2>Cauchy</h2>
<pre class="brush: bash; title: ; notranslate">
6.31157 seconds - C++11
6.35053 seconds - R Standalone
</pre>
<p>As expected the C++11 implementation is faster but not by a huge amount. As the computational cost of my code is dominated by other linear algebra procedures of O(n^3) I&#8217;d actually be willing to use the R standalone library because the syntax is more user friendly.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/linux-unix/c11-r-standalone-random-number-generation-performance-comparison/">C++11 versus R Standalone Random Number Generation Performance Comparison</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/linux-unix/c11-r-standalone-random-number-generation-performance-comparison/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Stochastic Optimization in R by Parallel Tempering</title>
		<link>http://www.lindonslog.com/programming/stochastic-optimization-r-rmpi-parallel-tempering/</link>
		<comments>http://www.lindonslog.com/programming/stochastic-optimization-r-rmpi-parallel-tempering/#comments</comments>
		<pubDate>Sat, 12 Oct 2013 21:24:04 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[optimization]]></category>
		<category><![CDATA[parallel]]></category>
		<category><![CDATA[stochastic]]></category>
		<category><![CDATA[tempering]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=910</guid>
		<description><![CDATA[<p>I&#8217;ve written a few posts now about using parallel tempering to sample from complicated multi-modal target distributions but there are also other benefits and uses to this algorithm. There is a nice post on Darren Wilkinson&#8217;s blog about using tempered posteriors for marginal likelihood calculations. There is also another area where parallel tempering finds application, [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/stochastic-optimization-r-rmpi-parallel-tempering/">Stochastic Optimization in R by Parallel Tempering</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p><br />
I&#8217;ve written a few posts now about using <a href="http://www.lindonslog.com/programming/openmp/parallel-tempering-algorithm-c/" title="Parallel Tempering Algorithm with OpenMP / C++">parallel tempering</a> to sample from complicated multi-modal target distributions but there are also other benefits and uses to this algorithm. There is a nice post on Darren Wilkinson&#8217;s blog about using <a href="http://darrenjw.wordpress.com/2013/10/01/marginal-likelihood-from-tempered-bayesian-posteriors/">tempered posteriors for marginal likelihood</a> calculations. There is also another area where parallel tempering finds application, namely in stochastic optimization. I first encountered parallel tempering whilst doing my MPhys degree at the University of Warwick but at that time it was employed as a stochastic optimization algorithm to find the minimum energy configuration of a Lennard-Jones cluster as opposed to a sampling algorithm. All that is required is one observation to turn this sampling algorithm into a stochastic optimization algorithm. Lets break this observation down into a few steps.<br />
Consider sampling from a simple exponential distribution $$f(E)\propto e^{-\beta E}1_{(0,\infty )}(E),$$<br />
with rate parameter beta. For now lets fix beta=5. One could sample from this distribution using the same <a href="http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/" title="Parallel Tempering Code in R Rmpi">Rmpi parallel tempering code</a> given in my previous post by simply changing the target distribution to the exponential above. The histograms of mcmc draws from four tempered distribution would then look something like this:<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/exphist.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/exphist-300x300.jpeg" alt="Histogram of mcmc draws" width="300" height="300" class="aligncenter size-medium wp-image-920" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/exphist-300x300.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2013/10/exphist-150x150.jpeg 150w, http://www.lindonslog.com/wp-content/uploads/2013/10/exphist.jpeg 1000w" sizes="(max-width: 300px) 100vw, 300px" /></a><br />
Note the scale on the x-axis. The two important observations mentioned earlier are
<li> The minimum value of E occurs most frequently as it the mode of the target distribution </li>
<li> The greater the rate parameter, the more concentrated the distribution is around E-min</li>
<p> The second point is important because although the sampling algorithm is creating draws that are not the minimum value of E, by increasing the rate parameter one can force these draws to be arbitrarily close to E-min.</p>
<h2>A Uni-modal Optimization Function</h2>
<p>How does this relate to optimization? Consider setting $$E(\theta)=(\theta-40)^2$$ Whereas before where using the Metropolis algorithm one would propose a new value of E, say E&#8217;, now the proposal is made in θ, and θ&#8217; is accepted based on u < f(E(θ')) / f(E(θ)). By construction the algorithm gives draws close to E-min, which occurs when θ=40. The traceplot of θ is shown below:
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/unioptim.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/unioptim-300x300.jpeg" alt="Stochastic Optimization mcmc traceplot" width="350" height="350" class="aligncenter size-medium wp-image-929" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/unioptim-300x300.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2013/10/unioptim-150x150.jpeg 150w, http://www.lindonslog.com/wp-content/uploads/2013/10/unioptim.jpeg 600w" sizes="(max-width: 350px) 100vw, 350px" /></a><br />
Click here for the <a href="http://www.lindonslog.com/example_code/unioptim.R">code</a>.</p>
<h2>A Harder Optimization Function</h2>
<p>The above quadratic was an easy uni-modal example. Let&#8217;s try a harder function. Consider the minimum of $$ E(\theta)=3sin(\theta)+(0.1\theta-3)^2,$$ which looks like this:<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/optimtest.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/optimtest-300x185.jpeg" alt="Optimization Function" width="300" height="185" class="aligncenter size-medium wp-image-932" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/optimtest-300x185.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2013/10/optimtest.jpeg 894w" sizes="(max-width: 300px) 100vw, 300px" /></a><br />
This function has infinitely many local minima but one global minimum around 30. Local minima make optimization challenging and many optimization algorithms get stuck in these regions as locally it appears the minimum has been reached. This is where the parallel tempering really helps. The traceplots of theta are shown for six tempered distributions below:<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/hardoptim.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/hardoptim-682x1024.jpeg" alt="optimization traceplots mcmc" width="640" height="960" class="aligncenter size-large wp-image-938" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/hardoptim-682x1024.jpeg 682w, http://www.lindonslog.com/wp-content/uploads/2013/10/hardoptim-200x300.jpeg 200w, http://www.lindonslog.com/wp-content/uploads/2013/10/hardoptim.jpeg 1000w" sizes="(max-width: 640px) 100vw, 640px" /></a><br />
Click here for the <a href="http://www.lindonslog.com/example_code/multioptim.R">code</a>.</p>
<p>I&#8217;m currently working on another example just for fun, namely finding the lowest energy configuration of an n-particle Lennard-Jones cluster. This is a nice example because one can visualize the process using <a href="http://www.ks.uiuc.edu/Research/vmd/" rel="external nofollow">vmd</a> and it also provides some insight into the origins of such terminology as &#8220;tempering&#8221;, &#8220;annealing&#8221; and &#8220;temperature&#8221; which always look somewhat out of place in the statistics literature. </p>
<h2>An Even Harder Function</h2>
<p>Consider the function<br />
$$ E(\theta)=10\sin(0.3\theta)\sin(1.3\theta^2) + 0.00001\theta^4 + 0.2\theta+80, $$<br />
which is shown below.<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/wildfunction.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/wildfunction.jpeg" alt="Optimization Function" width="600" height="500" class="aligncenter size-full wp-image-945" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/wildfunction.jpeg 600w, http://www.lindonslog.com/wp-content/uploads/2013/10/wildfunction-300x250.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<p>The trace-plots for the parallel tempering optimization are shown below<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/goodwild.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/goodwild-682x1024.jpeg" alt="Parallel Tempering Optimization" width="640" height="960" class="aligncenter size-large wp-image-946" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/goodwild-682x1024.jpeg 682w, http://www.lindonslog.com/wp-content/uploads/2013/10/goodwild-200x300.jpeg 200w, http://www.lindonslog.com/wp-content/uploads/2013/10/goodwild.jpeg 1000w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>
<p>Examining the mcmc draws the minimum is obtained at theta=-15.81515.</p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1016%2Fj.amc.2009.02.023&#038;rft.atitle=Hybrid+parallel+tempering+and+simulated+annealing+method&#038;rft.jtitle=Applied+Mathematics+and+Computation&#038;rft.artnum=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0096300309001362&#038;rft.volume=212&#038;rft.issue=1&#038;rft.issn=00963003&#038;rft.spage=216&#038;rft.epage=228&#038;rft.date=2009&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Li+Yaohang&#038;rft.aulast=Li&#038;rft.aufirst=Yaohang&#038;rft.au=Protopopescu+Vladimir+A.&#038;rft.aulast=Protopopescu&#038;rft.aufirst=Vladimir+A.&#038;rft.au=Arnold+Nikita&#038;rft.aulast=Arnold&#038;rft.aufirst=Nikita&#038;rft.au=Zhang+Xinyu&#038;rft.aulast=Zhang&#038;rft.aufirst=Xinyu&#038;rft.au=Gorin+Andrey&#038;rft.aulast=Gorin&#038;rft.aufirst=Andrey&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CMathematics%2CPhysics">Li Y., Protopopescu V.A., Arnold N., Zhang X. &#038; Gorin A. (2009). Hybrid parallel tempering and simulated annealing method, <span style="font-style:italic;">Applied Mathematics and Computation, 212</span> (1) 216-228. DOI: <a rel="author" href="http://dx.doi.org/10.1016%2Fj.amc.2009.02.023">10.1016/j.amc.2009.02.023</a></span></p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/stochastic-optimization-r-rmpi-parallel-tempering/">Stochastic Optimization in R by Parallel Tempering</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/programming/stochastic-optimization-r-rmpi-parallel-tempering/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Parallel Tempering in R with Rmpi</title>
		<link>http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/</link>
		<comments>http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/#comments</comments>
		<pubDate>Mon, 07 Oct 2013 05:00:54 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Mathematics]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[mcmc]]></category>
		<category><![CDATA[metropolis coupled]]></category>
		<category><![CDATA[mpi]]></category>
		<category><![CDATA[parallel]]></category>
		<category><![CDATA[parallel tempering]]></category>
		<category><![CDATA[rmpi]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=883</guid>
		<description><![CDATA[<p>My office computer recently got a really nice upgrade and now I have 8 cores on my desktop to play with. I also at the same time received some code for a Gibbs sampler written in R from my adviser. I wanted to try a metropolis-coupled markov chain monte carlo, , algorithm on it to [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/">Parallel Tempering in R with Rmpi</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>My office computer recently got a really nice upgrade and now I have 8 cores on my desktop to play with. I also at the same time received some code for a Gibbs sampler written in R from my adviser. I wanted to try a metropolis-coupled markov chain monte carlo, <img src="//s0.wp.com/latex.php?latex=MC%5E%7B3%7D&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="MC^{3}" title="MC^{3}" class="latex" />, algorithm on it to try and improve the mixing but the problem was that it was written in R and I&#8217;m used to writing parallel code in C/C++ with OpenMP or MPI. Previously I wrote about a parallel tempering algorithm with an implementation in C++ using OpenMP and so I thought I would try and code up the same sort of thing in R as a warm-up exercise before I started with the full <img src="//s0.wp.com/latex.php?latex=MC%5E%7B3%7D&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="MC^{3}" title="MC^{3}" class="latex" /> algorithm. Sadly I don&#8217;t think there is any facility in R for OpenMP style parallelism. There are packages such as <em>snow</em> and <em>multicore</em> but these are very high level packages and don&#8217;t really allow one to control the finer details. There is, however, Rmpi. It is a little bit different from regular C/Fortran MPI implementations and I once had a very bad experience getting some Rmpi code to work for a project deadline, it wasn&#8217;t pretty, so I was a little reluctant to reconsider this package but if you look at the changelogs it is still being actively maintained and in the end I&#8217;m very happy with the outcome of this experiment. I tried to write the below code as generally as possible, so that it is easily adapted by myself, or others, in the future.</p>
<h2>Target Density</h2>
<p>First one needs to write a density one wishes to sample from</p>
<pre class="brush: r; title: ; notranslate">
logdensity&lt;-function(theta){
  #Distribution one wishes to sample from here.
  #It may be more convinient to pass a theta as a list
  sigma2=0.001;
  Sigma=matrix(0,2,2);
  Sigma[1,1]=sigma2;
  Sigma[2,2]=sigma2;
  density=dmvnorm(theta,c(0,0),Sigma)+dmvnorm(theta,c(-2,0.8),Sigma)+dmvnorm(theta,c(-1,1),Sigma)+dmvnorm(theta,c(1,1),Sigma)+dmvnorm(theta,c(0.5,0.5),Sigma);
  return(log(density))
}
</pre>
<p>The density I chose was a mixture of 5 well-separated bi-variate Normals. One should note that it is probably cleanest to pass all the arguments to this function as a list theta. It wasn&#8217;t really necessary in this case but if you have a posterior distribution with a number of parameters of varying dimension then it would be much nicer as a list. In a future blog post I may change the target density to be the energy distribution of a Lennard-Jones cluster. </p>
<h2>Parallel Tempering Algorithm</h2>
<p>This too is written as a function because Rmpi allows you to pass the function to all slaves and execute it. It was basically the easiest way of writing it for Rmpi.</p>
<pre class="brush: r; title: ; notranslate">
temper&lt;-function(niter,Bmin,swap.interval){
  rank=mpi.comm.rank();
  size=mpi.comm.size();
  swap=0;
  swaps.attempted=0;
  swaps.accepted=0;
  
  #Higher ranks run the higher &quot;temperatures&quot; (~smaller fractional powers)
  B=rep(0,size-1);
  for(r in 1:size-1){
    temp=(r-1)/(size-2);
    B[r]=Bmin^temp;
  }
  
  
  #Create a list for proposal moves
  prop=rep(0,2);
  theta=matrix(0,niter,2)
  
  for(t in 2:niter){
    
    for(c in 1:length(prop))   prop=theta[t-1,c]+rnorm(1,0,0.1);
    
    #Calculate Log-Density at proposed and current position
    logdensity.current=logdensity(theta[t-1,])
    logdensity.prop=logdensity(prop);
    
    #Calculate log acceptance probability
    lalpha=B[rank]*(logdensity.prop-logdensity.current)
    
    if(log(runif(1))&lt;lalpha){
      #Accept proposed move
      theta[t,]=prop;
      logdensity.current=logdensity.prop;
    }else{
      #Otherwise do not move
      theta[t,]=theta[t-1,];
    } 
    
    if(t%%swap.interval ==0){
      for(evenodd in 0:1){
        swap=0;
        logdensity.partner=0;
        if(rank%%2 == evenodd%%2){
          rank.partner=rank + 1;
          #ranks range from 1:size-1. Cannot have a partner rank == size
          if(0&lt;rank.partner &amp;&amp; rank.partner&lt;size){
            #On first iteration, evens receive from above odd
            #On second iteration, odds receive from above evens
            logdensity.partner&lt;-mpi.recv.Robj(rank.partner,rank.partner);
            lalpha = (B[rank]-B[rank.partner])*(logdensity.partner-logdensity.current);
            swaps.attempted=swaps.attempted+1;
            if(log(runif(1))&lt;lalpha){
              swap=1;
              swaps.accepted=swaps.accepted+1;
            }
            mpi.send.Robj(swap,dest=rank.partner,tag=rank)
          }
          if(swap==1){
            thetaswap=theta[t,];
            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank)
            theta[t,]=mpi.recv.Robj(rank.partner,rank.partner)
          }
        }else{
          rank.partner=rank-1;
          #ranks range from 1:size-1. Cannot have a partner rank ==0
          if(0&lt;rank.partner &amp;&amp; rank.partner&lt;size){
            #On first iteration, odds send to evens below
            #On second iteration, evens sent to odds below
            mpi.send.Robj(logdensity.current,dest=rank.partner,tag=rank);
            swap=mpi.recv.Robj(rank.partner,rank.partner);
          }
          if(swap==1){
            thetaswap=theta[t,];
            theta[t,]=mpi.recv.Robj(rank.partner,rank.partner);
            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank);
          }
        }
      }
    }
  }
  return(theta)
}
</pre>
<p>The bulk of the above code is the communication of each processor with its next nearest neighbors. Metropolis moves will be attempted every <em>swap.interval</em> iterations, an argument one can pass to the function. When this code block is entered, even rank processors will partner with their higher ranked odd neighbours (they have a high rank so higher temperature i.e. smaller fractional power &#8211; a more &#8220;melted down&#8221; target density). The higher odd partners will send their lower even partners the value of their density and then the lower even partners will calculate an acceptance probabilty. If the move succeeds the lower rank even processors send their higher rank odd processors a binary swap=1 telling the higher rank odd processors that a send/receive procedure will occur. The lower even rank sends the higher odd rank its parameters and then subsequently the higher odd rank sends its lower even rank its parameters. In this way a metropolis move between processors is achieved. Next, odd rank processors form partners with their higher even ranked neighbours (because we need to swap with processor rank 1, the target density). The same procedure occurs as before but swapping odd for even. More visually, first swaps are attempted between 2-3, 4-5, 6-7 etc and then swaps are attempted between 1-2, 3-4, 5-6. This is almost like a merge-sort style algorithm. One can see how the parameters could be passed from 3 down to 2 and then from 2 down to 1. The main point is that each processor attempts a swap with its nearest-neighbours, the one above and the one below, every <em>swap.interval</em> iterations.<br />
With these functions defined one can now proceed to set up the mpi communicator/world.</p>
<h2>Rmpi</h2>
<p>First spawn some slaves.</p>
<pre class="brush: r; title: ; notranslate">
library(Rmpi)
mpi.spawn.Rslaves(nslaves=6)
</pre>
<p>If it worked, you should see something like this:</p>
<pre class="brush: r; title: ; notranslate">
&gt; mpi.spawn.Rslaves(nslaves=6)
	6 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 7 is running on: cabbage 
slave1 (rank 1, comm 1) of size 7 is running on: cabbage 
slave2 (rank 2, comm 1) of size 7 is running on: cabbage 
slave3 (rank 3, comm 1) of size 7 is running on: cabbage 
slave4 (rank 4, comm 1) of size 7 is running on: cabbage 
slave5 (rank 5, comm 1) of size 7 is running on: cabbage 
slave6 (rank 6, comm 1) of size 7 is running on: cabbage 
</pre>
<p>(yes, my office computer was named cabbage, lettuce is the one next to me). One can then send the function definitions to the slave processors.</p>
<pre class="brush: r; title: ; notranslate">
niter=3000
Bmin=0.005
swap.interval=3
#Send to slaves some required data
mpi.bcast.Robj2slave(niter)
mpi.bcast.Robj2slave(Bmin)
mpi.bcast.Robj2slave(swap.interval)
#Send to slaves the logdensity function
mpi.bcast.Robj2slave(logdensity)
#Send to slaves the temper function
mpi.bcast.Robj2slave(temper) 
#Send to slaves the dmvnorm function
mpi.bcast.Robj2slave(dmvnorm) 
</pre>
<p>If you want to make sure that the slaves have the correct function definition, one can execute the command <em>mpi.remote.exec(temper)</em> and this will return the function definition. That is all, now it can be run.</p>
<pre class="brush: r; title: ; notranslate">
mcmc=mpi.remote.exec(temper(niter,Bmin,swap.interval))
</pre>
<p>This returns a list object containing the mcmc draws for each slave.</p>
<h3>Results</h3>
<p>The end product is something that looks like this<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing-300x300.jpeg" alt="parallel tempering mixing" width="300" height="300" class="aligncenter size-medium wp-image-889" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing-300x300.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing-150x150.jpeg 150w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing.jpeg 1000w" sizes="(max-width: 300px) 100vw, 300px" /></a><br />
Which are the draws (in black) from the target distribution. It is also useful to build up intuition for parallel tempering to look at what is happening on the other processors. The draws for all processors are shown below:<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering.jpeg"><img src="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering-682x1024.jpeg" alt="parallel tempering draws for each processor" width="640" height="960" class="aligncenter size-large wp-image-891" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering-682x1024.jpeg 682w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering-200x300.jpeg 200w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering.jpeg 1000w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>
<p>N.B. Although my computer only has 8 cores I tried running the code 12 slaves. At first I was concerned that the MPI communications would enter a deadlock and the code would hang but it didn&#8217;t, so it seems you can scale up the number of slaves above the number of cores.</p>
<h2>Temperature Set</h2>
<p>Notice that the temperature set used in the code has the property that <img src="//s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cbeta_%7Bn%7D%7D%7B%5Cbeta_%7Bn%2B1%7D%7D%3Dc&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="&#92;frac{&#92;beta_{n}}{&#92;beta_{n+1}}=c" title="&#92;frac{&#92;beta_{n}}{&#92;beta_{n+1}}=c" class="latex" />, for c a constant. There is a paper by Kofke(2002) that justifies this temperature set as it yields a constant acceptance ratio between cores under certain conditions. Indeed, the acceptance ratio (the fraction of metropolis moves that succeeded between cores) are roughly constant, as shown below:</p>
<pre class="brush: r; title: ; notranslate">
[1] 0.7227723
[1] 0.7926793
[1] 0.710171
[1] 0.8037804
[1] 0.7191719
[1] 0.7974797
[1] 0.729673
[1] 0.8223822
[1] 0.8184818
[1] 0.8445845
</pre>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1039%2Fb509983h&#038;rft.atitle=Parallel+tempering%3A+Theory%2C+applications%2C+and+new+perspectives&#038;rft.jtitle=Physical+Chemistry+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Fxlink.rsc.org%2F%3FDOI%3Db509983h&#038;rft.volume=7&#038;rft.issue=23&#038;rft.issn=1463-9076&#038;rft.spage=3910&#038;rft.date=2005&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Earl+David+J.&#038;rft.aulast=Earl&#038;rft.aufirst=David+J.&#038;rft.au=Deem+Michael+W.&#038;rft.aulast=Deem&#038;rft.aufirst=Michael+W.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CMathematics%2CPhysics">Earl D.J. &#038; Deem M.W. (2005). Parallel tempering: Theory, applications, and new perspectives, <span style="font-style:italic;">Physical Chemistry Chemical Physics, 7</span> (23) 3910. DOI: <a rel="author" href="http://dx.doi.org/10.1039%2Fb509983h">10.1039/b509983h</a></span><br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1063%2F1.1507776&#038;rft.atitle=On+the+acceptance+probability+of+replica-exchange+Monte+Carlo+trials&#038;rft.jtitle=The+Journal+of+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Flink.aip.org%2Flink%2FJCPSA6%2Fv117%2Fi15%2Fp6911%2Fs1%26Agg%3Ddoi&#038;rft.volume=117&#038;rft.issue=15&#038;rft.issn=00219606&#038;rft.spage=6911&#038;rft.date=2002&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Kofke+David+A.&#038;rft.aulast=Kofke&#038;rft.aufirst=David+A.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CPhysics">Kofke D.A. (2002). On the acceptance probability of replica-exchange Monte Carlo trials, <span style="font-style:italic;">The Journal of Chemical Physics, 117</span> (15) 6911. DOI: <a rel="author" href="http://dx.doi.org/10.1063%2F1.1507776">10.1063/1.1507776</a></span></p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/">Parallel Tempering in R with Rmpi</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/mathematics/parallel-tempering-r-rmpi/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
		<item>
		<title>Easy 3-Minute Guide to Making apply() Parallel over Distributed Grids and Clusters in R</title>
		<link>http://www.lindonslog.com/programming/apply-parallel-distributed-grid-cluster-r/</link>
		<comments>http://www.lindonslog.com/programming/apply-parallel-distributed-grid-cluster-r/#respond</comments>
		<pubDate>Mon, 02 Sep 2013 01:00:13 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
				<category><![CDATA[Programming]]></category>
		<category><![CDATA[R]]></category>
		<category><![CDATA[Statistics]]></category>

		<guid isPermaLink="false">http://www.lindonslog.com/?p=859</guid>
		<description><![CDATA[<p>Last week I attended a workshop on how to run highly parallel distributed jobs on the Open Science Grid (osg). There I met Derek Weitzel who has made an excellent contribution to advancing R as a high performance computing language by developing BoscoR. BoscoR greatly facilitates the use of the already existing package &#8220;GridR&#8221; by [&#8230;]</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/apply-parallel-distributed-grid-cluster-r/">Easy 3-Minute Guide to Making apply() Parallel over Distributed Grids and Clusters in R</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Last week I attended a workshop on how to run highly parallel distributed jobs on the Open Science Grid (osg). There I met <a href="http://derekweitzel.blogspot.com/" title="BoscoR">Derek Weitzel</a> who has made an excellent contribution to advancing R as a high performance computing language by developing BoscoR. BoscoR greatly facilitates the use of the already existing package &#8220;GridR&#8221; by allowing the R user to use <a href="http://bosco.opensciencegrid.org/about/">Bosco</a> to manage the submission of jobs. It seems no matter how many kinds of queue-submission system I become familiar with (torque,sge,condor), the current cluster I&#8217;m working on uses something foreign and so I have to relearn how to write a job submission file. One of the two major selling points of Bosco is that it allows the user to write one job submission file locally (based on HTCondor) and use it to submit jobs on various remote clusters all using different interfaces. The second major selling point is that Bosco will manage work sharing if you have access to more than one cluster, that is it will submit jobs to each cluster proportional to how unburdened that cluster is, which is great if you have access to 3 clusters. It means the users apply jobs will get through the queue as quickly as possible by cleverly distributing the work over all available clusters. Hopefully that will have convinced you that Bosco is worth having, now lets proceed with how to use it. I will illustrate the process by using Duke University&#8217;s cluster, the DSCR. There are three steps: 1) Installing Bosco 2) Installing GridR 3) Running a test job.<br />
<br \></p>
<h2>Installing Bosco</h2>
<p>First go ahead and download <a href="http://bosco.opensciencegrid.org/download-form/?package=1.2/bosco_quickstart.tar.gz" rel="external nofollow" title="boscor download">Bosco</a>, the sign-up is only for the developers to get an idea of how many people are using it. Detailed install instructions can be found <a href="https://confluence.grid.iu.edu/pages/viewpage.action?pageId=10944561">here</a> but I will also go through the steps.</p>
<pre class="brush: bash; title: ; notranslate">
[lindon@laptop Downloads]$ tar xvzf ./bosco_quickstart.tar.gz
[lindon@laptop Downloads]$ ./bosco_quickstart
</pre>
<p>The executable will then ask some questions:</p>
<p>Do you want to install Bosco? Select y/n and press [ENTER]: y<br />
Type the cluster name and press [ENTER]: dscr-login-01.oit.duke.edu<br />
When prompted &#8220;Type your name at dscr-login-01.oit.duke.edu (default YOUR_USER) and press [ENTER]: NetID<br />
When prompted &#8220;Type the queue manager for login01.osgconnect.net (pbs, condor, lsf, sge, slurm) and press [ENTER]: sge<br />
Then when prompted &#8220;NetID@dscr-login-01.oit.duke.edu&#8217;s password: XXXXXXX</p>
<p>For duke users, the HostName of the DCSR is dscr-login-01.oit.duke.edu. You login with your NetID and the queue submission system is the Sun Grid Engine, so type sge. If you already have <a href="http://www.lindonslog.com/linux-unix/ssh-keygen-keys/" title="Creating SSH Keys with ssh-keygen and ssh-copy-id" target="_blank">SSH-Keys</a> set up then I think the last question gets skipped. That takes care of the installation. You can now try submitting on the remote cluster locally from your laptop. Download this test <a href="http://www.lindonslog.com/example_code/short.sh">executable</a> and this <a href="http://www.lindonslog.com/example_code/bosco01.sub">submission file</a>. Start Bosco and try submitting a job.</p>
<pre class="brush: bash; title: ; notranslate">
[msl33@hotel ~/tutorial-bosco]$ source ~/bosco/bosco_setenv
[msl33@hotel ~/tutorial-bosco]$ bosco_start
BOSCO Started
[msl33@hotel ~/tutorial-bosco]$ condor_submit bosco01.sub 
Submitting job(s).
1 job(s) submitted to cluster 70.
[msl33@hotel ~/tutorial-bosco]$ condor_q


-- Submitter: hotel.stat.duke.edu : &lt;127.0.0.1:11000?sock=21707_cbb6_3&gt; : hotel.stat.duke.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  70.0   msl33           8/31 12:08   0+00:00:00 I  0   0.0  short.sh          

1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended
</pre>
<p>This is the result if all has worked well. Note that you need to start Bosco by the above two lines.</p>
<p><br \></p>
<h2>Installing GridR</h2>
<p>The current version of GridR on CRAN is an older version doesn&#8217;t support job submission by bosco. It will when CRAN gets the latest version of GridR but until then you need to install GridR from source so download it <a href="https://github.com/osg-bosco/GridR/releases/download/v0.9.7/GridR_0.9.7.tar.gz" rel="external nofollow">here</a> and install it:</p>
<pre class="brush: r; title: ; notranslate">
install.packages(&quot;~/Downloads/GridR_0.9.7.tar.gz&quot;, repos=NULL, type=&quot;source&quot;)
</pre>
<p><br \></p>
<h2>Running a Parallel Apply on the Cluster</h2>
<p>Consider a toy example which approximates pi by monte-carlo.</p>
<pre class="brush: r; title: ; notranslate">
montecarloPi &lt;- function(trials, inst) {
  count = 0
  for(i in 1:trials) {
    if((runif(1,0,1)^2 + runif(1,0,1)^2)&lt;1) {
      count = count + 1
    }
  }
  return((count*4)/trials)
}
</pre>
<p>One can now use grid.apply from the GridR package combined with Bosco to submit jobs on the remote cluster from within the users local R session.</p>
<pre class="brush: plain; title: ; notranslate">
# load the GridR library
library(&quot;GridR&quot;)
grid.init(service=&quot;bosco.direct&quot;, localTmpDir=&quot;tmp&quot;)
# Send 10 instances of the montecarloPi
grid.apply(&quot;pi_estimate&quot;, montecarloPi, 10000000, c(1:10), batch=c(2))
</pre>
<p>You can then see how your jobs are getting on by the &#8220;grid.printJobs()&#8221; command.<br />
<a href="http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply.png"><img src="http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply-1024x358.png" alt="parallel grid apply" width="640" height="223" class="aligncenter size-large wp-image-876" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply-1024x358.png 1024w, http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply-300x105.png 300w, http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply.png 1325w" sizes="(max-width: 640px) 100vw, 640px" /></a><br />
When it completes, &#8220;pi_estimate&#8221; will be a list object with 10 elements containing approximations to pi. Obviously, there is an overhead with submitting jobs and also a lag time while these jobs get through the queue. One must balance this overhead with the computational time required to complete a single iteration of the apply function. Bosco will create and submit a job for every iteration of the apply function. If each iteration does not take too long but there exists a great many of them to perform, one could consider blocking these operations into, say, 10 jobs so that the queue lag and submission overhead is negligible in comparison to the time taken to complete no_apply_iteraions/10 computations, which also saves creating a large number of jobs on the cluster which might aggravate other users. One can also add clusters to bosco using the &#8220;bosco_cluster &#8211;add&#8221; command, so that jobs are submitted to whichever cluster has the most free cores available. All in all this is a great aid for those doing computationally intensive tasks and makes parallel work-sharing very easy indeed.</p>
<p>The post <a rel="nofollow" href="http://www.lindonslog.com/programming/apply-parallel-distributed-grid-cluster-r/">Easy 3-Minute Guide to Making apply() Parallel over Distributed Grids and Clusters in R</a> appeared first on <a rel="nofollow" href="http://www.lindonslog.com">Lindons Log</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://www.lindonslog.com/programming/apply-parallel-distributed-grid-cluster-r/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
