<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns#">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />

<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="http://www.lindonslog.com/xmlrpc.php" />
<link href='http://fonts.googleapis.com/css?family=Economica:400,400italic,700,700italic' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans:400,700' rel='stylesheet' type='text/css'>
<!--[if lt IE 9]>
<script src="http://www.lindonslog.com/wp-content/themes/Winter/js/html5.js" type="text/javascript"></script>
<![endif]-->


<!-- This site is optimized with the Yoast SEO plugin v3.1.2 - https://yoast.com/wordpress/plugins/seo/ -->
<title>Programming Archives - Page 2 of 3 - Lindons Log</title>
<link rel="canonical" href="index.html" />
<link rel="prev" href="../../index.html" />
<link rel="next" href="../3/index.html" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="object" />
<meta property="og:title" content="Programming Archives - Page 2 of 3 - Lindons Log" />
<meta property="og:url" content="http://www.lindonslog.com/category/programming/page/2/" />
<meta property="og:site_name" content="Lindons Log" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Programming Archives - Page 2 of 3 - Lindons Log" />
<meta name="twitter:site" content="@lindonslog" />
<!-- / Yoast SEO plugin. -->

<link rel='dns-prefetch' href='http://s0.wp.com/' />
<link rel='dns-prefetch' href='http://s.gravatar.com/' />
<link rel='dns-prefetch' href='http://s.w.org/' />
<link rel="alternate" type="application/rss+xml" title="Lindons Log &raquo; Feed" href="../../../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="Lindons Log &raquo; Comments Feed" href="../../../../comments/feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="Lindons Log &raquo; Programming Category Feed" href="../../feed/index.html" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.lindonslog.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.6.1"}};
			!function(a,b,c){function d(a){var c,d,e,f,g,h=b.createElement("canvas"),i=h.getContext&&h.getContext("2d"),j=String.fromCharCode;if(!i||!i.fillText)return!1;switch(i.textBaseline="top",i.font="600 32px Arial",a){case"flag":return i.fillText(j(55356,56806,55356,56826),0,0),!(h.toDataURL().length<3e3)&&(i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,65039,8205,55356,57096),0,0),c=h.toDataURL(),i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,55356,57096),0,0),d=h.toDataURL(),c!==d);case"diversity":return i.fillText(j(55356,57221),0,0),e=i.getImageData(16,16,1,1).data,f=e[0]+","+e[1]+","+e[2]+","+e[3],i.fillText(j(55356,57221,55356,57343),0,0),e=i.getImageData(16,16,1,1).data,g=e[0]+","+e[1]+","+e[2]+","+e[3],f!==g;case"simple":return i.fillText(j(55357,56835),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode8":return i.fillText(j(55356,57135),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode9":return i.fillText(j(55358,56631),0,0),0!==i.getImageData(16,16,1,1).data[0]}return!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i;for(i=Array("simple","flag","unicode8","diversity","unicode9"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='papercite_css-css'  href='../../../../wp-content/plugins/papercite/papercite.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='style-css'  href='../../../../wp-content/themes/Winter/style.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='grid-css'  href='../../../../wp-content/themes/Winter/css/grid.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='wp-style-css'  href='../../../../wp-content/themes/Winter/css/wp-style.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fontwaesome-css'  href='../../../../wp-content/themes/Winter/css/font-awesome.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fontwaesome-ie-css'  href='../../../../wp-content/themes/Winter/css/font-awesome-ie7.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='flexslider-css'  href='../../../../wp-content/themes/Winter/css/flexslider.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='fancybox-css'  href='../../../../wp-content/themes/Winter/css/jquery.fancybox.css%3Fver=4.6.1.css' type='text/css' media='all' />
<link rel='stylesheet' id='jetpack_css-css'  href='../../../../wp-content/plugins/jetpack/css/jetpack.css%3Fver=3.9.7.css' type='text/css' media='all' />
<script type='text/javascript' src='../../../../wp-includes/js/jquery/jquery.js%3Fver=1.12.4'></script>
<script type='text/javascript' src='../../../../wp-includes/js/jquery/jquery-migrate.min.js%3Fver=1.4.1'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/papercite/js/papercite.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jquery.flexslider-min.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jquery.fancybox.pack.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/jwplayer.js%3Fver=1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/custom.js%3Fver=1'></script>
<link rel='https://api.w.org/' href='../../../../wp-json/index.html' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 4.6.1" />

<link rel='dns-prefetch' href='http://jetpack.wordpress.com/'>
<link rel='dns-prefetch' href='http://s0.wp.com/'>
<link rel='dns-prefetch' href='http://s1.wp.com/'>
<link rel='dns-prefetch' href='http://s2.wp.com/'>
<link rel='dns-prefetch' href='http://public-api.wordpress.com/'>
<link rel='dns-prefetch' href='http://0.gravatar.com/'>
<link rel='dns-prefetch' href='http://1.gravatar.com/'>
<link rel='dns-prefetch' href='http://2.gravatar.com/'>
<link rel='dns-prefetch' href='http://widgets.wp.com/'>
<style type="text/css" id="syntaxhighlighteranchor"></style>
<link rel="icon" href="../../../../wp-content/uploads/2015/09/cropped-L-32x32.png" sizes="32x32" />
<link rel="icon" href="../../../../wp-content/uploads/2015/09/cropped-L-192x192.png" sizes="192x192" />
<link rel="apple-touch-icon-precomposed" href="../../../../wp-content/uploads/2015/09/cropped-L-180x180.png" />
<meta name="msapplication-TileImage" content="http://www.lindonslog.com/wp-content/uploads/2015/09/cropped-L-270x270.png" />

<style id="custom-css-css">.input_prompt{color:#06c}.output_prompt{color:#c00}.prompt{font-family:monospace;font-size:14px}.c,c1{color:#408080;font-style:italic}.k{color:#382;font-weight:700}.kn{color:#382;font-weight:700}.mi{color:#080}.mf{color:#080}.o{color:#96f}.ow{color:#BA22FF;font-weight:700}.nb{color:#382}.n{color:#000}.s,.s1{color:#c22}.se{color:#c22;font-weight:700}.si{color:#C06688;font-weight:700}.nn{color:#4D00FF;font-weight:700}.output_area pre{background-color:#FFF;padding-left:5%}.code_cell{padding-left:1%}.cell{margin-top:10px;margin-bottom:10px}br{line-height:2}.cell h1,h2,h3,h4{margin-top:30px;margin-bottom:10px}</style>
</head>

<body class="archive paged category category-programming category-6 paged-2 category-paged-2">
<div id="page" class="hfeed site">

	<header id="masthead" class="site-header" role="banner">
		<div id="botmenu">
			<div class="container_6">
			<div id="submenu" class="menu-top-container"><ul id="web2feel" class="sfmenu"><li id="menu-item-434" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-434"><a href="../../../../index.html">About</a></li>
<li id="menu-item-405" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-405"><a href="../../../mathematics/index.html">Mathematics</a>
<ul class="sub-menu">
	<li id="menu-item-814" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-814"><a href="../../../mathematics/linear-algebra/index.html">Linear Algebra</a></li>
	<li id="menu-item-406" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-406"><a href="../../../mathematics/statistics/index.html">Statistics</a></li>
</ul>
</li>
<li id="menu-item-404" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-404"><a href="../../../linux-unix/index.html">Linux/Unix</a></li>
<li id="menu-item-407" class="menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item menu-item-has-children menu-item-407"><a href="../../index.html">Programming</a>
<ul class="sub-menu">
	<li id="menu-item-408" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-408"><a href="../../openmp/index.html">OpenMP</a></li>
	<li id="menu-item-487" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-487"><a href="../../r/index.html">R</a></li>
</ul>
</li>
<li id="menu-item-842" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-842"><a href="../../../../links/index.html">Links</a></li>
</ul></div>			</div>
		</div>
		
		<div class="top cf ">
			<div class="head container_6 cf">
				<div class="logo grid_3">
					
					<h1 class="site-title logo"><a id="blogname" rel="home" href="../../../../index.html" title="Lindons Log">Lindons Log</a></h1>
	
					<h2 class="site-description"></h2>
				</div>
				<div class="topbar grid_3">
						<form method="get" id="searchform" action="../../../../index.html" role="search">
		<label for="s" class="assistive-text">Search</label>
		<input type="text" class="field" name="s" value="" id="s" placeholder="Search &hellip;" />
		<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search" />
	</form>
				</div>
			</div>	
		</div>
	</header><!-- #masthead .site-header -->

	<div id="main" class="site-main container_6">
		<section id="primary" class="content-area">
			<div id="content" class="site-content" role="main">

			
				<header class="page-header">
					<h1 class="page-title">
						Category Archives: <span>Programming</span>					</h1>
									</header><!-- .page-header -->

								
					
<article id="post-883" class="grid_6 post-883 post type-post status-publish format-standard has-post-thumbnail hentry category-mathematics category-programming category-r category-statistics tag-mcmc tag-metropolis-coupled tag-mpi tag-parallel tag-parallel-tempering tag-r tag-rmpi">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1181 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/parallel-tempering-r-rmpi/index.html#comments"><span class="dsq-postid" data-dsqidentifier="883 http://www.lindonslog.com/?p=883">5 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/parallel-tempering-r-rmpi/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Parallel Tempering in R with Rmpi</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
									<a class="thickbox" title= "Parallel Tempering in R with Rmpi" href="../../../../wp-content/uploads/2013/10/parallel_tempering_mixing.jpeg"><img style="margin-bottom:20px;" src="../../../../wp-content/uploads/2013/10/parallel_tempering_mixing-720x405.jpeg"/></a>
								
				<p>My office computer recently got a really nice upgrade and now I have 8 cores on my desktop to play with. I also at the same time received some code for a Gibbs sampler written in R from my adviser. I wanted to try a metropolis-coupled markov chain monte carlo, <img src="http://s0.wp.com/latex.php?latex=MC%5E%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="MC^{3}" title="MC^{3}" class="latex" />, algorithm on it to try and improve the mixing but the problem was that it was written in R and I&#8217;m used to writing parallel code in C/C++ with OpenMP or MPI. Previously I wrote about a parallel tempering algorithm with an implementation in C++ using OpenMP and so I thought I would try and code up the same sort of thing in R as a warm-up exercise before I started with the full <img src="http://s0.wp.com/latex.php?latex=MC%5E%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="MC^{3}" title="MC^{3}" class="latex" /> algorithm. Sadly I don&#8217;t think there is any facility in R for OpenMP style parallelism. There are packages such as <em>snow</em> and <em>multicore</em> but these are very high level packages and don&#8217;t really allow one to control the finer details. There is, however, Rmpi. It is a little bit different from regular C/Fortran MPI implementations and I once had a very bad experience getting some Rmpi code to work for a project deadline, it wasn&#8217;t pretty, so I was a little reluctant to reconsider this package but if you look at the changelogs it is still being actively maintained and in the end I&#8217;m very happy with the outcome of this experiment. I tried to write the below code as generally as possible, so that it is easily adapted by myself, or others, in the future.</p>
<h2>Target Density</h2>
<p>First one needs to write a density one wishes to sample from</p>
<pre class="brush: r; title: ; notranslate" title="">
logdensity&lt;-function(theta){
  #Distribution one wishes to sample from here.
  #It may be more convinient to pass a theta as a list
  sigma2=0.001;
  Sigma=matrix(0,2,2);
  Sigma[1,1]=sigma2;
  Sigma[2,2]=sigma2;
  density=dmvnorm(theta,c(0,0),Sigma)+dmvnorm(theta,c(-2,0.8),Sigma)+dmvnorm(theta,c(-1,1),Sigma)+dmvnorm(theta,c(1,1),Sigma)+dmvnorm(theta,c(0.5,0.5),Sigma);
  return(log(density))
}
</pre>
<p>The density I chose was a mixture of 5 well-separated bi-variate Normals. One should note that it is probably cleanest to pass all the arguments to this function as a list theta. It wasn&#8217;t really necessary in this case but if you have a posterior distribution with a number of parameters of varying dimension then it would be much nicer as a list. In a future blog post I may change the target density to be the energy distribution of a Lennard-Jones cluster. </p>
<h2>Parallel Tempering Algorithm</h2>
<p>This too is written as a function because Rmpi allows you to pass the function to all slaves and execute it. It was basically the easiest way of writing it for Rmpi.</p>
<pre class="brush: r; title: ; notranslate" title="">
temper&lt;-function(niter,Bmin,swap.interval){
  rank=mpi.comm.rank();
  size=mpi.comm.size();
  swap=0;
  swaps.attempted=0;
  swaps.accepted=0;
  
  #Higher ranks run the higher &quot;temperatures&quot; (~smaller fractional powers)
  B=rep(0,size-1);
  for(r in 1:size-1){
    temp=(r-1)/(size-2);
    B[r]=Bmin^temp;
  }
  
  
  #Create a list for proposal moves
  prop=rep(0,2);
  theta=matrix(0,niter,2)
  
  for(t in 2:niter){
    
    for(c in 1:length(prop))   prop=theta[t-1,c]+rnorm(1,0,0.1);
    
    #Calculate Log-Density at proposed and current position
    logdensity.current=logdensity(theta[t-1,])
    logdensity.prop=logdensity(prop);
    
    #Calculate log acceptance probability
    lalpha=B[rank]*(logdensity.prop-logdensity.current)
    
    if(log(runif(1))&lt;lalpha){
      #Accept proposed move
      theta[t,]=prop;
      logdensity.current=logdensity.prop;
    }else{
      #Otherwise do not move
      theta[t,]=theta[t-1,];
    } 
    
    if(t%%swap.interval ==0){
      for(evenodd in 0:1){
        swap=0;
        logdensity.partner=0;
        if(rank%%2 == evenodd%%2){
          rank.partner=rank + 1;
          #ranks range from 1:size-1. Cannot have a partner rank == size
          if(0&lt;rank.partner &amp;&amp; rank.partner&lt;size){
            #On first iteration, evens receive from above odd
            #On second iteration, odds receive from above evens
            logdensity.partner&lt;-mpi.recv.Robj(rank.partner,rank.partner);
            lalpha = (B[rank]-B[rank.partner])*(logdensity.partner-logdensity.current);
            swaps.attempted=swaps.attempted+1;
            if(log(runif(1))&lt;lalpha){
              swap=1;
              swaps.accepted=swaps.accepted+1;
            }
            mpi.send.Robj(swap,dest=rank.partner,tag=rank)
          }
          if(swap==1){
            thetaswap=theta[t,];
            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank)
            theta[t,]=mpi.recv.Robj(rank.partner,rank.partner)
          }
        }else{
          rank.partner=rank-1;
          #ranks range from 1:size-1. Cannot have a partner rank ==0
          if(0&lt;rank.partner &amp;&amp; rank.partner&lt;size){
            #On first iteration, odds send to evens below
            #On second iteration, evens sent to odds below
            mpi.send.Robj(logdensity.current,dest=rank.partner,tag=rank);
            swap=mpi.recv.Robj(rank.partner,rank.partner);
          }
          if(swap==1){
            thetaswap=theta[t,];
            theta[t,]=mpi.recv.Robj(rank.partner,rank.partner);
            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank);
          }
        }
      }
    }
  }
  return(theta)
}
</pre>
<p>The bulk of the above code is the communication of each processor with its next nearest neighbors. Metropolis moves will be attempted every <em>swap.interval</em> iterations, an argument one can pass to the function. When this code block is entered, even rank processors will partner with their higher ranked odd neighbours (they have a high rank so higher temperature i.e. smaller fractional power &#8211; a more &#8220;melted down&#8221; target density). The higher odd partners will send their lower even partners the value of their density and then the lower even partners will calculate an acceptance probabilty. If the move succeeds the lower rank even processors send their higher rank odd processors a binary swap=1 telling the higher rank odd processors that a send/receive procedure will occur. The lower even rank sends the higher odd rank its parameters and then subsequently the higher odd rank sends its lower even rank its parameters. In this way a metropolis move between processors is achieved. Next, odd rank processors form partners with their higher even ranked neighbours (because we need to swap with processor rank 1, the target density). The same procedure occurs as before but swapping odd for even. More visually, first swaps are attempted between 2-3, 4-5, 6-7 etc and then swaps are attempted between 1-2, 3-4, 5-6. This is almost like a merge-sort style algorithm. One can see how the parameters could be passed from 3 down to 2 and then from 2 down to 1. The main point is that each processor attempts a swap with its nearest-neighbours, the one above and the one below, every <em>swap.interval</em> iterations.<br />
With these functions defined one can now proceed to set up the mpi communicator/world.</p>
<h2>Rmpi</h2>
<p>First spawn some slaves.</p>
<pre class="brush: r; title: ; notranslate" title="">
library(Rmpi)
mpi.spawn.Rslaves(nslaves=6)
</pre>
<p>If it worked, you should see something like this:</p>
<pre class="brush: r; title: ; notranslate" title="">
&gt; mpi.spawn.Rslaves(nslaves=6)
	6 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 7 is running on: cabbage 
slave1 (rank 1, comm 1) of size 7 is running on: cabbage 
slave2 (rank 2, comm 1) of size 7 is running on: cabbage 
slave3 (rank 3, comm 1) of size 7 is running on: cabbage 
slave4 (rank 4, comm 1) of size 7 is running on: cabbage 
slave5 (rank 5, comm 1) of size 7 is running on: cabbage 
slave6 (rank 6, comm 1) of size 7 is running on: cabbage 
</pre>
<p>(yes, my office computer was named cabbage, lettuce is the one next to me). One can then send the function definitions to the slave processors.</p>
<pre class="brush: r; title: ; notranslate" title="">
niter=3000
Bmin=0.005
swap.interval=3
#Send to slaves some required data
mpi.bcast.Robj2slave(niter)
mpi.bcast.Robj2slave(Bmin)
mpi.bcast.Robj2slave(swap.interval)
#Send to slaves the logdensity function
mpi.bcast.Robj2slave(logdensity)
#Send to slaves the temper function
mpi.bcast.Robj2slave(temper) 
#Send to slaves the dmvnorm function
mpi.bcast.Robj2slave(dmvnorm) 
</pre>
<p>If you want to make sure that the slaves have the correct function definition, one can execute the command <em>mpi.remote.exec(temper)</em> and this will return the function definition. That is all, now it can be run.</p>
<pre class="brush: r; title: ; notranslate" title="">
mcmc=mpi.remote.exec(temper(niter,Bmin,swap.interval))
</pre>
<p>This returns a list object containing the mcmc draws for each slave.</p>
<h3>Results</h3>
<p>The end product is something that looks like this<br />
<a href="../../../../wp-content/uploads/2013/10/parallel_tempering_mixing.jpeg"><img src="../../../../wp-content/uploads/2013/10/parallel_tempering_mixing-300x300.jpeg" alt="parallel tempering mixing" width="300" height="300" class="aligncenter size-medium wp-image-889" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing-300x300.jpeg 300w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing-150x150.jpeg 150w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering_mixing.jpeg 1000w" sizes="(max-width: 300px) 100vw, 300px" /></a><br />
Which are the draws (in black) from the target distribution. It is also useful to build up intuition for parallel tempering to look at what is happening on the other processors. The draws for all processors are shown below:<br />
<a href="../../../../wp-content/uploads/2013/10/parallel_tempering.jpeg"><img src="../../../../wp-content/uploads/2013/10/parallel_tempering-682x1024.jpeg" alt="parallel tempering draws for each processor" width="640" height="960" class="aligncenter size-large wp-image-891" srcset="http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering-682x1024.jpeg 682w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering-200x300.jpeg 200w, http://www.lindonslog.com/wp-content/uploads/2013/10/parallel_tempering.jpeg 1000w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>
<p>N.B. Although my computer only has 8 cores I tried running the code 12 slaves. At first I was concerned that the MPI communications would enter a deadlock and the code would hang but it didn&#8217;t, so it seems you can scale up the number of slaves above the number of cores.</p>
<h2>Temperature Set</h2>
<p>Notice that the temperature set used in the code has the property that <img src="http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cbeta_%7Bn%7D%7D%7B%5Cbeta_%7Bn%2B1%7D%7D%3Dc&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;frac{&#92;beta_{n}}{&#92;beta_{n+1}}=c" title="&#92;frac{&#92;beta_{n}}{&#92;beta_{n+1}}=c" class="latex" />, for c a constant. There is a paper by Kofke(2002) that justifies this temperature set as it yields a constant acceptance ratio between cores under certain conditions. Indeed, the acceptance ratio (the fraction of metropolis moves that succeeded between cores) are roughly constant, as shown below:</p>
<pre class="brush: r; title: ; notranslate" title="">
[1] 0.7227723
[1] 0.7926793
[1] 0.710171
[1] 0.8037804
[1] 0.7191719
[1] 0.7974797
[1] 0.729673
[1] 0.8223822
[1] 0.8184818
[1] 0.8445845
</pre>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1039%2Fb509983h&#038;rft.atitle=Parallel+tempering%3A+Theory%2C+applications%2C+and+new+perspectives&#038;rft.jtitle=Physical+Chemistry+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Fxlink.rsc.org%2F%3FDOI%3Db509983h&#038;rft.volume=7&#038;rft.issue=23&#038;rft.issn=1463-9076&#038;rft.spage=3910&#038;rft.date=2005&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Earl+David+J.&#038;rft.aulast=Earl&#038;rft.aufirst=David+J.&#038;rft.au=Deem+Michael+W.&#038;rft.aulast=Deem&#038;rft.aufirst=Michael+W.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CMathematics%2CPhysics">Earl D.J. &#038; Deem M.W. (2005). Parallel tempering: Theory, applications, and new perspectives, <span style="font-style:italic;">Physical Chemistry Chemical Physics, 7</span> (23) 3910. DOI: <a rel="author" href="http://dx.doi.org/10.1039%2Fb509983h">10.1039/b509983h</a></span><br />
<span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1063%2F1.1507776&#038;rft.atitle=On+the+acceptance+probability+of+replica-exchange+Monte+Carlo+trials&#038;rft.jtitle=The+Journal+of+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Flink.aip.org%2Flink%2FJCPSA6%2Fv117%2Fi15%2Fp6911%2Fs1%26Agg%3Ddoi&#038;rft.volume=117&#038;rft.issue=15&#038;rft.issn=00219606&#038;rft.spage=6911&#038;rft.date=2002&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Kofke+David+A.&#038;rft.aulast=Kofke&#038;rft.aufirst=David+A.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CPhysics">Kofke D.A. (2002). On the acceptance probability of replica-exchange Monte Carlo trials, <span style="font-style:italic;">The Journal of Chemical Physics, 117</span> (15) 6911. DOI: <a rel="author" href="http://dx.doi.org/10.1063%2F1.1507776">10.1063/1.1507776</a></span></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-883 -->

				
					
<article id="post-859" class="grid_6 post-859 post type-post status-publish format-standard has-post-thumbnail hentry category-programming category-r category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1216 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/apply-parallel-distributed-grid-cluster-r/index.html#respond"><span class="dsq-postid" data-dsqidentifier="859 http://www.lindonslog.com/?p=859">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/apply-parallel-distributed-grid-cluster-r/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Easy 3-Minute Guide to Making apply() Parallel over Distributed Grids and Clusters in R</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
									<a class="thickbox" title= "Easy 3-Minute Guide to Making apply() Parallel over Distributed Grids and Clusters in R" href="../../../../wp-content/uploads/2013/08/boscosmall.png"><img style="margin-bottom:20px;" src="../../../../wp-content/uploads/2013/08/boscosmall.png"/></a>
								
				<p>Last week I attended a workshop on how to run highly parallel distributed jobs on the Open Science Grid (osg). There I met <a href="http://derekweitzel.blogspot.com/" title="BoscoR">Derek Weitzel</a> who has made an excellent contribution to advancing R as a high performance computing language by developing BoscoR. BoscoR greatly facilitates the use of the already existing package &#8220;GridR&#8221; by allowing the R user to use <a href="http://bosco.opensciencegrid.org/about/">Bosco</a> to manage the submission of jobs. It seems no matter how many kinds of queue-submission system I become familiar with (torque,sge,condor), the current cluster I&#8217;m working on uses something foreign and so I have to relearn how to write a job submission file. One of the two major selling points of Bosco is that it allows the user to write one job submission file locally (based on HTCondor) and use it to submit jobs on various remote clusters all using different interfaces. The second major selling point is that Bosco will manage work sharing if you have access to more than one cluster, that is it will submit jobs to each cluster proportional to how unburdened that cluster is, which is great if you have access to 3 clusters. It means the users apply jobs will get through the queue as quickly as possible by cleverly distributing the work over all available clusters. Hopefully that will have convinced you that Bosco is worth having, now lets proceed with how to use it. I will illustrate the process by using Duke University&#8217;s cluster, the DSCR. There are three steps: 1) Installing Bosco 2) Installing GridR 3) Running a test job.<br />
<br \></p>
<h2>Installing Bosco</h2>
<p>First go ahead and download <a href="http://bosco.opensciencegrid.org/download-form/?package=1.2/bosco_quickstart.tar.gz" rel="external nofollow" title="boscor download">Bosco</a>, the sign-up is only for the developers to get an idea of how many people are using it. Detailed install instructions can be found <a href="https://confluence.grid.iu.edu/pages/viewpage.action?pageId=10944561">here</a> but I will also go through the steps.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[lindon@laptop Downloads]$ tar xvzf ./bosco_quickstart.tar.gz
[lindon@laptop Downloads]$ ./bosco_quickstart
</pre>
<p>The executable will then ask some questions:</p>
<p>Do you want to install Bosco? Select y/n and press [ENTER]: y<br />
Type the cluster name and press [ENTER]: dscr-login-01.oit.duke.edu<br />
When prompted &#8220;Type your name at dscr-login-01.oit.duke.edu (default YOUR_USER) and press [ENTER]: NetID<br />
When prompted &#8220;Type the queue manager for login01.osgconnect.net (pbs, condor, lsf, sge, slurm) and press [ENTER]: sge<br />
Then when prompted &#8220;NetID@dscr-login-01.oit.duke.edu&#8217;s password: XXXXXXX</p>
<p>For duke users, the HostName of the DCSR is dscr-login-01.oit.duke.edu. You login with your NetID and the queue submission system is the Sun Grid Engine, so type sge. If you already have <a href="../../../../linux-unix/ssh-keygen-keys/index.html" title="Creating SSH Keys with ssh-keygen and ssh-copy-id" target="_blank">SSH-Keys</a> set up then I think the last question gets skipped. That takes care of the installation. You can now try submitting on the remote cluster locally from your laptop. Download this test <a href="../../../../example_code/short.sh">executable</a> and this <a href="../../../../example_code/bosco01.sub">submission file</a>. Start Bosco and try submitting a job.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[msl33@hotel ~/tutorial-bosco]$ source ~/bosco/bosco_setenv
[msl33@hotel ~/tutorial-bosco]$ bosco_start
BOSCO Started
[msl33@hotel ~/tutorial-bosco]$ condor_submit bosco01.sub 
Submitting job(s).
1 job(s) submitted to cluster 70.
[msl33@hotel ~/tutorial-bosco]$ condor_q


-- Submitter: hotel.stat.duke.edu : &lt;127.0.0.1:11000?sock=21707_cbb6_3&gt; : hotel.stat.duke.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  70.0   msl33           8/31 12:08   0+00:00:00 I  0   0.0  short.sh          

1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended
</pre>
<p>This is the result if all has worked well. Note that you need to start Bosco by the above two lines.</p>
<p><br \></p>
<h2>Installing GridR</h2>
<p>The current version of GridR on CRAN is an older version doesn&#8217;t support job submission by bosco. It will when CRAN gets the latest version of GridR but until then you need to install GridR from source so download it <a href="https://github.com/osg-bosco/GridR/releases/download/v0.9.7/GridR_0.9.7.tar.gz" rel="external nofollow">here</a> and install it:</p>
<pre class="brush: r; title: ; notranslate" title="">
install.packages(&quot;~/Downloads/GridR_0.9.7.tar.gz&quot;, repos=NULL, type=&quot;source&quot;)
</pre>
<p><br \></p>
<h2>Running a Parallel Apply on the Cluster</h2>
<p>Consider a toy example which approximates pi by monte-carlo.</p>
<pre class="brush: r; title: ; notranslate" title="">
montecarloPi &lt;- function(trials, inst) {
  count = 0
  for(i in 1:trials) {
    if((runif(1,0,1)^2 + runif(1,0,1)^2)&lt;1) {
      count = count + 1
    }
  }
  return((count*4)/trials)
}
</pre>
<p>One can now use grid.apply from the GridR package combined with Bosco to submit jobs on the remote cluster from within the users local R session.</p>
<pre class="brush: plain; title: ; notranslate" title="">
# load the GridR library
library(&quot;GridR&quot;)
grid.init(service=&quot;bosco.direct&quot;, localTmpDir=&quot;tmp&quot;)
# Send 10 instances of the montecarloPi
grid.apply(&quot;pi_estimate&quot;, montecarloPi, 10000000, c(1:10), batch=c(2))
</pre>
<p>You can then see how your jobs are getting on by the &#8220;grid.printJobs()&#8221; command.<br />
<a href="../../../../wp-content/uploads/2013/08/parallel_grid_apply.png"><img src="../../../../wp-content/uploads/2013/08/parallel_grid_apply-1024x358.png" alt="parallel grid apply" width="640" height="223" class="aligncenter size-large wp-image-876" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply-1024x358.png 1024w, http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply-300x105.png 300w, http://www.lindonslog.com/wp-content/uploads/2013/08/parallel_grid_apply.png 1325w" sizes="(max-width: 640px) 100vw, 640px" /></a><br />
When it completes, &#8220;pi_estimate&#8221; will be a list object with 10 elements containing approximations to pi. Obviously, there is an overhead with submitting jobs and also a lag time while these jobs get through the queue. One must balance this overhead with the computational time required to complete a single iteration of the apply function. Bosco will create and submit a job for every iteration of the apply function. If each iteration does not take too long but there exists a great many of them to perform, one could consider blocking these operations into, say, 10 jobs so that the queue lag and submission overhead is negligible in comparison to the time taken to complete no_apply_iteraions/10 computations, which also saves creating a large number of jobs on the cluster which might aggravate other users. One can also add clusters to bosco using the &#8220;bosco_cluster &#8211;add&#8221; command, so that jobs are submitted to whichever cluster has the most free cores available. All in all this is a great aid for those doing computationally intensive tasks and makes parallel work-sharing very easy indeed.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-859 -->

				
					
<article id="post-822" class="grid_6 post-822 post type-post status-publish format-standard hentry category-programming">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1241 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/error-in-c-c-symbol-name-not-in-load-table/index.html#respond"><span class="dsq-postid" data-dsqidentifier="822 http://www.lindonslog.com/?p=822">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/error-in-c-c-symbol-name-not-in-load-table/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Error in .C(&#8221; *** &#8220;) : C symbol name &#8221; *** &#8221; not in load table</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>If you are getting this error, as I was, then you are probably trying to write an extension for R in <em>C++</em> and not C. I was just writing a function &#8220;<a href="https://github.com/michaellindon/linsgp">linsgp</a>&#8221; in C++. See if the following scenario is familiar to you</p>
<pre class="brush: r; title: ; notranslate" title="">
&gt; dyn.load(&quot;main.so&quot;)
&gt; .C(&quot;linsgp&quot;)
Error in .C(&quot;linsgp&quot;) : C symbol name &quot;linsgp&quot; not in load table
</pre>
<p>My C++ code looked like this</p>
<pre class="brush: cpp; title: ; notranslate" title="">
...
void linsgp(){
...
</pre>
<p>What is missing is extern &#8220;C&#8221;, so it should look like this:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
...
extern &quot;C&quot; void linsgp(){
...
</pre>
<p>The reason is that C++ supports overloading of function names and so the compiler mangles the name with information about the arguments. C, however, does not support this and doesn&#8217;t mangle the name. Inserting extern &#8220;C&#8221; tells the compiler not to mangle the name such that the name used for linkage is C-compatible.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-822 -->

				
					
<article id="post-762" class="grid_6 post-762 post type-post status-publish format-standard hentry category-mathematics category-r category-statistics">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1247 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../mathematics/mcmc-interweaving-parameterization-efficiency/index.html#comments"><span class="dsq-postid" data-dsqidentifier="762 http://www.lindonslog.com/?p=762">1 Comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../mathematics/mcmc-interweaving-parameterization-efficiency/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Model Scale Parameterization for MCMC Efficiency</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>I recently came across a very interesting paper by Y. Yu and X. Meng[<a class="papercite_bibcite" href="index.html#paperkey_0">1</a>] who present an interweaving strategy between different model parameterizations to improve mixing. It is well known that different model parameterizations can perform better than others under certain conditions. Papaspiliopoulos, Roberts and Sköld [<a class="papercite_bibcite" href="index.html#paperkey_1">2</a>] present a general framework for how to parameterize hierarchical models and provide insights into the conditions under which centered and non-centered parameterizations outperform each other. One isn&#8217;t, however, restricted to reperameterizations of location parameters only, as outlined in the aforementioned paper, and so I decided to experiment with reparameterizations of the scale parameter in a simple hierarchical model with improper priors on the parameters.</p>
<h2>Centered Parameterization</h2>
<p>Papaspiliopoulos gave a general definition of the centered parameterization to be when <img src="http://s0.wp.com/latex.php?latex=Y_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="Y_{i}" title="Y_{i}" class="latex" /> is independent of <img src="http://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda" title="&#92;lambda" class="latex" /> given <img src="http://s0.wp.com/latex.php?latex=X_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="X_{i}" title="X_{i}" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Y_%7Bi%7D%7CX_%7Bi%7D%2C%5Csigma%5E%7B2%7D+%5Csim+N%28X_%7Bi%7D%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle Y_{i}|X_{i},&#92;sigma^{2} &#92;sim N(X_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (1)" title="&#92;displaystyle Y_{i}|X_{i},&#92;sigma^{2} &#92;sim N(X_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (1)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+X_%7Bi%7D%7C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%280%2C%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle X_{i}|&#92;sigma^{2},&#92;lambda^{2} &#92;sim N(0,&#92;lambda^{2}&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (2)" title="&#92;displaystyle X_{i}|&#92;sigma^{2},&#92;lambda^{2} &#92;sim N(0,&#92;lambda^{2}&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (2)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28+%5Clambda%5E%7B2%7D+%29+%5Cpropto+%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle p( &#92;lambda^{2} ) &#92;propto &#92;frac{1}{&#92;lambda^{2}} &#92; &#92; &#92; &#92; &#92; (3)" title="&#92;displaystyle p( &#92;lambda^{2} ) &#92;propto &#92;frac{1}{&#92;lambda^{2}} &#92; &#92; &#92; &#92; &#92; (3)" class="latex" /></p>
<h3>Full Conditionals</h3>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clambda%5E%7B2%7D%7CY_%7B1%3An%7D%2CX_%7B1%3An%7D%2C%5Csigma%5E%7B2%7D+%5Csim+%5CGamma%5E%7B-1%7D%5Cleft%28+%5Cfrac%7Bn%7D%7B2%7D%2C+%5Cfrac%7B%5Csum_%7Bi%7D%5E%7Bn%7D+X_%7Bi%7D%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%29+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;lambda^{2}|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim &#92;Gamma^{-1}&#92;left( &#92;frac{n}{2}, &#92;frac{&#92;sum_{i}^{n} X_{i}^{2}}{2&#92;sigma^{2}}&#92;right) &#92; &#92; &#92; &#92; &#92; (4)" title="&#92;displaystyle &#92;lambda^{2}|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim &#92;Gamma^{-1}&#92;left( &#92;frac{n}{2}, &#92;frac{&#92;sum_{i}^{n} X_{i}^{2}}{2&#92;sigma^{2}}&#92;right) &#92; &#92; &#92; &#92; &#92; (4)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+X_%7Bi%7D%7CY_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%5Cleft%28+%5Cfrac%7B%5Cfrac%7BY_%7Bi%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%7B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%5Csigma%5E%7B2%7D%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle X_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{Y_{i}}{&#92;sigma^{2}}}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (5)" title="&#92;displaystyle X_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{Y_{i}}{&#92;sigma^{2}}}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{1}{&#92;sigma^{2}}+&#92;frac{1}{&#92;lambda^{2}&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (5)" class="latex" /></p>
<h2>Non-Centered Parameterization</h2>
<p>Papaspiliopoulos gave a general definition of the non-centered parameterization to be when <img src="http://s0.wp.com/latex.php?latex=%5Ctilde%7BX%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;tilde{X}_{i}" title="&#92;tilde{X}_{i}" class="latex" /> and <img src="http://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda" title="&#92;lambda" class="latex" /> are a priori independent.</p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Y_%7Bi%7D%7C%5Ctilde%7BX%7D_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda+%5Csim+N%28%5Clambda+%5Ctilde%7BX%7D_%7Bi%7D%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle Y_{i}|&#92;tilde{X}_{i},&#92;sigma^{2},&#92;lambda &#92;sim N(&#92;lambda &#92;tilde{X}_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (6)" title="&#92;displaystyle Y_{i}|&#92;tilde{X}_{i},&#92;sigma^{2},&#92;lambda &#92;sim N(&#92;lambda &#92;tilde{X}_{i},&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (6)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctilde%7BX%7D_%7Bi%7D%7C%5Csigma%5E%7B2%7D+%5Csim+N%280%2C%5Csigma%5E%7B2%7D%29+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;tilde{X}_{i}|&#92;sigma^{2} &#92;sim N(0,&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (7)" title="&#92;displaystyle &#92;tilde{X}_{i}|&#92;sigma^{2} &#92;sim N(0,&#92;sigma^{2}) &#92; &#92; &#92; &#92; &#92; (7)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28%5Clambda%29+%5Cpropto+1+%5C+%5C+%5C+%5C+%5C+%288%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle p(&#92;lambda) &#92;propto 1 &#92; &#92; &#92; &#92; &#92; (8)" title="&#92;displaystyle p(&#92;lambda) &#92;propto 1 &#92; &#92; &#92; &#92; &#92; (8)" class="latex" /></p>
<h3>Full Conditionals</h3>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clambda%7CY_%7B1%3An%7D%2CX_%7B1%3An%7D%2C%5Csigma%5E%7B2%7D+%5Csim+N+%5Cleft%28+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7DY_%7Bi%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7D%5E%7B2%7D%7D%2C+%5Cfrac%7B%5Csigma%5E%7B2%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctilde%7BX%7D_%7Bi%7D%5E%7B2%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%289%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;lambda|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim N &#92;left( &#92;frac{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}Y_{i}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}}, &#92;frac{&#92;sigma^{2}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}} &#92;right) &#92; &#92; &#92; &#92; &#92; (9)" title="&#92;displaystyle &#92;lambda|Y_{1:n},X_{1:n},&#92;sigma^{2} &#92;sim N &#92;left( &#92;frac{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}Y_{i}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}}, &#92;frac{&#92;sigma^{2}}{&#92;sum_{i=1}^{n}&#92;tilde{X}_{i}^{2}} &#92;right) &#92; &#92; &#92; &#92; &#92; (9)" class="latex" /></p>
<p align="center"><img src="http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctilde%7BX%7D_%7Bi%7D%7CY_%7Bi%7D%2C%5Csigma%5E%7B2%7D%2C%5Clambda%5E%7B2%7D+%5Csim+N%5Cleft%28+%5Cfrac%7B%5Cfrac%7B%5Clambda+Y_%7Bi%7D%7D%7B%5Csigma%5E%7B2%7D%7D%7D%7B%5Cfrac%7B%5Clambda%5E%7B2%7D%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Cfrac%7B%5Clambda%5E%7B2%7D%7D%7B%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%2810%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="&#92;displaystyle &#92;tilde{X}_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{&#92;lambda Y_{i}}{&#92;sigma^{2}}}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (10)" title="&#92;displaystyle &#92;tilde{X}_{i}|Y_{i},&#92;sigma^{2},&#92;lambda^{2} &#92;sim N&#92;left( &#92;frac{&#92;frac{&#92;lambda Y_{i}}{&#92;sigma^{2}}}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}}, &#92;frac{1}{&#92;frac{&#92;lambda^{2}}{&#92;sigma^{2}}+&#92;frac{1}{&#92;sigma^{2}}} &#92;right) &#92; &#92; &#92; &#92; &#92; (10)" class="latex" /></p>
<h2>Interweaving Strategy</h2>
<p>Generally when the CP works well, the NCP works poorly and vice versa. Yaming Yu and Xiao-Li Meng[<a class="papercite_bibcite" href="index.html#paperkey_0">1</a>] present a way of combining both strategies by interweaving the Gibbs steps of both parameterizations at each iteration. The details can be read in their paper. I decided to test all three Gibbs samplers with the following R code:</p>
<pre class="brush: r; title: ; notranslate" title="">
#Generate Data
lam2=0.5
lam=sqrt(lam2)
sig2=1
n=1000
Xt=rnorm(n,0,sqrt(lam2*sig2))
Y=rnorm(n,Xt,sqrt(sig2))
nmc=2000
X=Xt

#Centered Parameterization
cp_lam2=rep(0,nmc)
cp_X=matrix(0,nmc,n)
for(i in 1:nmc){
	inv_lam2=rgamma(1,(n)/2,rate=(t(X)%*%X)/(2*sig2))
	lam2=1/inv_lam2
	X=rnorm(n,(1/(1/sig2 + 1/(sig2*lam2)))*Y/sig2, sqrt(1/(1/sig2 + 1/(sig2*lam2))))
	cp_lam2[i]=lam2
	cp_X[i,]=X
}
mean_cp_X=apply(cp_X,2,mean)

#Non-Centered Parameterization
X=Xt
ncp_lam2=rep(0,nmc)
ncp_X=matrix(0,nmc,n)
for(i in 1:nmc){
	lam=rnorm(1,t(X)%*%Y/(t(X)%*%X), sqrt(sig2/(t(X)%*%X)))
	lam2=lam*lam;
	X=rnorm(n, (1/(1/sig2 + lam2/sig2))*lam*Y/sig2, sqrt(1/(1/sig2+lam2/sig2))  )
	ncp_lam2[i]=lam2
	ncp_X[i,]=X
}
mean_ncp_X=apply(ncp_X,2,mean)

#Interweaving Strategy
int_lam2=rep(0,nmc)
int_X=matrix(0,nmc,n)
for(i in 1:nmc){
	X=rnorm(n,(1/(1/sig2 + 1/(sig2*lam2)))*Y/sig2, sqrt(1/(1/sig2 + 1/(sig2*lam2))))
	inv_lam2=rgamma(1,(n)/2,rate=(t(X)%*%X)/(2*sig2))
	half_lam2=1/inv_lam2
	X=X/sqrt(half_lam2)    #Transform to Xtilde
	lam=rnorm(1,t(X)%*%Y/(t(X)%*%X), sqrt(sig2/(t(X)%*%X)))
	lam2=lam*lam;
	int_lam2[i]=lam2
	int_X[i,]=X
}
mean_cp_X=apply(cp_X,2,mean)

#Remove Burnin
cp_lam2=cp_lam2[-(1:1000)]
ncp_lam2=ncp_lam2[-(1:1000)]
int_lam2=int_lam2[-(1:1000)]

#Plot Results
par(mfrow=c(3,3))
acf(cp_lam2)
plot(cp_lam2,type=&quot;l&quot;)
plot(cp_lam2[1:nmc-1],cp_lam2[2:nmc])
acf(ncp_lam2)
plot(ncp_lam2,type=&quot;l&quot;)
plot(ncp_lam2[1:nmc-1],ncp_lam2[2:nmc])
acf(int_lam2)
plot(int_lam2,type=&quot;l&quot;)
plot(int_lam2[1:nmc-1],int_lam2[2:nmc])
</pre>
<h2>Results</h2>
<h3><img src="http://s0.wp.com/latex.php?latex=%5Clambda%3D0.3&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda=0.3" title="&#92;lambda=0.3" class="latex" /></h3>
<div id="attachment_798" style="width: 910px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/08/lam0.3.png"><img class="size-full wp-image-798" alt="mcmc parameterization" src="../../../../wp-content/uploads/2013/08/lam0.3.png" width="900" height="900" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3.png 900w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam0.3-300x300.png 300w" sizes="(max-width: 900px) 100vw, 900px" /></a><p class="wp-caption-text">Interweaving outperforms non-centered outperforms centered</p></div>
<h3><img src="http://s0.wp.com/latex.php?latex=%5Clambda%3D6&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;lambda=6" title="&#92;lambda=6" class="latex" /></h3>
<div id="attachment_802" style="width: 910px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/08/lam6.png"><img class="size-full wp-image-802" alt="ncp poorly mixing" src="../../../../wp-content/uploads/2013/08/lam6.png" width="900" height="900" srcset="http://www.lindonslog.com/wp-content/uploads/2013/08/lam6.png 900w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam6-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/08/lam6-300x300.png 300w" sizes="(max-width: 900px) 100vw, 900px" /></a><p class="wp-caption-text">Interweaving outperforms centered outperforms non-centered</p></div>
<h2>Discussion</h2>
<p>As lambda gets small the centered parameterization becomes ever more autocorrelated and poorly mixing. When lambda becomes large the non-centered parameterization becomes ever more autocorrelated and poorly mixing. The interweaved Gibbs sampler exhibits great mixing in all cases.</p>
<div id="paperkey_0" class="papercite_entry">[1]           <a href='http://dx.doi.org/10.1198/jcgs.2011.203main' class='papercite_doi' title='View document in publisher site'><img src='../../../../wp-content/plugins/papercite/img/external.png' width='10' height='10' alt='[doi]' /></a>        Y. Yu and X. Meng, &#8220;To Center or Not to Center: That Is Not the Question&#8211;An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency,&#8221; <span style="font-style: italic">Journal of computational and graphical statistics</span>, vol. 20, iss. 3, pp. 531-570, 2011. <br />    <a href="javascript:void(0)" id="papercite_0" class="papercite_toggle">[Bibtex]</a></div>
<div class="papercite_bibtex" id="papercite_0_block">
<pre><code class="tex bibtex">@article{Yu11,
author = {Yu, Yaming and Meng, Xiao-Li},
citeulike-article-id = {10408757},
citeulike-linkout-0 = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2011.203main},
citeulike-linkout-1 = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2011.203main},
citeulike-linkout-2 = {http://dx.doi.org/10.1198/jcgs.2011.203main},
doi = {10.1198/jcgs.2011.203main},
journal = {Journal of Computational and Graphical Statistics},
number = {3},
pages = {531--570},
posted-at = {2012-03-03 18:10:07},
priority = {2},
title = {{To Center or Not to Center: That Is Not the Question--An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2011.203main},
volume = {20},
year = {2011}
}</code></pre>
</div>
<div id="paperkey_1" class="papercite_entry">[2]                   O. Papaspiliopoulos, G. O. Roberts, and M. Sköld, &#8220;A general framework for the parametrization of hierarchical models,&#8221; <span style="font-style: italic">Statistical science</span>, vol. 22, iss. 1, pp. 59-73, 2007. <br />    <a href="javascript:void(0)" id="papercite_1" class="papercite_toggle">[Bibtex]</a></div>
<div class="papercite_bibtex" id="papercite_1_block">
<pre><code class="tex bibtex">@article{Papaspiliopoulos07,
abstract = {{In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.}},
author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sk\&quot;{o}ld, Martin},
citeulike-article-id = {8977350},
citeulike-linkout-0 = {http://www.jstor.org/stable/27645805},
journal = {Statistical Science},
number = {1},
pages = {59--73},
posted-at = {2011-03-10 18:55:50},
priority = {2},
publisher = {Institute of Mathematical Statistics},
title = {{A general framework for the parametrization of hierarchical models}},
url = {http://www.jstor.org/stable/27645805},
volume = {22},
year = {2007}
}</code></pre>
</div>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1198%2Fjcgs.2011.203main&#038;rft.atitle=To+Center+or+Not+to+Center%3A+That+Is+Not+the+Question%E2%80%94An+Ancillarity%E2%80%93Sufficiency+Interweaving+Strategy+%28ASIS%29+for+Boosting+MCMC+Efficiency&#038;rft.jtitle=Journal+of+Computational+and+Graphical+Statistics&#038;rft.artnum=http%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1198%2Fjcgs.2011.203main&#038;rft.volume=20&#038;rft.issue=3&#038;rft.issn=1061-8600&#038;rft.spage=531&#038;rft.epage=570&#038;rft.date=2011&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Yu+Yaming&#038;rft.aulast=Yu&#038;rft.aufirst=Yaming&#038;rft.au=Meng+Xiao-Li&#038;rft.aulast=Meng&#038;rft.aufirst=Xiao-Li&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Computer+Science+%2F+Engineering%2CMathematics%2COther">Yu Y. &#038; Meng X.L. (2011). To Center or Not to Center: That Is Not the Question—An Ancillarity–Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency, <span style="font-style:italic;">Journal of Computational and Graphical Statistics, 20</span> (3) 531-570. DOI: <a rel="author" href="http://dx.doi.org/10.1198%2Fjcgs.2011.203main">10.1198/jcgs.2011.203main</a></span></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-762 -->

				
					
<article id="post-605" class="grid_6 post-605 post type-post status-publish format-standard hentry category-openmp category-programming category-statistics tag-c tag-mcmc tag-openmp-2 tag-parallel-tempering">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1265 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/openmp/parallel-tempering-algorithm-c/index.html#comments"><span class="dsq-postid" data-dsqidentifier="605 http://www.lindonslog.com/?p=605">2 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/openmp/parallel-tempering-algorithm-c/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Parallel Tempering Algorithm with OpenMP / C++</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p><a href="index.html#theory">1.1. Parallel Tempering Theory</a><br />
<a href="index.html#physics">1.2. Physics Origins</a><br />
<a href="index.html#intra">2.1 Intra-Thread Metropolis Move</a><br />
<a href="index.html#inter">2.2. Inter-Thread Parallel Tempering</a><br />
<a href="index.html#openmp">2.3. OpenMP Parallelization</a><br />
<a href="index.html#fullcode">3. Full Code</a><br />
<a href="index.html#simstudy">4. Simulation Study<br />
<a href="index.html#futureuse">5. On the Future use of Parallel Tempering with OpenMP</a></p>
<p>Parallel tempering is one of my favourite sampling algorithms to improve MCMC mixing times. This algorithm seems to be used <em>exclusively</em> on distributed memory architectures using MPI and remains unexploited on shared memory architectures such as our office computers, which have up to eight cores. I&#8217;ve written parallel tempering algorithms in MPI and Rmpi but never in OpenMP. It turns out that the latter has substantial advantages. I guess when people think of parallel tempering they think of processors communicating with each other via MPI and swapping parameters directly.  If you are on a shared memory device, however, you can have processor A simply write to a shared array and have processor B read therefrom, which really saves a lot of aggro fiddling around with message numbers, blocking/non-blocking calls and deadlocks etc. Moreover, with OpenMP you can spawn more threads than you have processors, which translates to more parallel MCMC chains in the present context, whereas this becomes troublesome with MPI due to the danger of deadlocks. OpenMP is also much easier to use than MPI, with one line you can fork a serial thread into a desired and hardware-independent  number of parallel threads. The code looks as follows:</p>
<p><a name="theory"></a><br />
<h2>Parallel Tempering Theory</h2>
<p>Each thread simulates an MCMC trajectory from the posterior raised to a fractional power, B. When B=1, the MCMC draws are from the posterior from which we wish to sample. When B=0, the MCMC trajectory is just a realization of a Brownian motion random walk. To see this, consider the acceptance probability of the metropolis move. The density evaluated at the proposed parameters over the density evaluated at the current parameters all raised to the power of zero is unity, whatever the densities are, so the moves always get accepted. Similarly if B is close to zero, then the acceptance probability is near unity and the distribution from which this MCMC is sampling is quite uniform over the parameter space, so the trajectory explores a relatively larger part of the parameter space. As B is increased toward one, the features of the distribution from which we wish to sample start to become more prominent. In the other direction from B=1 to 0 one commonly says that the posterior is &#8220;melted down&#8221; and spreading out its mass. The terminology has remained from its origins in statistical physics where one would simulated particles at a hotter temperature, so that they would jostle around more and escape wells in the potential energy. The key to parallel tempering is to use the more diffuse, hotter or melted down MCMC chains as proposal distributions for the actual cold distribution we wish to sample from. One proceeds by performing a Metropolis-Hastings move because the proposal distributions are not symmetric. For illustration, thread j uses the hotter thread j+1 as its partner and as proposal distribution. Let theta j+1 be the proposed new position for thread j, being the current position of thread j+1.<br />
<img src="http://s0.wp.com/latex.php?latex=%5Calpha%3Dmin%281%2C%5Cfrac%7B++p_%7Bj%7D+%28%5Ctheta_%7Bj%2B1%7D+%29%7D++%7B+++p_%7Bj%7D%28%5Ctheta_%7Bj%7D+%29+%7D++%5Cfrac%7B++p_%7Bj%2B1%7D+%28%5Ctheta_%7Bj%7D+%29%7D++%7B+++p_%7Bj%2B1%7D%28%5Ctheta_%7Bj%2B1%7D+%29+%7D++++%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;alpha=min(1,&#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    )  " title="&#92;alpha=min(1,&#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    )  " class="latex" /><br />
The second fraction is the Hastings addition to the Metropolis algorithm and is required to satisfy detailed balance for an unsymmetrical proposal distribution. Now realise that<br />
<img src="http://s0.wp.com/latex.php?latex=p_%7Bj%7D%3D%5Cpi%28%5Ctheta%7CY%29%5E%7BB_%7Bj%7D%7D%5C%5C++p_%7Bj%2B1%7D%3D%5Cpi%28%5Ctheta%7CY%29%5E%7BB_%7Bj%2B1%7D%7D++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="p_{j}=&#92;pi(&#92;theta|Y)^{B_{j}}&#92;&#92;  p_{j+1}=&#92;pi(&#92;theta|Y)^{B_{j+1}}  " title="p_{j}=&#92;pi(&#92;theta|Y)^{B_{j}}&#92;&#92;  p_{j+1}=&#92;pi(&#92;theta|Y)^{B_{j+1}}  " class="latex" /><br />
i.e. they are the same distribution raised to different fractional powers. Working now on the log scale, it can be shown that<br />
<img src="http://s0.wp.com/latex.php?latex=log+%5Cleft%28+%5Cfrac%7B++p_%7Bj%7D+%28%5Ctheta_%7Bj%2B1%7D+%29%7D++%7B+++p_%7Bj%7D%28%5Ctheta_%7Bj%7D+%29+%7D++%5Cfrac%7B++p_%7Bj%2B1%7D+%28%5Ctheta_%7Bj%7D+%29%7D++%7B+++p_%7Bj%2B1%7D%28%5Ctheta_%7Bj%2B1%7D+%29+%7D++++%5Cright%29+%3D++%28B_%7Bj%7D-B_%7Bj%2B1%7D%29+%5Cleft%28+log%28%5Cpi%5B%5Ctheta_%7Bj%2B1%7D%7CY%5D%29+-+log%28%5Cpi%5B%5Ctheta_%7Bj%7D%7CY%5D%29+%5Cright%29++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="log &#92;left( &#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    &#92;right) =  (B_{j}-B_{j+1}) &#92;left( log(&#92;pi[&#92;theta_{j+1}|Y]) - log(&#92;pi[&#92;theta_{j}|Y]) &#92;right)  " title="log &#92;left( &#92;frac{  p_{j} (&#92;theta_{j+1} )}  {   p_{j}(&#92;theta_{j} ) }  &#92;frac{  p_{j+1} (&#92;theta_{j} )}  {   p_{j+1}(&#92;theta_{j+1} ) }    &#92;right) =  (B_{j}-B_{j+1}) &#92;left( log(&#92;pi[&#92;theta_{j+1}|Y]) - log(&#92;pi[&#92;theta_{j}|Y]) &#92;right)  " class="latex" /><br />
<a name="physics"></a><br />
<h3>Physics Origins</h3>
<p>It is at this point where sometimes, in order to make things correspond to the earlier physics literature, one defines the &#8220;Energy&#8221; as<br />
 <img src="http://s0.wp.com/latex.php?latex=E_%7Bj%7D%3D-log%28%5Cpi%5B%5Ctheta_%7Bj%7D%7CY%5D%29.&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="E_{j}=-log(&#92;pi[&#92;theta_{j}|Y])." title="E_{j}=-log(&#92;pi[&#92;theta_{j}|Y])." class="latex" /><br />
So that the acceptance probability becomes<br />
<img src="http://s0.wp.com/latex.php?latex=%5Calpha%3Dmin%281%2Ce%5E%7B-%28B_%7Bj%7D-B_%7Bj%2B1%7D%29%28E_%7Bj%2B1%7D+-+E_%7Bj%7D%29++%7D%29.++&amp;bg=ffffff&amp;fg=000&amp;s=0" alt="&#92;alpha=min(1,e^{-(B_{j}-B_{j+1})(E_{j+1} - E_{j})  }).  " title="&#92;alpha=min(1,e^{-(B_{j}-B_{j+1})(E_{j+1} - E_{j})  }).  " class="latex" /><br />
It&#8217;s not necessary to define this energy, it only defines an equivalence mapping between statistics and physics. In physics particles get stuck in the local minima of the energy landscape and in statistics the MCMC gets stuck in the local peaks of the posterior. The reason for this is that in a canonical ensemble lower energy states are more probable (recall that nature tries to minimize the potential energy and that force is the negative gradient of the potential energy), so regions of the parameter space with low potential energy, physically, correspond to regions of high probability density, statistically. To be more precise, a result from statistical physics is that the distribution of energy is exponential with scale parameter kT, where k is Boltzmann&#8217;s constant and T is temperature (this condition holds only for a canonical ensemble). An exponential distribution with this scale parameter is called the Boltzmann distribution by physicists. As the temperature increases, higher energy states become more probable and the particle jumps out of the minima more. If you are a statistician you don&#8217;t need to worry about this, but sometimes this notation crops up in the literature. Its also the same acceptance probability now as in physics when sampling energies from a Boltzmann distribution. I have decided not to adopt the physics notation for this post.</p>
<p><a name="intra"></a><br />
<h2>Intra-Thread Metropolis Move</h2>
<p>Each thread, within itself, performs a normal vanilla metropolis move:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
//Propose Candidate Position//
			t1new=t1[rank*nmc+i-1] + normal(stream[rank]);
			t2new=t2[rank*nmc+i-1] + normal(stream[rank]);

			//Calculate log-Density at Newly-Proposed and Current Position//
			lpost_new[rank]=lLikelihood(t1new,t2new) + lprior(t1new,t2new);
			lpost[rank]=lLikelihood(t1[rank*nmc+i-1],t2[rank*nmc+i-1]) + lprior(t1[rank*nmc+i-1],t2[rank*nmc+i-1]);

			//Melt Density and Calculate log-Acceptance Probability//
			lalpha=B[rank]*(lpost_new[rank]-lpost[rank]);

			//Perform Metropolis Accept-Reject Step//
			if( log(u(stream[rank])) &lt; lalpha ){
				//Accept
				//Proposed as Current Position
				t1[rank*nmc+i]=t1new;
				t2[rank*nmc+i]=t2new;
			}else{
				//Do not Accept
				//Propogate Current Position
				t1[rank*nmc+i]=t1[rank*nmc+i-1];
				t2[rank*nmc+i]=t2[rank*nmc+i-1];
			}
</pre>
<p>A few comments about the variables. &#8220;nmc&#8221; is the number of mcmc draws I wish to generate. I have two parameters which I have denoted t1 and t2, because t is closest to theta. Moreover, each processor stores its <em>nmc</em> draws of t1 and t2 in a contiguous array in the memory of length nmc times number of threads. &#8220;Rank&#8221; Identifies the thread and &#8220;lpost&#8221; and &#8220;B&#8221; are arrays of length equal to the number of threads in which to store the log posterior density at the current position and the fractional melting power. All of these variables are defined at the top of the code.</p>
<p><a name="inter"></a><br />
<h2>Inter-Thread Metropolis-Hastings Move</h2>
<pre class="brush: cpp; title: ; notranslate" title="">

				if(u(stream[rank]) &lt; 0.5){
					rank_partner=rank+1;
					if(rank_partner &lt; size){
						//Inter-Thread Metropolis-Hastings Part
						lalpha = (B[rank]-B[rank_partner])*(lpost[rank_partner]-lpost[rank]);
						if(log(u(stream[rank])) &lt; lalpha){
							//accept swap
							swap(t1[rank*nmc+i],t1[rank_partner*nmc+i]);
							swap(t2[rank*nmc+i],t2[rank_partner*nmc+i]);
						}

					}
				}
</pre>
<p>The only additional thing to add is that each chain attempts a swap with its neighbour at each iteration with probability 1/2. There is nothing special about 1/2, you could choose what you like, but there are pros and cons. How this made parallel in OpenMP is shown below.</p>
<p><a name="openmp"></a><br />
<h2>OpenMP Parallelization</h2>
<p>The OpenMP parallel implementation of the above algorithm is very simple!</p>
<pre class="brush: plain; title: ; notranslate" title="">
#pragma omp parallel private(i,t1new,t2new,rank,lalpha,rank_partner) shared(B, lpost, lpost_new,t1,t2,swapt1,swapt2)
	{
		//Identify Each Thread
		rank=omp_get_thread_num();

		for (i = 1; i &lt; nmc; ++i)
		{

                 //***Intra-Thread Metropolis Part***//
	
#pragma omp barrier      //Synchronise Threads
#pragma omp critical     //Executed Critical Code Block Oney Thread at a Time. 
			{

                 //***Inter-Thread Parallel Tempering Part***//

			}
#pragma omp barrier   //Synchronise Threads
		}
	}
</pre>
<p>The first parallel pragma simply forks the master thread into a number of threads whereby each thread executes the following code block independently i.e. a number of independent parallel mcmcs. Specifying variables as private means that each thread gets a copy of that variable in its own seperate location in the memory. Shared is the opposite, although I think variables are shared by default. The barrier pragma means that each thread halts until all threads have reached this point. The critical pragma means the following code block is executed by one thread at a time only. This prevents thread j swapping with thread j+1 whilst thread j+1 is attempting a swap with thread j+2, nasty things such as race conditions can occur. The last pragma barrier waits for all threads to have reached the end and then the next iteration of the for loop proceeds.</p>
<p><a name="fullcode"></a><br />
<h2>Full code</h2>
<p>The full code can be found <a href="../../../../example_code/tempering.cpp">here</a>. It depends on <a href="../../openmp/index.html">OpenMP</a> and the <a href="../../../../programming/parallel-random-number-generation-trng/index.html" title="Parallel Random Number Generation using TRNG">TRNG</a> library in order to generate multiple independent streams of random numbers. It takes the number of mcmc draws as a command-line argument.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael tempering]$ wget http://www.lindonslog.com/example_code/tempering.cpp
[michael@michael tempering]$ g++ tempering.cpp -fopenmp -o tempering  -ltrng4 -lm
[michael@michael tempering]$ ./tempering 10000
Thread 0 has fractional power 1
Thread 1 has fractional power 0.469117
Thread 2 has fractional power 0.220071
Thread 3 has fractional power 0.103239
Thread 4 has fractional power 0.0484313
Thread 5 has fractional power 0.0227199
Thread 6 has fractional power 0.0106583
Thread 7 has fractional power 0.005
[michael@michael tempering]$
</pre>
<p><a name="simstudy"></a><br />
<h2>Simulation Study</h2>
<p>I chose the likelihood to be 5 sharply peaked normal distributions located at the corners of a sort-of unit square plus one at the origin with variances of 0.001. The prior was a normal of variance 1000 centered at the origin. The parallel tempering algorithm was run with 8 threads. The posterior draws and mixing results are below:<br />
<div id="attachment_632" style="width: 490px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/07/partempdraws.png"><img src="../../../../wp-content/uploads/2013/07/partempdraws.png" alt="Posterior Draws" width="480" height="480" class="size-full wp-image-632" srcset="http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws.png 480w, http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/07/partempdraws-300x300.png 300w" sizes="(max-width: 480px) 100vw, 480px" /></a><p class="wp-caption-text">Posterior Draws from Parallel Tempering</p></div><br />
<div id="attachment_630" style="width: 610px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/07/parallel_tempering.png"><img src="../../../../wp-content/uploads/2013/07/parallel_tempering.png" alt="parallel tempering mixing" width="600" height="900" class="size-full wp-image-630" srcset="http://www.lindonslog.com/wp-content/uploads/2013/07/parallel_tempering.png 600w, http://www.lindonslog.com/wp-content/uploads/2013/07/parallel_tempering-200x300.png 200w" sizes="(max-width: 600px) 100vw, 600px" /></a><p class="wp-caption-text">Mixing of parallel tempering algorithm</p></div></p>
<p><a name="futureuse"></a><br />
<h2>On the Future use of Parallel Tempering with OpenMP</h2>
<p>I hope the code exemplifies how easy it is to run parallel MCMC chains with OpenMP. I would argue that the metropolis moves are the hardest part. If you can write them for a single serial chain, then it is only a few extra steps to run parallel chains and imlement that parallel tempering algorithm. My laptop has four cores and my office computer has eight. Given the trajectory of technology that shared memory devices have an ever increasing number of cores, it seems to me that parallel tempering is becoming an ever-more valuable algorithm to improve mixing times of MCMC runs. Afterall, had I not used the extra 3 cores on my laptop, they would have remained idle. If you have extra cores, why not use them! Moreover with OpenMP you can spawn as many parallel MCMCs as you desire, avoiding the pitalls of MPI.</p>
<p><span class="Z3988" title="ctx_ver=Z39.88-2004&#038;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&#038;rft_id=info%3Adoi%2F10.1039%2Fb509983h&#038;rft.atitle=Parallel+tempering%3A+Theory%2C+applications%2C+and+new+perspectives&#038;rft.jtitle=Physical+Chemistry+Chemical+Physics&#038;rft.artnum=http%3A%2F%2Fxlink.rsc.org%2F%3FDOI%3Db509983h&#038;rft.volume=7&#038;rft.issue=23&#038;rft.issn=1463-9076&#038;rft.spage=3910&#038;rft.date=2005&#038;rfr_id=info%3Asid%2Fscienceseeker.org&#038;rft.au=Earl+David+J.&#038;rft.aulast=Earl&#038;rft.aufirst=David+J.&#038;rft.au=Deem+Michael+W.&#038;rft.aulast=Deem&#038;rft.aufirst=Michael+W.&#038;rfs_dat=ss.included=1&#038;rfe_dat=bpr3.included=1;bpr3.tags=Chemistry%2CComputer+Science+%2F+Engineering%2CMathematics%2CPhysics">Earl D.J. &#038; Deem M.W. (2005). Parallel tempering: Theory, applications, and new perspectives, <span style="font-style:italic;">Physical Chemistry Chemical Physics, 7</span> (23) 3910. DOI: <a rel="author" href="http://dx.doi.org/10.1039%2Fb509983h">10.1039/b509983h</a></span></p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-605 -->

				
					
<article id="post-562" class="grid_6 post-562 post type-post status-publish format-standard hentry category-programming category-r">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1269 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/parallel-random-number-generation-trng/index.html#comments"><span class="dsq-postid" data-dsqidentifier="562 http://www.lindonslog.com/?p=562">4 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/parallel-random-number-generation-trng/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Parallel Random Number Generation using TRNG</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>To my surprise and disappointment, popular scientific libraries like <a href="http://www.boost.org/doc/libs/1_54_0/doc/html/boost_random.html" title="Random Number Libraries">Boost</a> or <a href="http://www.gnu.org/software/gsl/" title="GNU Scientific Library">GSL</a> provide no native support for parallel random number generation. Recently I came across TRNG, an excellent random number generation library for C++ built specifically with parallel architectures in mind. Over the last few days I&#8217;ve been trawling internet forums and reading discussions about the best parallel random number generation libraries. Given the trend in CPU architectures whereby each contains an ever increasing number of cores, it makes sense to start a project by considering what libraries are available to best make use of this technology. The first libraries I came across were <a href="http://statmath.wu.ac.at/software/RngStreams/doc/rngstreams.html" rel="nofollow">RngStream</a> and <a href="http://www.sprng.org/" title="SPRNG" rel="nofollow">SPRNG</a>. It seems SPRNG was built specifically with MPI, i.e. for distributed memory architectures, in mind and there are some excellent examples and resources of how to get started with <a href="http://darrenjw.wordpress.com/2010/12/14/getting-started-with-parallel-mcmc/" rel="nofollow">parallel MCMC on Darren Wilkinson&#8217;s blog</a>. As a result, it seems a bit contrived to get SPRNG to work with OpenMP, i.e. for shared memory architectures. Moreover, I specifically wanted to use OpenMP because I wanted to write an extension for R for use on personal computers. RngStream is written by the man himself, Pierre L&#8217;Ecuyer, and is much more OpenMP amenable. Both of these, however, only generate uniform pseudo random numbers. This isn&#8217;t a fault, but it means you need to code up transformations and samplers to generate non-uniform pseudo random numbers. While this would be a good exercise, life is short, and I&#8217;d rather leave this sort of thing to the professionals (I don&#8217;t want to code up my own Ziggurat algorithm). Also, the generators or engines are of defined types and I found it hard to convert them into the corresponding types of other libraries like Boost or GSL so that I could use their non-uniform generation code. That probably says more about my ability rather than the actual difficulty of the problem and Darren Wilkinson provides some of his own code for getting the RNG engine of SPRNG into the correct datatype to be compatible with GSL. </p>
<h2>TRNG</h2>
<p>At this point I was quite discouraged but then I came across <a href="http://numbercrunch.de/trng/" title="Tina's Random Number Generator" rel="nofollow">TRNG</a>, written by Heiko Bauke. At first glance TRNG is an excellently documented C++ PRNG (which stands for pseudo random number generator, not parallel, that would be PPRNG) library built specifically with parallel architectures in mind. Not only does it provide non-uniform distributions, but it can be used easily with MPI, OpenMP, CUDA and TBB, for which many examples are supplied. The documentation is excellent and the many examples of the same problem coded with each of the aforementioned parallelization methods are enlightening. If that weren&#8217;t enough, TRNG can be used in combination and interchangeably with the Boost random as well as the C++11 TR1 random libraries, that is, the engines/generators from TRNG can be used with the distribution functions of Boost and C++11 TR1, which was a problem I encountered with RngStream and SPRNG. The way TRNG and RngStream work are slightly different. Whereas RngStream generates multiple independent streams, TRNG uses a single stream and either divides it into blocks, or interleaves it between different processors by a leap-frog type scheme, much like dealing out cards round a table. The point of all this is that the streams of different processors never overlap, otherwise one would get the same draws on processor A as processor B. While purists might argue that  L&#8217;Ecuyer&#8217;s method is more rigorous, I&#8217;m happy enough with the way Heiko has done it, especially given TRNG&#8217;s out-of-box easy of use and compatibility.</p>
<h3>Installation of TRNG</h3>
<p>Clone the repository off Github.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael$git clone https://github.com/rabauke/trng4
[michael@michael$cd trng4/
[michael@michael trng4]$./configure --prefix=/usr
[michael@michael trng4]$make
[michael@michael trng4]$make inst
[michael@michael trng4]$ sudo bash
[sudo] password for michael: 
[root@michael trng4]# ldconfig
[root@michael trng4]#
</pre>
<p>the &#8220;&#8211;prefix=&#8221; argument just sets where I want the files to be installed and is not necessary. If omitted the default case is /usr/local. After make install, run ldconfig as root in order to update the dynamic linker/loader with the presence of the new library. </p>
<p>Basically there exists a cache /etc/ld.so.cache which is used by the dynamic linker/loader at run-time as a cross-reference for a library&#8217;s soname with its full file path. ldconfig is normally run during booting but can also be run anytime to update the cache with the locations of new libraries. Here is what happens if you don&#8217;t run ldconfig, as I did the first time.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael ~]$ g++ hello_world.cc -L /usr/lib -ltrng4
[michael@michael ~]$ ./a.out
./a.out: error while loading shared libraries: libtrng4.so.0: cannot open shared object file: No such file or directory
</pre>
<p>It compiled fine, but at run-time the loader couldn&#8217;t find the library.</p>
<h2>Parallel Random Number Generation in C++</h2>
<p><strong>Nachtrag:</strong> I think instead of using trng::yarn2 gen[max] it is better to do:</p>
<pre class="brush: cpp; title: ; notranslate" title="">
trng::yarn2 * gen;
gen=new trng::yarn2[max];
</pre>
<p>The approach will be to generate the the PRNGs in C++ and call it from R using <a href="../../../../programming/r/rcpp/index.html" title="Calling C++ from R using Rcpp">Rcpp</a>. First lets consider the C++ code to generate some random uniforms.</p>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;omp.h&gt;
#include &lt;trng/yarn2.hpp&gt;
#include &lt;trng/uniform01_dist.hpp&gt;

int main() {

	int max=omp_get_max_threads();
	omp_set_num_threads(max);
	int rank;
	trng::yarn2 gen[max];
	trng::uniform01_dist&lt;&gt; u;
	std::cout &lt;&lt; max &lt;&lt; &quot; =max num of threads&quot; &lt;&lt; std::endl;


	for (int i = 0; i &lt; max; i++)
	{
		gen[i].split(max,i);
	}


#pragma omp parallel for private(rank)
	for (int i = 0; i &lt; max; ++i)
	{
		rank=omp_get_thread_num();
#pragma omp critical
	std::cout &lt;&lt; u(gen[rank]) &lt;&lt; &quot; from thread &quot; &lt;&lt; rank &lt;&lt;  std::endl;
	}
	return EXIT_SUCCESS;
}

</pre>
<p>which returns</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael ~]$ g++ omprng.cpp -o omprng -fopenmp -ltrng4
[michael@michael ~]$ ./omprng 
4 =max num of threads
0.919233 from thread 0
0.408994 from thread 1
0.943502 from thread 2
0.401236 from thread 3
[michael@michael ~]$ 
</pre>
<p>The salient feature of this code is the leapfrog process by calling split. There exists a sequence of random uniforms and &#8220;.split(max,i)&#8221; divides it into <em>max</em> subsequences, leap-frogging each other, and grab the <em>i&#8217;th</em> subsequence. You can think of this as <em>max</em> players sitting around a poker table and the .split() as continuously dealing out random uniforms to each of the players. The code says let processor i be &#8220;player&#8221; i and use the sequence of random uniforms dealt to it.</p>
<h2>Parallel Random Number Generation in R using Rcpp</h2>
<p>Thanks to Rcpp the above C++ code can be trivially changed so that it can be used from R. Just include the Rcpp header and change the function return type.</p>
<pre class="brush: bash; title: ; notranslate" title="">
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;omp.h&gt;
#include &lt;trng/yarn2.hpp&gt;
#include &lt;trng/uniform01_dist.hpp&gt;
#include &lt;Rcpp.h&gt;

// [[Rcpp::export]]
Rcpp::NumericVector prunif(int n) {

	int max=omp_get_max_threads();
	omp_set_num_threads(max);
	int rank;
	trng::yarn2 gen[max];
	trng::uniform01_dist&lt;&gt; u;
	Rcpp::NumericVector draws(n);

	for (int i = 0; i &lt; max; i++)
	{
		gen[i].split(max,i);
	}


#pragma omp parallel for private(rank)
	for (int i = 0; i &lt; n; ++i)
	{
		rank=omp_get_thread_num();
		draws[i]=u(gen[rank]);
	}

	return draws;
}
</pre>
<p>This code can be compiled and loaded into R on the fly, so  lets test it.</p>
<h3>Speedup Performance</h3>
<pre class="brush: r; title: ; notranslate" title="">
&gt; library(Rcpp)
&gt; library(rbenchmark) 
&gt; Sys.setenv(&quot;PKG_CXXFLAGS&quot;=&quot;-fopenmp&quot;)
&gt; Sys.setenv(&quot;PKG_LIBS&quot;=&quot;-ltrng4&quot;)
&gt; sourceCpp(&quot;prunif.cpp&quot;)
&gt; benchmark(replications=rep(100,0,1),runif(1000000),prunif(1000000))
           test replications elapsed relative user.self sys.self user.child
2 prunif(1e+06)          100   0.611     1.00     2.227    0.114          0
1  runif(1e+06)          100   3.837     6.28     3.745    0.086          0
</pre>
<p><div id="attachment_583" style="width: 490px" class="wp-caption aligncenter"><a href="../../../../wp-content/uploads/2013/07/speedup_prunif.png"><img src="../../../../wp-content/uploads/2013/07/speedup_prunif.png" alt="Speedup" width="480" height="480" class="size-full wp-image-583" srcset="http://www.lindonslog.com/wp-content/uploads/2013/07/speedup_prunif.png 480w, http://www.lindonslog.com/wp-content/uploads/2013/07/speedup_prunif-150x150.png 150w, http://www.lindonslog.com/wp-content/uploads/2013/07/speedup_prunif-300x300.png 300w" sizes="(max-width: 480px) 100vw, 480px" /></a><p class="wp-caption-text">Parallel RNG speedup</p></div><br />
There are a few things to note. Spawning threads incurs its own overhead, so it will obviously be slower for very few draws. As the number of draws becomes larger the time taken to spawn new threads is dwarfed by the time taken to create the draws and so it is worthwhile to do it in parallel. One caveat is that prunif and runif did not in this case use the same generating algorithm. R&#8217;s algorithm can be changed with RNG.kind and the TRNG algorithm can be changed by using an alternative to yarn in &#8220;trng::yarn2&#8221;. Even if they were the same though I would expect the same qualitative behaviour.</p>
<h2>Discussion</h2>
<p>Generating large samples of random numbers in one hit quickly is not the reason why I started looking for a good parallel random number generator. Rarely is it important to me to generate large amount of draws in one go but it certainly is important to me to have independent streams. Generally I will port expensive parts of my R code, usually for loops, to C++ and inevitably I will somewhere within these for loops or other expensive parts of code need to draw some random numbers. Since these expensive pieces of code are self-evidently <em>expensive</em>, I will want to compute them in parallel in C++ if I can and so it is very important to me to have independent streams from which to draw random numbers.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-562 -->

				
					
<article id="post-539" class="grid_6 post-539 post type-post status-publish format-standard hentry category-programming">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1282 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/atlas-blas-lapack-linear-algebra-libraries/index.html#comments"><span class="dsq-postid" data-dsqidentifier="539 http://www.lindonslog.com/?p=539">4 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/atlas-blas-lapack-linear-algebra-libraries/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Getting started with ATLAS, BLAS and LAPACK</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>I decided to experiment with Atlas (Automatically Tuned Linear Algebra Software) because it contains a parallel BLAS library. For those that don&#8217;t have access to the Intel math kernel library Atlas is a good choice for obtaining an automatically optimized BLAS library. It also provides a nice C interface to BLAS, hence there are two ways to go about using the library. On my Fedora 18 the Atlas build is found under &#8220;/usr/lib64/atlas/&#8221;. I&#8217;ll start with a short example of how to scale a vector.</p>
<h2>Without cblas.h</h2>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;stdio.h&gt;

int
main ()
{
  int i, j, n, one;
  double coefficient;
  double x[] = { 1, 1, 1 };

  coefficient = 4.323;
  one = 1;
  n = 3;

  dscal_(&amp;n, &amp;coefficient, &amp;x[0], &amp;one);

  for (i = 0; i &lt; 3; ++i)
    printf (&quot;%f\n&quot;, x[i]);

  return 0;
}
</pre>
<h3>Calling Fortran from C</h3>
<p>The Fortran compiler appends an underscore after subroutinenames (subroutine is Fortran-speak for &#8220;function&#8221;). So bearing in mind that BLAS is written in Fortran, to call the subroutine &#8220;dscal()&#8221; from C one must write &#8220;dscal_()&#8221;. Similarly if a C function is named &#8220;myfunc_(&#8230;)&#8221;, it may be called from Fortran using &#8220;myfunc(&#8230;)&#8217;. In addition, C passes arguments by value whereas Fortran passes arguments by reference, so to call a Fortran subroutine from C one must pass the memory address of the arguments by using the ampersand and not something like &#8220;dscal_(3, 4.323, &#038;x[0], 1);&#8221;.<br />
To compile we must link with the atlas libraries.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael lin]$ gcc example.c -o example -L /usr/lib64/atlas -lf77blas 
[michael@michael lin]$ ./testblas 
4.323000
4.323000
4.323000
</pre>
<h2>With cblas.h</h2>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;stdio.h&gt;
#include &lt;cblas.h&gt;

int
main ()
{
  int i;
  double x[] = { 1, 1, 1 };


  cblas_dscal(3, 4.323, x, 1);

  for (i = 0; i &lt; 3; ++i)
    printf (&quot;%f\n&quot;, x[i]);

  return 0;
}
</pre>
<p>The main difference here is that instead of an &#8220;_&#8221; suffix there is a &#8220;cblas_&#8221; prefix, the cblas header has been included and that the arguments are passed by value than by reference. To compile this we link against the cblas library instead of the blas library.</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael lin]$ gcc testblasc.c -o testblas -L /usr/lib64/atlas -lcblas -latlas
[michael@michael lin]$ ./testblas 
4.323000
4.323000
4.323000
</pre>
<h2>Compiling with C++</h2>
<p>There are further modifications to the code if you want to write in C++ rather than C. The first is instead of &#8220;#include <cblas.h>&#8221; one must write</p>
<pre class="brush: cpp; title: ; notranslate" title="">
extern &quot;C&quot;
{
	   #include &lt;cblas.h&gt;
}
</pre>
<p>I&#8217;ve now rewritten the above example using the vectors library and making use of cpp&#8217;s input/output.</p>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;cstdlib&gt;
#include &lt;vector&gt;
#include &lt;iostream&gt;

extern &quot;C&quot;
{
	   #include &lt;cblas.h&gt;
}

using namespace std;
int main ()
{
  int i;
  vector&lt;double&gt; x(3,1);

  cblas_dscal (x.size(), 4.323, &amp;x[0], 1);

  for (i = 0; i &lt; x.size(); ++i)
	cout &lt;&lt; x[i] &lt;&lt; endl;
  return 0;
}
</pre>
<p>The advantages of the vector class instead of a C array is that now one doesn&#8217;t need an extra variable to store the length of the array. Link against -lcblas this time than -lf77blas. All atlas libraries can be found <a href="http://math-atlas.sourceforge.net/errata.html#LINK" title="atlas libraries" rel="nofollow">here</a>.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-539 -->

				
					
<article id="post-527" class="grid_6 post-527 post type-post status-publish format-standard hentry category-linux-unix category-r">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1283 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../linux-unix/r-snippets-vim-snipmate/index.html#respond"><span class="dsq-postid" data-dsqidentifier="527 http://www.lindonslog.com/?p=527">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../linux-unix/r-snippets-vim-snipmate/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">R snippets for vim-SnipMate</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>Vim is my editor of choice, reasonable so, whether it be for coding C++, LaTeX or even R. I&#8217;ve used RStudio, which even has a Vim-Mode, but I still prefer to use Vim. Vim has it&#8217;s own R plugin, namely Vim-R-plugin, but this post is about snippets. SnipMate is an awesome auto-completion plugin for Vim which is fully customizable. One simply writes a string, rnorm for example, and presses tab to autocomplete the code to rnorm(n=,mean=,sd=), where repeated press of tab cycles through the placeholders at the function parameters. The strings to recognize, referred to as snippets, are stored in a snippets file &#8220;languagetype.snippets&#8221; along with the corresponding code to auto-complete. These can be user defined for any language, not just R. It&#8217;s usually not necessary to write these snippets files yourself, as there are already existing snippets files within the vim community, including one for R.  <a href="https://github.com/honza/vim-snippets/tree/master/snippets" rel="nofollow" title="Vim R snippets">Here</a> is a github repository containing snippets files for a great many languages, including R. Simply put this into your vim-SnipMate snippets directory. The last thing to do is to tell Vim to recognize an r filetype. If you open an R file and type,</p>
<pre class="brush: bash; title: ; notranslate" title="">
:set ft=r
</pre>
<p>then this will tell Vim that the file is an R file. Obviously you don&#8217;t want to do this all the time, so to get Vim to automatically recognize &#8220;.r&#8221; and &#8220;.R&#8221; extensions as R files simply append your .vimrc file with:</p>
<pre class="brush: bash; title: ; notranslate" title="">
au BufNewFile,BufRead *.r set filetype=r
au BufNewFile,BufRead *.R set filetype=r
</pre>
<h2>Writing Custom Snippets</h2>
<p>This is an example of how to complete a for loop having written only &#8220;for&#8221;. Append the r.snippets file with the following code.</p>
<pre class="brush: bash; title: ; notranslate" title="">
snippet for
    for(${1:i} in ${2:begin}:${3:end}){
    ${4}
    }${5}
</pre>
<p>This defines &#8220;for&#8221; as the snippet, the string after which we will want to press tab. The dollar signs define the place holders. &#8220;${x:text}&#8221; defines the x&#8217;th placeholder and the highlighted text which will be replaced.</p>
<pre class="brush: r; title: ; notranslate" title="">
for&lt;tab&gt;
</pre>
<p>becomes</p>
<pre class="brush: r; title: ; notranslate" title="">
for(i in begin:end){

}
</pre>
<p>.</p>
<p>Repeated pressing of tab cycles between i, begin, end, body of foor loop and then breaks out of the for loop.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-527 -->

				
					
<article id="post-463" class="grid_6 post-463 post type-post status-publish format-standard hentry category-r">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1287 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/r/rcpp/index.html#comments"><span class="dsq-postid" data-dsqidentifier="463 http://www.lindonslog.com/?p=463">7 Comments</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/r/rcpp/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Calling C++ from R using Rcpp</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<h2>Why call C/C++ from R?</h2>
<p>I really like programming in R. The fact that it is open source immediately wins my favour over Matlab. It can, however, be quite slow especially if you &#8220;speak&#8221; R with a strong C/C++ accent. This sluggishness, especially when writing unavoidable for loops, has led me to consider other programming languages. I have come to the conclusion that there is no all-round best programming language but a combination of R and C++ is very hard to beat. My resolution is to write the expensive codes in C++ and to call them as functions within R so that I still have the nice dynamic interactivity of an R session with the speed and optimization of C++. There are 3 ways you can get started implementing C code in R, namely, &#8220;.C,&#8221;, &#8220;.Call&#8221; and Rcpp. It is generally not recommended to use &#8220;.C&#8221; but to use &#8220;.Call&#8221; instead. &#8220;.Call&#8221;, however, requires quite a bit of boilerplate code and can be tiresome and obfuscating to write. &#8220;Rcpp&#8221;, while not intrinsically part of R itself, is a package written by<a href="http://dirk.eddelbuettel.com/" rel="nofollow"> D Eddelbuettel</a> and R François which relies on &#8220;.Call&#8221; but allows you to write neater more efficient code by providing an interface between &#8220;.Call&#8221; and the programmer. There are many things to consider when writing C++ code for R but most likely you will want to get coding as fast as possible and worry about these other tangential matters later.</p>
<h2>How to call C/C++ from R using Rcpp [The Hard/Conscientious Way]</h2>
<p><em>Note: This is the longest and hardest way to compile C++ code for R, but it is arguably the most flexible for the conscientious user who requires complex code and desires to know all the details of whats going on. For a simpler no-nonsense approach scroll down to the &#8220;sourceCpp&#8221; command.</em><br />
Lets jump right in with an example</p>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;Rcpp.h&gt;
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;


using namespace std;
RcppExport SEXP comp(SEXP x, SEXP y){
        int i,n;
        Rcpp::NumericVector vector1(x);
        Rcpp::NumericVector vector2(y);
        n=vector2.size();
        Rcpp::NumericVector product(n);
for(i=0;i&lt;n;i++){
product[i]=vector1[i]*vector2[i];
}
return(product);
}
</pre>
<p><em>Note: You do not need R.h and Rdefines.h when you use Rcpp.h.</em><br />
<em>Note: This is not the only way to compile and call code for R. See bottom for a neater alternative.</em><br />
The function has two R objects as arguments, which are datatype &#8220;SEXP&#8221;, and returns an R object. The &#8220;SEXP&#8221; R objects, which are technically pointers to structures with typedef SEXPREC, are quite complex and so must be coerced into a vector using Rcpp::NumericVector, which makes a vector out of the SEXP object. Similarly, Rcpp::NumeriVector can be used to create a vector of length n. The rest is standard C++ code. Before compiling some compiler flags must be set:</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael rcpp]$ export PKG_CXXFLAGS=`Rscript -e &quot;Rcpp:::CxxFlags()&quot;`
[michael@michael rcpp]$ export PKG_LIBS=`Rscript -e &quot;Rcpp:::LdFlags()&quot;`
</pre>
<p>Compile with:</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael rcpp]$ R CMD SHLIB comp.cpp
</pre>
<p>The function is now ready to be used in R. Consider the following R code:</p>
<pre class="brush: r; title: ; notranslate" title="">
library(Rcpp)
dyn.load('comp.so')
x=rnorm(100,0,1)
y=rnorm(100,0,1)
.Call('comp',x,y)
</pre>
<p>First the Rcpp library is loaded. Then, C++ code is loaded with &#8220;dyn.load&#8221;, after which it is ready to be used in R. The &#8220;.Call&#8221; function calls the &#8220;comp&#8221; function and the two arguments are supplied after. Now lets consider a more interesting example by parallelizing the C++ with openmp.</p>
<h2>Performance with OpenMP</h2>
<pre class="brush: cpp; title: ; notranslate" title="">
#include &lt;Rcpp.h&gt;
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;omp.h&gt;

using namespace std;
RcppExport SEXP parad(SEXP x, SEXP y){
	int i,n,max;
	Rcpp::NumericVector vector1(x);
	Rcpp::NumericVector vector2(y);
	n=vector2.size();
	Rcpp::NumericVector product(n);
	max=omp_get_max_threads();
	omp_set_num_threads(max);

#pragma omp parallel for
for(i=0;i&lt;n;i++){
product[i]=vector1[i]/vector2[i];
}
return(product);
}
</pre>
<p>The purpose of this code is rather trivial, just dividing each element of x by the corresponding element of y. Now the compiler must be told to link against the openmp libraries so to compile we write</p>
<pre class="brush: bash; title: ; notranslate" title="">
[michael@michael rcpp]$ export PKG_LIBS='`Rscript -e &quot;Rcpp:::LdFlags()&quot;` -fopenmp -lgomp'
[michael@michael rcpp]$ export PKG_CXXFLAGS='`Rscript -e &quot;Rcpp:::CxxFlags()&quot;` -fopenmp'
[michael@michael rcpp]$ R CMD SHLIB parad.cpp
</pre>
<p>How does this fair against the standard R &#8220;x/y&#8221;? To compare the &#8220;rbenchmark&#8221; package will be used.</p>
<pre class="brush: r; title: ; notranslate" title="">
library(Rcpp)
library(rbenchmark)
dyn.load('unpar.so')
dyn.load('parad.so')

x=rnorm(10000000,0,1)
y=rnorm(10000000,0,1)
benchmark(replications=rep(1,0,1),.Call('unpar',x,y),.Call('parad',x,y),x/y)
</pre>
<pre class="brush: r; title: ; notranslate" title="">
&gt; benchmark(replications=rep(1,0,1),.Call('unpar',x,y),.Call('parad',x,y),x/y)
                  test replications elapsed relative user.self sys.self
2   .Call(&quot;parad&quot;, x, y)            1   0.036    1.000     0.112    0.002
1 .Call(&quot;unpar&quot;, x, y)            1   0.082    2.278     0.081    0.000
3                  x/y            1   0.053    1.472     0.048    0.005
</pre>
<p>The parallel version of the x-divided-by-y code is even faster than R&#8217;s native &#8220;x/y&#8221;.</p>
<h2>Converting between C++ and R datatypes using &#8220;as&#8221; and &#8220;wrap&#8221;</h2>
<p>If you feel more comfortable seeing familiar C/C++ variable types, one is free to convert all of the function arguments of SEXP type to the desired C++ variable types for use in the body of the code and then to convert them back to SEXP at the end before returning the function. Say, for example, that I have a vector of 100 standard normal draws as one might obtain from x=rnorm(100,0,1). Passing x as an argument to the C++ function and the variable type of x is SEXP. I can then convert it to a vector of doubles using the <strong>&#8220;as&#8221;</strong> command, perform any processing with it in the body of the function and then convert it back to SEXP at the end using the <strong>&#8220;wrap&#8221;</strong> command. Here is a short example:</p>
<pre class="brush: r; title: ; notranslate" title="">
#include &lt;Rcpp.h&gt;
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;vector&gt;

using namespace std;
RcppExport SEXP conv(SEXP x, SEXP y){
	int i,n;
	vector&lt;double&gt; vector1=Rcpp::as&lt;vector&lt;double&gt; &gt;(x);
	vector&lt;double&gt; vector2=Rcpp::as&lt;vector&lt;double&gt; &gt;(y);
n=vector1.size();
vector&lt;double&gt; product(n);

for(i=0;i&lt;n;i++){
product[i]=vector1[i]/vector2[i];
}
return( Rcpp::wrap(product) );
}
</pre>
<p>The x and y SEXP&#8217;s are converted to vector doubles and then converted back at the end. I like this approach because my function remains looking like C++.</p>
<h2>Streamlined sourceCpp() and cppFunction()</h2>
<p>It is not necessary to compile, link and load the C++ code as above. There exist some wrappers for all of these tasks such as <strong>include, sourceCpp and cppFunction</strong>. I like sourceCpp() the best. Consider the C++ code stored in parad.cpp shown below:</p>
<pre class="brush: r; title: ; notranslate" title="">
#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include &lt;Rcpp.h&gt;
#include &lt;omp.h&gt;

using namespace std;
// [[Rcpp::export]]
Rcpp::NumericVector parad(Rcpp::NumericVector x, Rcpp::NumericVector y){
	int i,n,max;
	n=x.size();
	Rcpp::NumericVector product(n);
	max=omp_get_max_threads();
	omp_set_num_threads(max);

#pragma omp parallel for
for(i=0;i&lt;n;i++){
product[i]=x[i]/y[i];
}
return(product);
}
</pre>
<p>This is a neat little parallel C++ function. This can be compiled, linked and loaded in one step using the sourceCpp() function. All we need to do is set some terminal environment variables for the compiler to use openMP.</p>
<pre class="brush: bash; title: ; notranslate" title="">
library(Rcpp)
Sys.setenv(&quot;PKG_CXXFLAGS&quot;=&quot;-fopenmp&quot;)
Sys.setenv(&quot;PKG_LIBS&quot;=&quot;-fopenmp&quot;)
sourceCpp(&quot;parad.cpp&quot;)
a=rnorm(1000,0,1)
b=rnorm(1000,0,1)
c=parad(a,b)
</pre>
<p>Lines 2 and 3 are simply setting terminal environment variables from within R so that the compiler compiles as &#8220;g++ &#8230;. -fopenmp &#8230;&#8221; which we need for the &#8220;omp.h&#8221; header. sourceCpp is a wrapper that takes care of everything. After that one can immediately start using the parad() function.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-463 -->

				
					
<article id="post-52" class="grid_6 post-52 post type-post status-publish format-standard hentry category-programming">

<div class="format-box">
<i class="icon-file"></i>
</div>

<div class="post-cover">

	<div class="entry-meta">
		<span class="clock"> <i class="icon-time"></i> 1441 days ago</span> 
		<span class="comments-link"> <i class="icon-comment"></i>  <a href="../../../../programming/struct-stat-and-stat/index.html#respond"><span class="dsq-postid" data-dsqidentifier="52 http://www.lindonslog.com/?p=52">Leave a comment</span></a></span>
		<span class="perml"> <i class="icon-bolt"></i> <a href="../../../../programming/struct-stat-and-stat/index.html" rel="bookmark">Permalink</a></span>
	</div><!-- #entry-meta -->
	
	<header class="entry-header">
		<h2 class="entry-title">Struct stat and stat()</h2>
	</header><!-- .entry-header -->

	<div class="entry-content">
								
								
				<p>I came across this useful bit of code when wanting to read in some data from a text file. Suppose you do not know in advance how much data there is in a text file, how long should you make your buffer array to read the data into? The problem is you don&#8217;t know. You don&#8217;t want to make your buffer array neither too short and miss data, nor needlessly large.</p>
<p>You can use the stat() function to populate a structure containing information about a file, including its size, which you can then use to dynamically allocate memory. Here&#8217;s some example code:</p>
<pre>
#include <stdio.h>
#include <stdlib.h>
#include <sys/stat.h>
#include <sys/types.h>


int main(void){
    FILE *fp;
    char *buffer;
    struct stat statistics;
    fp = fopen("test.txt", "rb");
    stat("test.txt", &statistics);
    buffer = (char *) malloc(statistics.st_size);
    fread(buffer, 1, statistics.st_size, fp);
    fclose(fp);  
}
</pre>
<p>First, you want to include the last two header files. Next you declare the struct stat structure where you want to store all the data. The stat() function takes two arguements. Number 1 being the file in question, number 2 the memory address of the structure where you want to store the information. So stat() will populate your structure. You can then call one of the members of this strucutre, statistics.st_size (which is of type off_t) to see how big in bytes the file is. Now a char is ALWAYS 1 byte, the standard says so, so you then allocate this number of bytes(==characters) for your array. fread() reads statistics.st_size objects, each of size 1, from fp and stores them in buffer. It advances the file position indicator by the number of bytes read.</p>
					</div><!-- .entry-content -->

</div>	

</article><!-- #post-52 -->

				
				<div class="clear"></div>
				<div class='pagination cf'><a href='../../index.html' class='inactive' >1</a><span class='current'>2</span><a href='../3/index.html' class='inactive' >3</a></div>

			
			</div><!-- #content .site-content -->
		</section><!-- #primary .content-area -->


	</div><!-- #main .site-main -->

<div id="bottom">
<div class="container_6 cf">

<li class="botwid grid_2 widget_categories"><h3 class="bothead">Categories</h3>		<ul>
	<li class="cat-item cat-item-3"><a href="../../../linux-unix/index.html" >Linux/Unix</a> (16)
</li>
	<li class="cat-item cat-item-5"><a href="../../../mathematics/index.html" >Mathematics</a> (22)
<ul class='children'>
	<li class="cat-item cat-item-10"><a href="../../../mathematics/linear-algebra/index.html" >Linear Algebra</a> (5)
</li>
	<li class="cat-item cat-item-7"><a href="../../../mathematics/statistics/index.html" >Statistics</a> (17)
</li>
</ul>
</li>
	<li class="cat-item cat-item-6 current-cat"><a href="../../index.html" >Programming</a> (24)
<ul class='children'>
	<li class="cat-item cat-item-48"><a href="../../clojure/index.html" >clojure</a> (1)
</li>
	<li class="cat-item cat-item-51"><a href="../../functional-programming/index.html" >functional programming</a> (1)
</li>
	<li class="cat-item cat-item-50"><a href="../../haskell-programming/index.html" >haskell</a> (1)
</li>
	<li class="cat-item cat-item-40"><a href="../../julia/index.html" >julia</a> (2)
</li>
	<li class="cat-item cat-item-4"><a href="../../openmp/index.html" >OpenMP</a> (6)
</li>
	<li class="cat-item cat-item-11"><a href="../../r/index.html" >R</a> (12)
</li>
	<li class="cat-item cat-item-43"><a href="../../scala/index.html" >scala</a> (1)
</li>
</ul>
</li>
		</ul>
</li>		<li class="botwid grid_2 widget_recent_entries">		<h3 class="bothead">Recent Posts</h3>		<ul>
					<li>
				<a href="../../../../programming/partial-application-functions-julia/index.html">Partial Application for Functions in Julia</a>
						</li>
					<li>
				<a href="../../../../programming/newtons-iteration-scala-clojure-haskell/index.html">Newtons Iteration in Scala, Clojure and Haskell Comparison</a>
						</li>
					<li>
				<a href="../../../../mathematics/statistics/mala-metropolis-adjusted-langevin-algorithm-julia/index.html">MALA &#8211; Metropolis Adjusted Langevin Algorithm in Julia</a>
						</li>
					<li>
				<a href="../../../../programming/passing-julia-type-to-c-function-as-struct/index.html">Passing Julia Type to C Function as Struct</a>
						</li>
					<li>
				<a href="../../../../linux-unix/send-lines-code-vim-r-julia-python-repl-slime/index.html">Send Lines of Code from Vim to R/Julia/Python REPL</a>
						</li>
					<li>
				<a href="../../../../linux-unix/c-merge-sort-algorithm/index.html">C++ Merge Sort Algorithm</a>
						</li>
					<li>
				<a href="../../../../programming/generate-random-inverse-gaussian-r/index.html">Generate Random Inverse Gaussian in R</a>
						</li>
					<li>
				<a href="../../../../mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/index.html">Generalized Double Pareto Priors for Regression</a>
						</li>
					<li>
				<a href="../../../../mathematics/em-algorithm-bayesian-lasso-r-cpp-code/index.html">EM Algorithm for Bayesian Lasso R Cpp Code</a>
						</li>
				</ul>
		</li>			<div class="squarebanner grid_2 cf">
	<h3 class="bothead"> Sponsors </h3>
<ul><li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>			

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li>

<li>
<a rel="nofollow" href="index.html" title=""><img src="index.html" alt="" style="vertical-align:bottom;" /></a>
</li></ul>
</div></div>
</div>


	<footer id="colophon" class="site-footer" role="contentinfo">
	<div class="container_6">
	<div class="site-info">
			<div class="fcred">
			Copyright &copy; 2016 <a href="../../../../index.html" title="Lindons Log">Lindons Log</a> - .<br />
 | <a href="http://fabthemes.com/Winter/" >Winter WordPress Theme</a>
			</div>		
		</div><!-- .site-info -->	
		</footer><!-- #colophon .site-footer -->
	
</div><!-- #page .hfeed .site -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40536457-1', 'lindonslog.com');
  ga('send', 'pageview');

</script><!-- MathJax Latex Plugin installed: Disabled as no shortcodes on this page -->	<div style="display:none">
	</div>
<script type="text/javascript">var elLogo = document.getElementById("ft_logo"); if (elLogo) {elLogo.style.maxHeight = elLogo.getAttribute("relHeight") ? elLogo.getAttribute("relHeight") + "px" : "100px";} if (elLogo) {elLogo.style.maxWidth = elLogo.getAttribute("relWidth") ? elLogo.getAttribute("relWidth") + "px" : "100px";}</script><script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shCore.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/third-party-brushes/shBrushR.js%3Fver=20100919'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushBash.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushPlain.js%3Fver=3.0.9b'></script>
<script type='text/javascript' src='../../../../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushCpp.js%3Fver=3.0.9b'></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "http://www.lindonslog.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.getElementsByTagName("head")[0].insertBefore( corecss, document.getElementById("syntaxhighlighteranchor") );
		var themecssurl = "http://www.lindonslog.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		//document.getElementById("syntaxhighlighteranchor").appendChild(themecss);
		document.getElementsByTagName("head")[0].insertBefore( themecss, document.getElementById("syntaxhighlighteranchor") );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();
</script>
<script type='text/javascript' src='http://s0.wp.com/wp-content/js/devicepx-jetpack.js?ver=201652'></script>
<script type='text/javascript' src='http://s.gravatar.com/js/gprofiles.js?ver=2016Decaa'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type='text/javascript' src='../../../../wp-content/plugins/jetpack/modules/wpgroho.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='../../../../wp-content/themes/Winter/js/superfish.js%3Fver=20120206'></script>
<script type='text/javascript' src='../../../../wp-includes/js/wp-embed.min.js%3Fver=4.6.1'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var countVars = {"disqusShortname":"michaellindon"};
/* ]]> */
</script>
<script type='text/javascript' src='../../../../wp-content/plugins/disqus-comment-system/media/js/count.js%3Fver=4.6.1'></script>
<script type='text/javascript' src='http://stats.wp.com/e-201652.js' async defer></script>
<script type='text/javascript'>
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:3.9.7',blog:'27325203',post:'0',tz:'-4',srv:'www.lindonslog.com'} ]);
	_stq.push([ 'clickTrackerInit', '27325203', '0' ]);
</script>

</body>
</html>
