{
    "docs": [
        {
            "location": "/",
            "text": "Introduction to sss4clj\n\n\nShotgun Stochastic Search is an algorithm for maximizing functions {0,1}\u1d3e \u2192 \u211d described in \nShotgun Stochastic Search for \u201cLarge p\u201d Regression\n.\nThe philosophy of sss4clj is \"less is more\". Rather than provide a monolithic library catering to all possible optimization problems, sss4clj provides a minimal set of very general composable functions which the user can combine to apply to any specific case. Each element a \u2208 {0,1}\u1d3e can be uniquely identified by an \"active set\" i.e. {i \u2208 \u2115| a\u1d62 = 1}. This project makes the design choice to work with the latter.\n\n\nInstallation\n\n\nAdd the following dependency in your project.clj file\n\n\n\n\nBase Usage\n\n\nAn example.clj file can be found in the source directory which one can experiment with in the repl.\n\n\n\n(ns sss4clj.example\n  (:require [sss4clj.core :refer :all]\n            [clojure.math.combinatorics :as combo]))\n\n;; Let A \u2286 \u03a9 \u2282 \u2115\n(def A #{0 3 4})\n(def Omega #{0 1 2 3 4})\n\n;; use drop-1 to get all sets formed by removing 1 element\n(drop-1 A)\n=> (#{4 3} #{0 3} #{0 4})\n\n;; drop-1 of empty set is considered empty\n(drop-1 #{})\n=> ()\n\n;; use add-1 to get all sets formed from the union of A\n;; with one element of \u03a9\\A.\n(add-1 Omega A)\n=> (#{0 1 4 3} #{0 4 3 2})\n\n;; add-1 of Omega is considered empty\n(add-1 Omega Omega)\n=> ()\n\n;; use swap-1 to get all sets formed from exchanging one element\n;; of A with one element of its \u03a9\\A\n(swap-1 Omega A)\n=> (#{1 4 3} #{0 1 3} #{0 1 4} #{4 3 2} #{0 3 2} #{0 4 2})\n\n;; use neighbours to obtain all such sets formed form drop-1\n;; add-1 and swap-1 operations\n(neighbours Omega A)\n=> [(#{4 3} #{0 3} #{0 4}) (#{0 1 4 3} #{0 4 3 2}) (#{1 4 3} #{0 1 3} #{0 1 4} #{4 3 2} #{0 3 2} #{0 4 2})]\n\n;;; To illustrate the next sss4clj functions we need a\n;; mathematical objective function that maps sets A \u2286 \u03a9 to R.\n\n;; To do this lets just zip all sets in the power set of \u03a9\n;; with a random uniform and convert to a function\n(def subvalues (into (hash-map) (map vector (map #(set %) (combo/subsets (into (vector) Omega))) (repeatedly #(rand)))))\n(defn objective [x] (subvalues x))\n\n;; objective is now the function which we will try to maximize\n(objective #{})\n=> 0.5809422699383645\n(objective Omega)\n=> 0.37949346847568177\n(objective A)\n=> 0.1971199058962515\n\n;; add-priority-map returns a new function that on the surface\n;; is identical to objective\n(def max-count 2)\n(def score (add-priority-map objective max-count))\n(score #{})\n=> 0.5809422699383645\n\n;; but score has a priority-map appended to its meta-data for\n;; the purposes of recording and tracking the best values so far\n(deref (:scoreboard (meta score)))\n=> {#{} 0.5809422699383645}\n\n;; sss may run for a long time and so optionally the user may\n;; only want to keep track of the top `max-count` vals in memory.\n;; This is the purpose of the priority map.\n(objective A)\n=> 0.1971199058962515\n(deref (:scoreboard (meta score)))\n=> {#{0 4 3} 0.1971199058962515, #{} 0.5809422699383645}\n(score Omega)\n=> 0.37949346847568177\n(deref (:scoreboard (meta score)))\n=> {#{0 1 4 3 2} 0.37949346847568177, #{} 0.5809422699383645}\n\n;; Once the size of the priority-map becomes large than max-count\n;; it drops the key-value pair with the smallest value. In the\n;; previous example the key-value pair with key A was dropped.\n\n;; Given a score function, an initial set A, and \u03a9, the user\n;; can perform one iteration of sss\n(next-model score Omega A)\n=> #{0 4 2}\n\n;; The output is a new model, but one can check the sets visited\n;; within the iteration by dereferencing the meta-data of score\n(deref (:scoreboard (meta score)))\n=> {#{0 3 2} 0.8143170162925805, #{0 4} 0.8948151339032558}\n\n;; Alternatively, the user can abstract the fine-grained details\n;; of the implementation and call a one function do-all\n;; This returns the final scoreboard of sets visited after\n;; 100 iterations of sss with a max-count of 5\n(run-sss objective Omega 100 5)\n=> {#{0 1 4 3} 0.8108222058800102, #{0 3 2} 0.8143170162925805, #{0 4} 0.8948151339032558, #{0} 0.951193987214408, #{1} 0.9658085016841405}\n\n\n\n\n\nLicense\n\n\nCopyright \u00a9 2017 Michael Lindon\n\n\nDistributed under the Eclipse Public License either version 1.0",
            "title": "Home"
        },
        {
            "location": "/#introduction-to-sss4clj",
            "text": "Shotgun Stochastic Search is an algorithm for maximizing functions {0,1}\u1d3e \u2192 \u211d described in  Shotgun Stochastic Search for \u201cLarge p\u201d Regression .\nThe philosophy of sss4clj is \"less is more\". Rather than provide a monolithic library catering to all possible optimization problems, sss4clj provides a minimal set of very general composable functions which the user can combine to apply to any specific case. Each element a \u2208 {0,1}\u1d3e can be uniquely identified by an \"active set\" i.e. {i \u2208 \u2115| a\u1d62 = 1}. This project makes the design choice to work with the latter.",
            "title": "Introduction to sss4clj"
        },
        {
            "location": "/#installation",
            "text": "Add the following dependency in your project.clj file",
            "title": "Installation"
        },
        {
            "location": "/#base-usage",
            "text": "An example.clj file can be found in the source directory which one can experiment with in the repl.  \n(ns sss4clj.example\n  (:require [sss4clj.core :refer :all]\n            [clojure.math.combinatorics :as combo]))\n\n;; Let A \u2286 \u03a9 \u2282 \u2115\n(def A #{0 3 4})\n(def Omega #{0 1 2 3 4})\n\n;; use drop-1 to get all sets formed by removing 1 element\n(drop-1 A)\n=> (#{4 3} #{0 3} #{0 4})\n\n;; drop-1 of empty set is considered empty\n(drop-1 #{})\n=> ()\n\n;; use add-1 to get all sets formed from the union of A\n;; with one element of \u03a9\\A.\n(add-1 Omega A)\n=> (#{0 1 4 3} #{0 4 3 2})\n\n;; add-1 of Omega is considered empty\n(add-1 Omega Omega)\n=> ()\n\n;; use swap-1 to get all sets formed from exchanging one element\n;; of A with one element of its \u03a9\\A\n(swap-1 Omega A)\n=> (#{1 4 3} #{0 1 3} #{0 1 4} #{4 3 2} #{0 3 2} #{0 4 2})\n\n;; use neighbours to obtain all such sets formed form drop-1\n;; add-1 and swap-1 operations\n(neighbours Omega A)\n=> [(#{4 3} #{0 3} #{0 4}) (#{0 1 4 3} #{0 4 3 2}) (#{1 4 3} #{0 1 3} #{0 1 4} #{4 3 2} #{0 3 2} #{0 4 2})]\n\n;;; To illustrate the next sss4clj functions we need a\n;; mathematical objective function that maps sets A \u2286 \u03a9 to R.\n\n;; To do this lets just zip all sets in the power set of \u03a9\n;; with a random uniform and convert to a function\n(def subvalues (into (hash-map) (map vector (map #(set %) (combo/subsets (into (vector) Omega))) (repeatedly #(rand)))))\n(defn objective [x] (subvalues x))\n\n;; objective is now the function which we will try to maximize\n(objective #{})\n=> 0.5809422699383645\n(objective Omega)\n=> 0.37949346847568177\n(objective A)\n=> 0.1971199058962515\n\n;; add-priority-map returns a new function that on the surface\n;; is identical to objective\n(def max-count 2)\n(def score (add-priority-map objective max-count))\n(score #{})\n=> 0.5809422699383645\n\n;; but score has a priority-map appended to its meta-data for\n;; the purposes of recording and tracking the best values so far\n(deref (:scoreboard (meta score)))\n=> {#{} 0.5809422699383645}\n\n;; sss may run for a long time and so optionally the user may\n;; only want to keep track of the top `max-count` vals in memory.\n;; This is the purpose of the priority map.\n(objective A)\n=> 0.1971199058962515\n(deref (:scoreboard (meta score)))\n=> {#{0 4 3} 0.1971199058962515, #{} 0.5809422699383645}\n(score Omega)\n=> 0.37949346847568177\n(deref (:scoreboard (meta score)))\n=> {#{0 1 4 3 2} 0.37949346847568177, #{} 0.5809422699383645}\n\n;; Once the size of the priority-map becomes large than max-count\n;; it drops the key-value pair with the smallest value. In the\n;; previous example the key-value pair with key A was dropped.\n\n;; Given a score function, an initial set A, and \u03a9, the user\n;; can perform one iteration of sss\n(next-model score Omega A)\n=> #{0 4 2}\n\n;; The output is a new model, but one can check the sets visited\n;; within the iteration by dereferencing the meta-data of score\n(deref (:scoreboard (meta score)))\n=> {#{0 3 2} 0.8143170162925805, #{0 4} 0.8948151339032558}\n\n;; Alternatively, the user can abstract the fine-grained details\n;; of the implementation and call a one function do-all\n;; This returns the final scoreboard of sets visited after\n;; 100 iterations of sss with a max-count of 5\n(run-sss objective Omega 100 5)\n=> {#{0 1 4 3} 0.8108222058800102, #{0 3 2} 0.8143170162925805, #{0 4} 0.8948151339032558, #{0} 0.951193987214408, #{1} 0.9658085016841405}",
            "title": "Base Usage"
        },
        {
            "location": "/#license",
            "text": "Copyright \u00a9 2017 Michael Lindon  Distributed under the Eclipse Public License either version 1.0",
            "title": "License"
        },
        {
            "location": "/subsets/",
            "text": "Example: Best Subset Selection\n\n\nBy way of introduction consider the best subset selection problem. By this\nI mean minimizing the residual sum of squares subject to a cardinality constraint\non the support of the regression coefficients. Lets say that the zero norm\nof the regression coefficients must be less than an integer k, for which\nI will assign 5 for this problem.\n\n\nGenerate Some Data\n\n\nLets generate some data with which to illustrate the application of sss to\nthe best subset selection problem.\n\n\n(ns best-subset.core\n  (:require [clojure.core.matrix :refer :all]\n            [distributions.core :refer :all]\n            [sss4clj.core :refer :all]\n            [clojure.core.matrix.linear :as la]))\n\n\n(def n 200)\n(def p 20)\n(def X (reshape (sample (normal 0 1) (* n p)) [n p]))\n(def B (matrix (for [i (range 0 p)] (if (< i 4) i 0))))\n(def Y (add (mmul X B) (sample (normal 0 1) n)))\n\n\n\n\nRunning Shotgun Stochastic Search\n\n\nNote that sss requires that we sample a new state, for which the probability\nof being sampled is some function of the objective function. In this context\nwe want models with a low residual sum of squares to have a high probability\nof being sampled. For this reason we take the objective function to the \nexponential of the negative RSS with sum scaling. We also set the probability\nto zero if the cardinality constraint is broken.\n\n\n(defn RSS [g]\n  (if (empty? g)\n    (dot Y Y)\n    (let [p-g (count g)\n          X-g (transpose (reshape (flatten (map #(mget (columns X) %) g)) [p-g n]))\n          proj (mmul X-g (la/solve (mmul (transpose X-g) X-g)) (transpose X-g))\n          orth-proj (sub (identity-matrix n) proj)]\n      (dot Y (mmul orth-proj Y)))))\n\n(def k 5)\n\n(defn objective [g]\n  (if (> (count g) k) 0 (exp (negate (/ (RSS g) n)))))\n\n\n(def Omega (into #{} (range 0 p)))\n(def top-subsets (run-sss objective #{} Omega 10 5))\n\n\n\n\nNote Omega is the the set containing integers from 0 to p-1 corresponding\nto all possible values of the active subset of regression coefficients.\nIn order to run sss we specify the previously defined objective, an initial\nset (the emtpy set in this example), Omega, the number of iterations to run\nand the top number of models discovered. The latter is only really an issue\nfor long runs with memory constraints. If you want to see all the models visited,\nthen just set this to be a really large interger.\n\n\n{#{1 4 3 2 16} 0.36936833972859484, #{1 3 12 2 9} 0.36940132623186667, #{1 6 3 12 2} 0.37143277613460907, #{1 3 12 2 5} 0.3772822611159165, #{1 3 12 2 16} 0.37729521913614006}\n\n\n\n\nCopyright \u00a9 2017 Michael Lindon\n\n\nDistributed under the Eclipse Public License either version 1.0",
            "title": "Best Subset Selection"
        },
        {
            "location": "/subsets/#example-best-subset-selection",
            "text": "By way of introduction consider the best subset selection problem. By this\nI mean minimizing the residual sum of squares subject to a cardinality constraint\non the support of the regression coefficients. Lets say that the zero norm\nof the regression coefficients must be less than an integer k, for which\nI will assign 5 for this problem.",
            "title": "Example: Best Subset Selection"
        },
        {
            "location": "/subsets/#generate-some-data",
            "text": "Lets generate some data with which to illustrate the application of sss to\nthe best subset selection problem.  (ns best-subset.core\n  (:require [clojure.core.matrix :refer :all]\n            [distributions.core :refer :all]\n            [sss4clj.core :refer :all]\n            [clojure.core.matrix.linear :as la]))\n\n\n(def n 200)\n(def p 20)\n(def X (reshape (sample (normal 0 1) (* n p)) [n p]))\n(def B (matrix (for [i (range 0 p)] (if (< i 4) i 0))))\n(def Y (add (mmul X B) (sample (normal 0 1) n)))",
            "title": "Generate Some Data"
        },
        {
            "location": "/subsets/#running-shotgun-stochastic-search",
            "text": "Note that sss requires that we sample a new state, for which the probability\nof being sampled is some function of the objective function. In this context\nwe want models with a low residual sum of squares to have a high probability\nof being sampled. For this reason we take the objective function to the \nexponential of the negative RSS with sum scaling. We also set the probability\nto zero if the cardinality constraint is broken.  (defn RSS [g]\n  (if (empty? g)\n    (dot Y Y)\n    (let [p-g (count g)\n          X-g (transpose (reshape (flatten (map #(mget (columns X) %) g)) [p-g n]))\n          proj (mmul X-g (la/solve (mmul (transpose X-g) X-g)) (transpose X-g))\n          orth-proj (sub (identity-matrix n) proj)]\n      (dot Y (mmul orth-proj Y)))))\n\n(def k 5)\n\n(defn objective [g]\n  (if (> (count g) k) 0 (exp (negate (/ (RSS g) n)))))\n\n\n(def Omega (into #{} (range 0 p)))\n(def top-subsets (run-sss objective #{} Omega 10 5))  Note Omega is the the set containing integers from 0 to p-1 corresponding\nto all possible values of the active subset of regression coefficients.\nIn order to run sss we specify the previously defined objective, an initial\nset (the emtpy set in this example), Omega, the number of iterations to run\nand the top number of models discovered. The latter is only really an issue\nfor long runs with memory constraints. If you want to see all the models visited,\nthen just set this to be a really large interger.  {#{1 4 3 2 16} 0.36936833972859484, #{1 3 12 2 9} 0.36940132623186667, #{1 6 3 12 2} 0.37143277613460907, #{1 3 12 2 5} 0.3772822611159165, #{1 3 12 2 16} 0.37729521913614006}  Copyright \u00a9 2017 Michael Lindon  Distributed under the Eclipse Public License either version 1.0",
            "title": "Running Shotgun Stochastic Search"
        },
        {
            "location": "/kl/",
            "text": "Example: Kullback Leibler Divergence\n\n\nIn the context of Bayesian variable selection suppose we have a generative\nmodel for our data which is fully parameterized by an active set A \u2208 2\u1d3a.\nIn some cases, given an active set A, we would like to find alternative\nmodels, parameterized by an differenct active set B, which have similar\npredictive properties. To establish an optimization problem, lets try\nto find models which minimize the Kullback-Leibler divergence from\nthe model under consideration A.\n\n\nEstablishing some helper functions\n\n\nLets assume the generative model is a non standard t-distribution. To\nwork with this we need to write a few functions \n\n\n(defn logpdf-nst\n  \"Unnormalized logpdf of non standard t\"\n  [x mu sigma df]\n  (let [z (/ (- x mu) sigma)]\n    (negate (+ (log sigma) (* (/ (inc df) 2) (log (inc (/ (square z) df))))))))\n\n(defn sample-nst\n  \"Generate non standard t random variate\"\n  [n mu sigma df]\n  (add mu (mul sigma (sample-t n :df df))))\n\n\n\n\nNow lets write a function KL which maps a number of arguments which define\nthe generative model to a function 2\u1d3a -> R.\n\n\n(defn KL\n  \"Returns an objective function defined by prior parameters\"\n  [p-set X prior-mu prior-sigma scaler df]\n  (let [p (into [] p-set)\n        Xp (select X p)\n        p-mean (dot Xp (select prior-mu p))\n        p-sigma (if (empty? p)\n                  (square scaler)\n                  (* (square scaler) (inc (dot Xp (mmul (inverse (select prior-sigma p p)) Xp)))))\n        draws (sample-nst 10000 p-mean (sqrt p-sigma) df)\n        p-logpdf (fn [x] (logpdf-nst x p-mean (sqrt p-sigma) df))]\n    (fn [n-set]\n      (let [n (into [] n-set)\n            Xn (select X n)\n            n-mean (dot Xn (select prior-mu n))\n            n-sigma (if (empty? n)\n                      (square scaler)\n                      (* (square scaler) (inc (dot Xn (mmul (inverse (select prior-sigma n n)) Xn)))))\n            n-logpdf (fn [x] (logpdf-nst x n-mean (sqrt n-sigma) df))\n            difference (fn [x] (- (p-logpdf x) (n-logpdf x)))]\n        (mean (map difference draws))))))\n\n\n\n\n\nSimulating model parameters\n\n\n(def dim 10)\n(def L (reshape (matrix (sample-normal (* dim dim))) [dim dim]))\n(def prior-sigma (mmul (transpose L) L))\n(def prior-mu (sample-normal dim))\n(def previous-model #{0 1 4})\n(def X (sample-normal dim))\n\n\n\n\nObtain objective function\n\n\nNote the current formulation is a minimization problem, namely, trying to minimize the KL\ndivergence from the previous model, whilst sss4clj expects a maximization problem. Note\ninserting a minus sign is not quite enough because the objective function values given\na set of candidate active sets must be amenable to renormalization such that one can\nsample one of these candidates from a multinomial distribution. To this end one must\ncompose the divergence (with an optional scaling factor) with negation and exponentiation.\n\n\n(def divergence (KL previous-model X prior-mu prior-sigma 1 1))\n(def obj-fn (comp exp negate (partial * 10) divergence))\n\n\n\n\nRun-sss\n\n\nRun for 100 iterations keeping track of the 10 best candidates\n\n\n(def Omega (range 0 dim))\n(run-sss obj-fn Omega 100 10)\n\n\n\n\nLicense\n\n\nCopyright \u00a9 2017 Michael Lindon\n\n\nDistributed under the Eclipse Public License either version 1.0",
            "title": "Kullback Leibler Divergence"
        },
        {
            "location": "/kl/#example-kullback-leibler-divergence",
            "text": "In the context of Bayesian variable selection suppose we have a generative\nmodel for our data which is fully parameterized by an active set A \u2208 2\u1d3a.\nIn some cases, given an active set A, we would like to find alternative\nmodels, parameterized by an differenct active set B, which have similar\npredictive properties. To establish an optimization problem, lets try\nto find models which minimize the Kullback-Leibler divergence from\nthe model under consideration A.",
            "title": "Example: Kullback Leibler Divergence"
        },
        {
            "location": "/kl/#establishing-some-helper-functions",
            "text": "Lets assume the generative model is a non standard t-distribution. To\nwork with this we need to write a few functions   (defn logpdf-nst\n  \"Unnormalized logpdf of non standard t\"\n  [x mu sigma df]\n  (let [z (/ (- x mu) sigma)]\n    (negate (+ (log sigma) (* (/ (inc df) 2) (log (inc (/ (square z) df))))))))\n\n(defn sample-nst\n  \"Generate non standard t random variate\"\n  [n mu sigma df]\n  (add mu (mul sigma (sample-t n :df df))))  Now lets write a function KL which maps a number of arguments which define\nthe generative model to a function 2\u1d3a -> R.  (defn KL\n  \"Returns an objective function defined by prior parameters\"\n  [p-set X prior-mu prior-sigma scaler df]\n  (let [p (into [] p-set)\n        Xp (select X p)\n        p-mean (dot Xp (select prior-mu p))\n        p-sigma (if (empty? p)\n                  (square scaler)\n                  (* (square scaler) (inc (dot Xp (mmul (inverse (select prior-sigma p p)) Xp)))))\n        draws (sample-nst 10000 p-mean (sqrt p-sigma) df)\n        p-logpdf (fn [x] (logpdf-nst x p-mean (sqrt p-sigma) df))]\n    (fn [n-set]\n      (let [n (into [] n-set)\n            Xn (select X n)\n            n-mean (dot Xn (select prior-mu n))\n            n-sigma (if (empty? n)\n                      (square scaler)\n                      (* (square scaler) (inc (dot Xn (mmul (inverse (select prior-sigma n n)) Xn)))))\n            n-logpdf (fn [x] (logpdf-nst x n-mean (sqrt n-sigma) df))\n            difference (fn [x] (- (p-logpdf x) (n-logpdf x)))]\n        (mean (map difference draws))))))",
            "title": "Establishing some helper functions"
        },
        {
            "location": "/kl/#simulating-model-parameters",
            "text": "(def dim 10)\n(def L (reshape (matrix (sample-normal (* dim dim))) [dim dim]))\n(def prior-sigma (mmul (transpose L) L))\n(def prior-mu (sample-normal dim))\n(def previous-model #{0 1 4})\n(def X (sample-normal dim))",
            "title": "Simulating model parameters"
        },
        {
            "location": "/kl/#obtain-objective-function",
            "text": "Note the current formulation is a minimization problem, namely, trying to minimize the KL\ndivergence from the previous model, whilst sss4clj expects a maximization problem. Note\ninserting a minus sign is not quite enough because the objective function values given\na set of candidate active sets must be amenable to renormalization such that one can\nsample one of these candidates from a multinomial distribution. To this end one must\ncompose the divergence (with an optional scaling factor) with negation and exponentiation.  (def divergence (KL previous-model X prior-mu prior-sigma 1 1))\n(def obj-fn (comp exp negate (partial * 10) divergence))",
            "title": "Obtain objective function"
        },
        {
            "location": "/kl/#run-sss",
            "text": "Run for 100 iterations keeping track of the 10 best candidates  (def Omega (range 0 dim))\n(run-sss obj-fn Omega 100 10)",
            "title": "Run-sss"
        },
        {
            "location": "/kl/#license",
            "text": "Copyright \u00a9 2017 Michael Lindon  Distributed under the Eclipse Public License either version 1.0",
            "title": "License"
        }
    ]
}